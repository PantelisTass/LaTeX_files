\documentclass{article}
\input{preamble}  % Include the preamble from an external file

%\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}\addtocounter{page}{-1}}
\fancyhead{}
\fancyhead[L]{\text{Advanced Probability}}
\fancyhead[R]{\text{Pantelis Tassopoulos}}

\title{\Huge Part III Advanced Probability \\ 
\huge Based on lectures by P. Sousi}
\author{\Large Notes taken by Pantelis Tassopoulos}
\date{\Large Michaelmas 2023}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage 

\section{Conditional Expectation}
\mymark{Lecture 1}\subsection{Basic definitions}
Let $ (\Omega, \F, \PP) $ be a probability space. Remember the following definitions 
\begin{boxdef}[Sigma algebra]\label{def: sigma algebra}
	$ \F $ is a sigma algebra if and only if: ($ \F\in \mathcal{P}{\Omega} $)
	\begin{enumerate}
		\item $ \Omega \in \F $
		\item $ A\in \F \implies A^c\in \F$
		\item $ (A_n)_{n\in \N}\subseteq \F \implies \displaystyle\bigcup_{n\in\N}A_n\in\F $
	\end{enumerate}
\end{boxdef}
\begin{boxdef}[Probability measure]\label{def: prob measure}
	$ \PP $ is a probability measure if
	\begin{enumerate}
		\item $ \PP:\F\to [0,1] $ (i.e. a set function)
		\item $ \PP(\Omega) = 1 $, and $ \PP(\emptyset) = 0 $
		\item $ (A_n)_{n\in \N} $ pairwise disjoint $ \implies \PP\left(\displaystyle\bigcup_{n\in\N}A_n\right) =\displaystyle\sum^{\infty}_{n=1}\PP(A_n)$.
	\end{enumerate}
	
\end{boxdef} 
\begin{boxdef}[Random Variable]\label{def: rv}
	$ X:\Omega\to \R $ is a \underline{random variable} if for all $ B $ open in $ \R $, $ X^{-1}(B)\in \F $.
\end{boxdef}
\begin{remark}
	Observe that the sigma algebra $ \mathcal{G}=\{B\subseteq\R: X(B)\in\F\}\supseteq \mathcal{O} \implies \mathcal{G}\supseteq \mathcal{B}(\R) $, the former being the collection of open sets in $ \R $ and the latter the Borel sigma algebra on $ \R $ with the usual topology, namely, $ \sigma(\mathcal{O})$ (see below for the notation).
\end{remark}

Let $ \mathcal{A} $ be a collection of subsets of $ \Omega $. We define 
\[\begin{array}{ll}
	\sigma(\mathcal{A}) &= \text{smallest sigma algebra containing $ \mathcal{A} $} \\
     &=\displaystyle \bigcap \{\mathcal{T}:\mathcal{T} \text{ sigma algebra containing }\mathcal{A}\}.
\end{array}
\]

\begin{boxdef}[Borel sigma algebra on $ \R $]\label{def: borel sigma alg}
	Let $ \mathcal{O} = \{\text{open sets} \R\} $. Then, the Borel sigma algebra $ \mathcal{B}(\R)( \coloneq \mathcal{B} ) $ is defined as above, namely, 
	\[\mathcal{B}(\R)\coloneq \sigma(\mathcal{O}).\]
\end{boxdef}

Let $ (X_i)_{i\in I} $ be a family of random variables, then $ \sigma(X_{i}:i\in I) =$ the smallest sigma algebra that makes them all measurable. We also have the characterisation 
$ \sigma(X_{i}: i\in I) = \sigma(\{\underbrace{\{\omega\in \Omega: X_{i}(\omega)\in B\}}_{X^{-1}_{i}(B)}, i\in I, B\in \mathcal{B}(\R)\})$.

\subsection{Expectation}

Note we use the following for the indicator function on some event $ A $
\[
    \mathbf{1}(A)(x) = \mathbf{1}(x\in A) 
     \coloneqq \left. \begin{array}{@{}l@{}}
    1, \quad x\in A \\
    0, \quad x\notin A
     \end{array}\right\rbrace, \quad A\in \F.
\]


We now begin the construction of the expectation of generic random variables.\\

\underline{Positive simple random variables:} $X = \displaystyle\sum^{
n}_{i=1}\mathbf{1}(A_{i}), c_{i}\geq 0, A_{i}\in\F.  $.
\[
	\mathbb{E}[X]\coloneqq\displaystyle\sum^{n}_{i=1}c_{i}\PP(A_{i}). 
\]


\underline{Non-negative random variables:} $ (X\geq 0). $
We proceed by approximation. Namely, let $ X_{n}(\omega)\coloneqq 2^{-n}\lfloor 2^{-n}\cdot X(\omega)\rfloor \land n \uparrow X(\omega) , n\to \infty$. Now, by monotone convergence, 
\[
	\mathbb{E}[X]\coloneqq \uparrow \displaystyle\lim_{n\to\infty}\mathbb{E}[X_{n}]=\displaystyle\sup\mathbb{E}[X].
\]

\underline{General random variables:} Have the decomposition $ X = X^{+}-X^{-} $, where $ X^{+} = X\lor 0$, $X^{-}=-X\land 0 $. If one of $ \mathbb{E}[X^{+}], \mathbb{E}[X^{-}]  <\infty $ then set 
\[
	\mathbb{E}[X]\coloneqq \mathbb{E}[X^{+}]-\mathbb{E}[X^{-}].  
\]

\begin{boxdef}\label{def: integrable rv}
	$ X $ is called \underline{integrable} if $ \mathbb{E}[|X|]<\infty $.
\end{boxdef}

\begin{boxdef}\label{def: cond prob event}
Let $ B\in \F $ with $ \PP(B)>0 $. Then for all $ A\in \F $, set 
\[
\PP(A|B)\coloneqq \frac{\PP(A\cap B)}{\PP(B)}
\] 

\end{boxdef}


Now for an integer-valued random variable $ X $, we set:
\[
	\mathbb{E}[X|B]\coloneqq \frac{\mathbb{E}[X\cdot \mathbf{1}_{B}]}{\PP(B)}
\]


\subsection{Conditional expectation with respect to countably generated sigma algebras}

\mymark{Lecture 2}We now extend the definition of the conditional expectation for a \underline{countably generated sigma algebra}. Let $ (\Omega, \F, \PP) $ be a probability space. We call the sigma algebra $\mathcal{G} $ countably generated if there exists a collection $ (B_{n})_{n\in \N} $ of pairwise disjoint events such that $\displaystyle\bigcup_{n\in I}B_{n} = \Omega$ with ($ I $ countable) and $\mathcal{ G} = \sigma(B_{i}:i\in I)$.\\ 

Let $X$ be an integrable random variable. We want to define $\mathbb{E}[X|\mathcal{G}]$.\\ 

Define $X'(\omega) = \mathbb{E}[X|B_{i}]$, whenever $w\in B_{i}$, i.e. 
\[
	X' =\displaystyle\sum_{i\in I}\mathbf{1}(B_{i})\cdot\mathbb{E}[X|B_{i}]. 
\]

We make the convention that $\mathbb{E}[X|B_{i}] = 0$ if $\PP(B_{i}) = 0$. It is easy to check that $X'$ is $\mathcal{G}-$measurable. We also have that 
\[
\mathcal{G}  = \left\{\displaystyle\bigcup_{j\in } B_{j}: J\subseteq I \right\}
\]
and $X'$ satisfies for all $ G\in\mathcal{G}$:$\mathbb{E}[X\cdot\mathbf{1}_{G}]=\mathbb{E}[X'\cdot\mathbf{1}_{G}] $ and 
\[\begin{array}{ll}
	\mathbb{E}[|X'|] &\leq\mathbb{E} \left[\displaystyle\sum_{i\in I}|\mathbb{E}[X|B_{i}]\mathbf{1}(B_{i})  \right] \\
			 &=\displaystyle\sum_{i \in I}\PP(B_{i})\cdot \left|\mathbb{E}[X|B_{i}] \right|\\ 
			 &\leq\displaystyle\sum_{i\in I}\PP(B_{i})\cdot \underbrace{\mathbb{E}[X\cdot\mathbf{1}(B_{i})]}_{\PP(B_{i})}\\ 
			 &=\mathbb{E}[|X|]<\infty.
\end{array}
\]

\subsection{General case}

We say $ A\in \F$ happens \underline{a.s.} if $ \PP(A) = 1$. \underline{Recall} (from measure theory and basic functional analysis): 
\begin{theorem}[Monotone Convergence Theorem (MCT)]\label{thm: MCT}
	Let $(X_{n})_{n\in \N}$ be such that $ X_{n}\geq 0, X$ be random variables such that $ X_{n}\uparrow X$ as $ n\to \infty$. Then, $\mathbb{E}[X_{n}]\uparrow\mathbb{E}[X]$ as $ n\to \infty$.
\end{theorem}
 
\begin{theorem}[Dominanted Convergenec Theorem (DCT)]\label{thm: DCT}
	Let $ (X_{n})_{n\in \N}$ be random variables such that $ X_{n}\to X$ a.s. as $ n\to \infty$ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$, where $ Y$ is integrable, then $\mathbb{E}[X_{n}]\to\mathbb{E}[X]$, as $ n\to \infty$.
\end{theorem}

Let $ 1\leq p <\infty$ and $ f $ a measurable function, then set $ \norm{f}_{p}\coloneqq \left(\mathbb{E}[\norm{f}^{p}]\right)^{\frac{1}{p}}$. If $ p =\infty$, then set $ \norm{f}_{\infty}\coloneqq \displaystyle \inf \{\lambda: |f|\leq \lambda \text{ a.s.}\}$. Recall for all $ p$, the Lebesgue spaces, $\mathcal{L}^{p}(\Omega, \F, \PP)=\{f: \norm{f}_{p}<\infty\}$.

\begin{theorem}\label{thm: orthog proj hilbert}
	$ \mathcal{L}^{2}(\Omega, \F, \PP) $ is a Hilbert space, with inner product $  \bracket{u}{v}_{2}=\mathbb{E}[u\cdot v]$. Furthermore, for any closed subspace $\mathcal{H}$, if $ f\in\mathcal{L}^{2}$, there exists a unique $ g\in\mathcal{H}$ s.t. $ \norm{f-g}_{\mathcal{L}^{2}}=\displaystyle\inf_{h\in\mathcal{H}}\norm{f-h}_{\mathcal{L}^{2}}$ and $ \bracket{f-g}{h}=0$, for all $ h\in\mathcal{H}$. We say that $ g$ is the \underline{orthogonal projection} of $ f$ in $\mathcal{H}$.
\end{theorem}


We now construct the conditional expectation in the general case, for any integrably random variable with respect to an arbitrary sigma algebras.

\begin{theorem}[Conditional Expectation]\label{thm: cond exp}
Let $ (\Omega, \F, \PP)$ be a probability space, $\mathcal{G}\subseteq \F$ a sub-sigma algebra, $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$. Then there exists an integrable random variable $ Y$ satisfying:
\begin{enumerate}
	\item $ Y$ is $\mathcal{G}-$measurable
	\item for all $ G\in\mathcal{G},\mathbb{E}[X\cdot\mathbf{1}(G)]= \mathbb{E}[Y\cdot\mathbf{1}(G)]$.
\end{enumerate}
Moreover, $ Y$ unique in the sense that if $ Y'$ also satisfies the above $ 1),2)$, then $ Y = Y'$ a.s.. We call $ Y$ a version of the conditional expectation of $ X$ given $ G$. We write $ Y =\mathbb{E}[X\mathcal{G}]$ a.s. If $\mathcal{G} = \sigma(Z)$, where $ Z$ is a random variable, then we write $\mathbb{E}[Z] =\mathbb{E}[X|\mathcal{G}]$.

\end{theorem}

\begin{remark}
	$ 2)$ could be replaced by $\mathbb{E}[X\cdot Z] =\mathbb{E}[Y\cdot Z]$ for all $ Z$ bounded $\mathcal{G}-$measurable random variables. 
\end{remark}

We now state and prove the main theorem of this section:

\begin{proof}{(Theorem \ref{thm: cond exp})}
	\underline{Uniqueness:} Let $ Y, Y'$ satisfy $ 1), 2)$. Let $ A = \{Y > Y'\}\in\mathcal{G}$. Then $ 2) $  
	\[\begin{array}{ll}
	&\implies \mathbb{E}[Y\cdot\mathbf{1}(A)] =\mathbb{E}[Y'\cdot\mathbf{1}(A)]=\mathbb{E}[X\cdot\mathbf{1}(A)] \\
	&\implies\mathbb{E}[(Y-Y')\cdot\mathbf{1}(A)] = 0\\ 
	&\implies \PP(A) = \PP(Y>Y') = 0\\ 
	&\implies Y\leq Y' \text{ a.s.}.
	\end{array}
	\]
	We similarly obtain $ Y\geq Y'$ a.s., hence we deduce that $ Y = Y'$ a.s.

	\underline{Existence:} three steps. 
	\begin{enumerate}
		\item Assume that $ X \in\mathcal{L}^{2}(\Omega, \F, \PP)$. Observe that $\mathcal{L}^{2}(\Omega,\mathcal{G},\PP)$ is a closed subspace of $\mathcal{L}^{2}(\Omega, \F, \PP)$. Hence, Theorem \ref{thm: orthog proj hilbert}, we have the decomposition  $\mathcal{L}^{2}(\Omega, \F, \PP) =\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)\oplus\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.
Then, we have the corresponding decomposition $ X = Y+Z$, where $ Y\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)$ and $ Z\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP) $ respectively. Define $\mathbb{E}[X\mathcal{G}]\coloneqq Y$, $ Y$ is $\mathcal{G}-$measurable and for all $ A\in\mathcal{G}$, $\mathbb{E}[X\cdot\mathbf{1}(A)]\mathbb{E}[Y\cdot\mathbf{1}(A)]=\mathbb{E}[Z\cdot\mathbf{1}(A)]$ since $ Z\in  \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.

\underline{Claim:} If $ X \geq 0$, a.s. then $ Y \geq 0$ a.s.
Indeed, let $ A = \{Y< 0\}\in\mathcal{G}$. Then observe that $ 0\leq\mathbb{E}[X\cdot\mathbf{1}(A)]=\mathbb{E}[Y\cdot\mathbf{1}(A)]\leq 0$. Hence $\mathbb{E}[Y\cdot\mathbf{1}(A)]=0$ and so $ \PP(A) = 0$, gibing $ Y = 0$ a.s.

\item Assume $ X\geq 0 $.\\ 
	Define $ X_{n} = X\land n\leq n $, meaning $ X_{n}$ is bounded for all $ n\in \N$. So $ X_{n}\in\mathcal{L}^{2}(\Omega, \F, \PP)$. Let $ Y_{n} =\mathbb{E}[X_{n}]$ a.s.. $ (X_{n})_{n\in \N}$ is an increasing sequence. By the claim above, so is $ (Y_{n})_{n\in \N}$ a.s.\\
	Define $ Y = \displaystyle \limsup_{n}Y_{n}$ meaning $ Y$ is $\mathcal{G}-$measurable and $ Y = \uparrow \displaystyle \lim_{n\to \infty}Y_{n} $ a.s. Now, we have that for all $ A\in\mathcal{G}$, $\mathbb{E}[X_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]$. Thus, by theorem \ref{thm: MCT} (MCT), $\mathbb{E}[X\cdot\mathbf{1}(A)]= \displaystyle \lim_{n\to \infty} \mathbb{E}[X_{n}\cdot\mathbf{1}(A)] = \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y\cdot\mathbf{1}(A)]$.

\item $ X$ general in $\mathcal{L}^{1}$.\\ 
	Decompose as before $ X = X^{+}-X^{-}$. Define, $\mathbb{E}[X\mathcal{G}] =\mathbb{E}[X^{+}|\mathcal{G}]-\mathbb{E}[X^{-}|\mathcal{G}]$.
\end{enumerate}

\end{proof}

\mymark{Lecture 3}
\begin{remark}
From the second step of the proof of Theorem \ref{thm: cond exp} we see that we can define $\mathbb{E}[X|\mathcal{G}]$ for all $ X\geq 0$, not necessarily integrable. It satisfies all conditions $ 1) , 2)$ except for the integrability one.
\end{remark}

\begin{boxdef}\label{def: independence of sigma algebras}
$\underbrace{\mathcal{G}_{1},\mathcal{G}_{2}, \dots}_{\text{sigma algebras}} \subset \F$. We call them \underline{independent} if whenever $ G_{i}\in \mathcal{G}_{i}$ and $ i_{1}<\dots i_{k}$ for some $ k \in \N$, then $ \PP(G_{i_{1}}\cap \dots\cap G_{i_{k}}) = \displaystyle \prod^{k}_{j=1}\PP(G_{i_{j}})$.\\ 

Moreover, let $ X$ be a random variable and $\mathcal{G}$ a sigma algebra, then they are said to be int if $ \sigma(X)$ is independent of $\mathcal{G}$.
\end{boxdef}

\underline{Properties of conditional expectations:}
Fix $ X,y \in\mathcal{L}^{1}$, $ G\in \F$.
\begin{enumerate}
	\item $\mathbb{E}[\mathbb{E}[X\mathcal{G}]]=\mathbb{E}[X]$ (take $ A  = \Omega$)
	\item If $ X$ is $\mathcal{G}-$measurable, then $\mathbb{E}[X\mathcal{G}]=X$ a.s.
	\item If $ X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X\mathcal{G}]=\mathbb{E}[X]$
	\item If $ X\geq 0$ a.s., then $\mathbb{E}[X\mathcal{G}]\geq 0 $ a.s. 
	\item For $ \alpha, \beta \in \R$ $\mathbb{E}[\alpha X + \beta Y |\mathcal{G}] = \alpha\mathbb{E}[X]+\beta\mathbb{E}[Y]$
	\item $\mathbb{E}[X|\mathcal{G}]|\leq\mathbb{E}[|X| |\mathcal{G}]$ a.s. 
\end{enumerate}

Below we proved:we expansions of useful measure theoretic results for the expectation to their corresponding conditional counterparts. First recall:
\begin{boxlemma}[Fatou's Lemma]\label{lemma: Fatou}
Let $  X_{n}\geq 0$ for all $ n\in \N$. Then 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s}
\]


\end{boxlemma}


\begin{theorem}[Jensen's Inequality]\label{thm: jensen}
If $ X$ is integrable and $ \phi: \R \to \R$ is a convex function, then 
\[
	\phi(\mathbb{E}[X])\leq\mathbb{E}[\phi(X)]\quad   \text{ a.s.}
\]
\end{theorem}
Now the results themselves:

\begin{theorem}[Conditional Monotone Convergence theorem (MCT)]\label{thm: cond MCT}
Let $\mathcal{G}\subset \F$ be sigma algebras, $ X_{n}\geq 0$ a.a. and $ X_{n}\uparrow X$, as $ n\to \infty$ a.s. Then 
\[
	\mathbb{E}[X_{n}|\mathcal{G}]\uparrow\mathbb{E}[X|\mathcal{G}] \quad \text{ a.s.}
\]

\end{theorem}

\begin{proof}{Theorem \ref{thm: cond MCT}}
	Set $ Y_{n} =\mathbb{E}[X_{n}\mathcal{G}]$ a.s. Observe that $ Y_{n}$ is a.s. increasing. Set $ Y = \displaystyle\limsup_{n}Y_{n}$. $ Y_{n}$ is $\mathcal{G}-$measurable, hence, so is $ Y$ (as a $ \displaystyle \limsup $ of $\mathcal{G}-$measurable random variables) is also $\mathcal{G}-$measurable. Also, $ Y = \displaystyle \lim_{n\to \infty}Y_{n} $ a.s.\\ 

	\underline{Need to show:} $\mathbb{E}[Y\cdot\mathbf{1}(A)]\mathbb{E}[X\cdot\mathbf{1}(A)]$ for all $ A\in\mathcal{G}$.	Indeed,
	\[\begin{array}{ll}
	    \\
	    \mathbb{E}[Y\cdot\mathbf{1}(A)] &=\mathbb{E}[ \displaystyle \lim_{n\to \infty }Y_{n}\cdot\mathbf{1}(A) ] \stackrel{\text{MCT}}{=} \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]\\
																	      &=\displaystyle \lim_{n\to \infty }\mathbb{E}[X_{n}\cdot\mathbf{1}(A)]  =\mathbb{E}[X\cdot\mathbf{1}(A)].
	\end{array}
	\]
	
\end{proof}

\begin{proof}{Theorem \ref{lemma: Fatou}}
$ \displaystyle \liminf_{n}X_{n} = \displaystyle \lim_{n\to \infty }\left( \displaystyle\inf_{k\geq n}X_{k} \right) $, the limit of an increasing sequence. By Theorem \ref{thm: MCT}, we have 
\[
	\displaystyle \lim_{n\to \infty}\mathbb{E}[\displaystyle\inf_{k\geq n}X_{n}|\mathcal{G}] =\mathbb{E}[\displaystyle \liminf_{n}X_{n}|\mathcal{G}]
\]
and 
\[
	\mathbb{E}[\displaystyle \inf_{k\geq n}X_{k}|\mathcal{G}]\stackrel{\text{a.s.}}{\leq } \displaystyle\inf_{k\geq n}\mathbb{E}[X_{k}|\mathcal{G}]\footnote{\text{can take the infinum due to countability that preserves a.s.}}
\]
which gives the result 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s.}
\]

		
\end{proof}

\begin{theorem}[Conditional Dominated Convergence Theorem]\label{thm: cond DCT}
	SUppose $ X_{n}\to X$ a.s. $ n\to \infty $ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$ with $ Y$ integrable. Then $\mathbb{E}[X_{n}\mathcal{G}]\to\mathbb{E}[X\mathcal{G}]$ a.s. as $n\to \infty.$
\end{theorem}

\begin{proof}
	From $ -Y\leq X_{n}\leq Y$, we have $ X_{n}+Y\geq 0$ for all $ n\in \N$ and $ Y-X_{n}\geq 0 $a.s. By Theorem \ref{lemma: Fatou},
\[
		\begin{array}{ll}
		\mathbb{E}[X+Y\mathcal{G}] &=\mathbb{E}[\displaystyle\liminf_{n}(X_{n}+Y)|\mathcal{G}] \\
					   &\leq \displaystyle\liminf_{n}\mathbb{E}[X_{n}+Y|\mathcal{G}] = \displaystyle\liminf_{n}\mathbb{E}[X_{n}\mathcal{G}]+\mathbb{E}[X]
	\end{array}
\]
Thus, 
\[\begin{array}{ll}
	\mathbb{E}[|X-Y| |\mathcal{G}]&=\mathbb{E}[Y-\displaystyle\liminf_{n}X_{n}|\mathcal{G}]  \\
				     &\leq\mathbb{E}[Y]+\displaystyle\liminf_{n}  \mathbb{E}[X_{n}|\mathcal{G}] 
\end{array}
\]
Hence, 
\[
	\displaystyle\limsup_{n} \mathbb{E}[X_{n}|\mathcal{G}] \leq\mathbb{E}[X|\mathcal{G}]
\]
and 
\[
	\displaystyle\liminf_{n} \mathbb{E}[X_{n}|\mathcal{G}] \geq\mathbb{E}[X|\mathcal{G}]
\]
a.s., concluding the proof.

\end{proof}

\begin{theorem}[Conditional Jensen]\label{thm: cond jensen}
Let $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$, $ \phi:\R\to \R$ be a convex function s.t. $ \phi(X)$ is integrable or $ \phi(X)\geq 0 $
\[
	\phi(\mathbb{E}[X|\mathcal{G}])\leq\mathbb{E}[\phi(X)|\mathcal{G}] \quad \text{a.s.}
\]
\end{theorem}

\begin{proof}
	\underline{Claim:} (true for any convex function, no proof given) $ \phi(x)=\displaystyle\sup_{i\in\N}(a_{i}x+b_{i})$, $ a_{i}b_{i}\in\R$. 
Thus, 
\[
	\mathbb{E}[\phi(X)|\mathcal{G}]\geq a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i} \quad \text{ for all } i\in \N.
\]
Taking the supremum gives \footnote{can take the supremum due to countability which again preserves a.s.}
\[
\begin{array}{ll}
      
	\mathbb{E}[\phi(X)|\mathcal{G}]&\geq \displaystyle\sup_{i\in \N} \left(  a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i}  \right)\\ 
				       &= \phi(\mathbb{E}[X|\mathcal{G}]) \quad \text{ a.s.}
\end{array}
\]

\end{proof}

\begin{boxcor}\label{cor: norm contraction cond exp}
	For all $ 1\leq p <\infty \norm{\mathbb{E}[X|\mathcal{G}]}_{p}\leq \norm{X}_{p}$.
\end{boxcor}

\begin{proof}
    Apply conditional Jensen.
\end{proof}

\begin{boxprop}[Tower Property]\label{prop: tower ppty}
Let $ X$ be integrable and $\mathcal{H}\subseteq\mathcal{G}$ sigma algebras. Then 
\[
	\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]=\mathbb{E}[X|\mathcal{H}] \quad \text{ a.s.}
\]

\end{boxprop}

\begin{proof}
	\begin{enumerate}[(a)]
		\item $\mathbb{E}[X|\mathcal{H}] $ is $\mathcal{H}-$measurable.
		\item For all $ A\in\mathcal{H}$ NTS: 
			\[
				\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot\mathbf{1}(A)] =\mathbb{E}[\mathbb{E}[X|\mathcal{H}]\cdot\mathbf{1}(A)]
			\]
			Indeed, both terms above are equal to $\mathbb{E}[X\cdot\mathbf{1}(A)]$ since $ A\in\mathcal{G}\subseteq\mathcal{H}$.
			
    \end{enumerate}
    
\end{proof}

\begin{boxprop}\label{prop: meas factorisation cond exp}
Let $ X\in\mathcal{L}^{1}$, $\mathcal{G}\subseteq \F$, $ Y$ bounded $\mathcal{G}-$measurable. Then 
\[
	\mathbb{E}[X\cdot Y|\mathcal{G}] =  Y\cdot\mathbb{E}[X|\mathcal{G}].
\]

\end{boxprop}


\begin{proof}
	\begin{enumerate}[(a)]
		\item RHS is clearly $\mathcal{G}-$measurable.
		\item For all $ A\in\mathcal{G}$: 
			\[
			\begin{array}{ll}
				\mathbb{E}[X\cdot Y\cdot \mathbf{1}(A)] &=\mathbb{E}[Y\cdot\mathbb{E}[X\mathcal{G}]\cdot\mathbf{1}(A)] \\
				\mathbb{E}[X\cdot (\smash{\underbrace{Y\cdot\mathbf{1}(A)}_{\makebox[0pt]{$\mathcal{G}$-\text{meas. and bounded}}}})]&=\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot Y\cdot\mathbf{1}(A)]=RHS.
			\end{array}
			\]
			
    \end{enumerate}
    \vspace{1em} 
(Also observe that by a monotone class argument, we have for any integrable function $ f:\Omega \to \R$, $\mathbb{E}[X\cdot f] =\mathbb{E}[\mathbb{E}[X|\mathcal{ G}]\cdot f] $ ) 
\end{proof}


\mymark{Lecture 4}

We are building towards the Theorem
\begin{theorem}\label{thm: cond expectation sigma indep}
$ X\in \mathcal{L}^1, \mathcal{G}, \mathcal{H} \subseteq \F$. Assume $ \sigma( \mathcal{G}, \mathcal{H})\perp \mathcal{H}$, Then
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]

\end{theorem}


We begin with a definition
\begin{boxdef}\label{def: pi system}
	Let $ \mathcal{A} $ be a collection of sts. It is called a \underline{$ \pi-$system} if for all $ A,B\in \mathcal{A}$, we also have $ A\cap B\in \mathcal{A}$.
\end{boxdef}


\begin{theorem}[Uniquenes of extension]\label{thm: uniqueness meas extension}
Let $ (E, \xi)$be a measurable space and let $ \mathcal{A}$ be a $ \pi-$system generating the sigma algebra $ \xi$. Let $ \mu, \nu$ be two measures on $ (E, \xi)$ with $ \mu(E)=\nu(E)<\infty$. If $ \mu = \nu$ on $ \mathcal{A}$, then $ \mu = \nu$ on $ \xi$.
\end{theorem}

\begin{proof}{(Theorem \ref{thm: cond expectation sigma indep})}
    NTS: for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$
    \[
	    \mathbb{E}[X\cdot \mathbf{1}_{F}] =\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}_{F}]
    \]
    Now, set $  \mathcal{ A} = \{A\cap B : A\in \mathcal{ G}, B\in \mathcal{ H}\}$. It is easy to check that $  \mathcal{A}$ is a $ \pi-$system generating $ \sigma( \mathcal{G}, \mathcal{H})$. If $ F = A\cap B$ for some $ A\in \mathcal{G}$ and $ B \in \mathcal{H}$, Then 
    \[
    \begin{array}{ll}
	    \mathbb{E}[X\cdot \mathbf{1}(A\cap B)] &=\mathbb{E}[X\cdot \mathbf{1}(A)\cdot \mathbf{1}(B)] \\
						  & =\mathbb{E}[X\cdot \mathbf{1}(A)]\cdot\mathbb{E}[ \mathbf{1}(B)] \stackrel{H\perp \sigma( \mathcal{G}, \mathcal{H})}{=}\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}(A\cap B)].
    \end{array}
    \]

    Now assume $ X\geq 0$; in the general case, decompose $ X = X^{+}- X^{-}$ and apply same argument to both $ X^{\pm}$. Define the measures $ \mu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ and $ \nu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$. Observe that $ \mu(\Omega) = \nu(\Omega) =\mathbb{E}[X]<\infty$ and we have shown that $ \mu = \nu$ on $ \mathcal{A}$. Thus, $ \mu=\nu$ on $ \sigma( \mathcal{G}, \mathcal{H})$ which finally implies the result 
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]


\end{proof}


\begin{examplesblock}{Examples: }\label{examples: 1}

\begin{enumerate}
	\item 
\begin{boxdef}[Gaussian]\label{def: gaussian dist}
$ (X_{1}, X_{2}, \cdots, X_{n})\in \R^{n}$ has the Gaussian distribution if and only if for all scalars $ a_{1}, a_{2}, \cdots, a_{n}\in \R$, $ a_{1}X_{1}+\cdots a_{n}X_{n}$ has the Gaussian distrubition in $ \R$.
\end{boxdef}

A stochastic process (more on that later) $ (X_{t})_{t\geq 0}$ is a \underline{Gaussian process} if for all $ t_{1}<t_{2}<\cdots t_{n}$ the vector $ (X_{t_{1}}, X_{t_{2}}, \cdots, X_{t_{n}})$ is Gaussian.

Let $ (X,Y)$ be a Gaussian vector in $ \R^{2}$. We compute $\mathbb{E}[X|Y]$.\\ 
Let $ X' =\mathbb{E}[X|Y]$. Looking for $ f$ a Borel measurable function s.t. $ \mathbb{E}[X|Y] = f(Y)$ a.s. Let $ f(y) = ay+b$ for some $ a,b\in \R$ to be determined. Now, $ X' = aY+b$, $\mathbb{E}[X'] =\mathbb{E}[X] = a\mathbb{E}[Y]+b$ and $\mathbb{E}[X'\cdot Y] =\mathbb{E}[X\cdot Y]\implies\mathbb{E}[(X-X')\cdot Y]=0$. Thus $ \text{Cov}(X-X', Y)=0\implies \text{Cov}(X,Y) = a^{2}\text{Var}(Y)$.\\ 

\underline{Need to check:} that for all $ Z$ bounded $ \sigma(Y)-$measurable, $ \mathbb{E}[(X-X')\cdot Z] = 0$.\\ 
Indeed, observe that $ (X-X', Y)$ is a Gaussian vector and since $ \text{Cov}(X-X', Y) = 0\implies X-X'\perp Y\implies (X-X')\perp Z$.

\item Let $ (X,Y)$ be a random vector with density in $ \R^{2}$ with joint density function $ f_{X,Y}:\R^{2}\to \R$. Let $ h:\R\to \R$ be a Borel function such that  $ h(X)$ is integrable. We now compute $\mathbb{E}[h(X)| Y]$.\\ 
We have for all $ g$ bounded $ \sigma{Y}-$measurable functions.
	
\[
\begin{array}{ll}
	\displaystyle\int_{\R^{2}}h(x)g(y)f_{X,Y}(x,y) \diff x \diff y &=  \mathbb{E}[h(X)g(Y)]\\ 
								       &=\mathbb{E}[\mathbb{E}[h(X)|Y]g(Y)] =\mathbb{E}[\phi(Y)g(Y)]\\ 
								       &= \displaystyle\int_{\R^{2}}\phi(y)g(y)f_{Y(y)} \diff y  
\end{array}
\]
where $ f_{Y}(y) = \int_{\R}f_{X,Y}(x,y) \diff x  $ and $ \phi:\R\to \R$ is some Borel measurable function. Hence, 

\[
\phi(y) = \left\lbrace
\begin{array}{@{}l@{}}
    \displaystyle\int_{\R} h(x)\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \diff dx, \quad f_{Y}(y)>0   \\
    0, \quad \text{otherwise}
\end{array}\right.
\]
can be seen to work. Thus, we obtain 
\[
	\mathbb{E}[h(X)|Y] = \phi(Y) \quad \text{a.s.}
\]
	
\end{enumerate}
\end{examplesblock}


\section{Discrete Time Martingales}

\begin{boxdef}[Filtration]\label{def: filtration}
	Let $ (\Omega, \F, \PP)$ be a probability space. A \underline{filtration} is a sequences of increasing sigma sub-algebras of $ \F$, $ (\F_{n})_{n\in \N}$, $ \F_{n}\subseteq \F_{n+1}$ for all $ n\in \N$. We call $ (\Omega, \F, (\F_{n})_{n\in\N})$ a \underline{filtered probability space}.\\ 

	Let $X =  (X_{n})_{n\in \N}$ be a sequence of random variables/a stochastic process. Then it induces $ (\F^{X}_{n})_{n\in N}$, where $ \F^{X}_{n}\coloneqq \sigma(X_{:k\leq n})$ for all $ n\in \N$: the canonical filtration associated to $ X$. We call $ X$ \underline{adapted} to a filtration $ (\F_{n})_{n\in \N}$ if $ X$is $ \F_{n}-$measurable for all $ n\in \N$.$ X$ is called \underline{integrable} if $ X_{n}$ is integrable for all $ n\in \N$.
\end{boxdef}

\begin{boxdef}[Martingale discrete time]\label{def: martingale}
Let $ (\Omega, \F, (\F_{n})_{n\in\N}, \PP)$ be a filtered probability space. Let $ X = (X_{n})_{n\in \N}$be an integrable and adapted process. 
\begin{itemize}
	\item $ X$ is called a \underline{martingale} if $ \mathbb{E}[X_{n}| \F_{m}]=X_{m}$ a.s. for all $ n\geq m$.
	\item $ X$ is called a \underline{super-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\leq X_{m}$ a.s. for all $ n\geq m$.
        \item $ X$ is called a \underline{sub-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\geq X_{m}$ a.s. for all $ n\geq m$.
\end{itemize}

\end{boxdef}

\begin{remark}
	If $  X$ is a (super/sub)martingale with respect to $ (\F_{n})_{n\in\N}$, then it is also a martingale with respect to $ (\F^{X}_{n})_{n\in \N}$. To see this, one has to use the tower property \ref{prop: tower ppty}: $ \F^{X}_{n}\subseteq \F_{n}$ for all $ n\in\N$ implies $ \mathbb{E}[X_{n}|\F^{X}_{m}]= \mathbb{E}[ \mathbb{E}[X_{n}|\F_{m}]|\F^{X}_{m}]$ (since $ \mathbb{E}[X_{n}|\F_{m}]$ a.s.).
\end{remark}

\begin{examplesblock}{Examples: }\label{examples: 2}
\begin{enumerate}
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=0$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \xi_{1}+\cdots +\xi_{n}$ for all $ n\in\N$, $ X_{0} = 0$. $ X$ is a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=1$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \displaystyle\prod^{n}_{i=1}\xi_{i}$ for all $ n\in\N$, $ X_{0} = 1$. $ X$ is again a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.

\end{enumerate}

\end{examplesblock}

\mymark{Lecture 5}Let $ (\Omega, \F, (\F_{n})_{n\in \N}, \PP)$ be a filtered probability space.

\begin{boxdef}[Stopping time discrete time]\label{def: stopping time discrete}
	A \underline{stopping time} $ T$ is a random variable $ T:\Omega\to \Z_{+}\cup \{\infty\}$ s.t. $ \{T\leq n\}\in \F_{n}$ for all $ n\in \N$. Equivalently, if $ \{f=n\}\in \F_{n}$ for all $ n\in \N$ since 
	\[
		\{T=n\}=\underbrace{\{T\leq n\}}_{\F_{n}}\setminus \underbrace{\{T\leq n-1\}}_{\F_{n-1}\subset \F_{n}}\in \F_{n}.
	\]
and 
\[
	\{T\leq n\} =\displaystyle\bigcup^{n}_{k=1}\{T=k\}\in \F_{k}\subset \F_{n}.
\]

\end{boxdef}

\begin{examplesblock}{Examples: }\label{examples: 3}
\begin{enumerate}
	\item Constant time are trivially stopping times.
	\item Let $ X = (X_{n})_{n\in\N}$ be a stochastic process taking values in $ \R$ and $ A\in  \mathcal{B}(\R)$ ($ X$ adapted). Define 
		\[
			T_{A} = \{n\geq 0: X_{n\in A}\}.
		\]
		Then $ \{T_{A}\leq n \} =\displaystyle\bigcup^{n}_{k=0}\{X_{k\in A}\}\in \F_{n} $ for all $ n\in\N$ (with convention $ \inf \emptyset = \infty$).
	\item $ L_{A} = \sup\{n\geq 0: X_{n\in A}\}$ is \underline{NOT} a stopping time.	
\end{enumerate}
\end{examplesblock}

\underline{Properties:} $ S,T, (T_{n})_{n\in\N}$ stopping times. Then $ S\land T, S\lor T$, $ \displaystyle\inf_{n}T_{n}, \displaystyle\sup_{n}T_{n}$, $ \displaystyle\liminf_{n}T_{n}$, $ \displaystyle\limsup_{n}T_{n}$ are also stopping times.

\begin{boxdef}[Stopping time sigma algerbra]\label{def: stopping time sigma algebra}
It $ T$is a stopping time, define 
\[
	\F_{T}=\{A\in \F: A\cap \{T\leq t \}\in \F_{t}\}
\]
Let $ (X_{n})_{n\geq 0}$ be a process. Write $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ for $ \omega \in \Omega$ whenever $ T(\omega)<\infty$. Define the \underline{stopped process:} $ X^{T}_{t}\coloneqq X_{T\land t}$.
\end{boxdef}


\begin{boxprop}\label{prop: stopping time discrete}
	Let $ S$ and $ T$ be stopping times, and let $ X$ be an adapted process, then:
	\begin{enumerate}
		\item If $ S\leq T$, then $ \F_{S}\subseteq\F_{T}$.
		\item $ X_{T}\cdot$ is $ \F_{T}-$measurable.
		\item $ X^{T}$ is adapted. 
		\item If $ X$ is integrable, then the stopped process iss integrable.
	\end{enumerate}
	
\end{boxprop}
\begin{proof}
    \begin{enumerate}
	    \item Immediate from definition.
	    \item Let $ A\in \mathcal{B}(\R)$. Need to show: 
		    \[
			    \{X_{T} \mathbf{1}(T<\infty)\}\cap \{T\leq t\} \in A, \quad \text{ for all }t\geq 0.
		    \]
Indeed, we have that 
\[
	\{X_{T} \mathbf{1}(T<\infty)\} =\displaystyle\bigcup^{t}_{s=0}\underbrace{\{X_{s}\in A\}}_{\F_{s}\subseteq \F_{t}}\cap \underbrace{\{T = s\}}_{\F_{s}}\in \F_{t}.
\]

\item $ X^{T}_{t} = X_{T\land t}$, this being $ \F_{T\land t}-$measurable $ \subseteq\F_{t}-$measurable by $ 1)$, so we conclude it is $ \F_{t}-$measurable.

\item 
	\[
	\begin{array}{ll}
		\mathbb{E}[|X_{t}^{T}|] &=\mathbb{E}[|X_{T\land t}|] \\
					&=\displaystyle\sum^{t-1}_{s=0}\mathbb{E}[|X_{s}|\cdot \mathbf{1}(T = s)]+\mathbb{E}[|X_{t}|\cdot \mathbf{1}(T\geq t)]\\ 
					&\leq\displaystyle\sum^{ t}_{s=0}\mathbb{E}[|X_{s}|]\underbrace{<\infty}_{X_{t} \text{ is integrable}}.
	\end{array}
	\]
	
    \end{enumerate}
    
\end{proof}

We now state and prove a fundamental theorem in Martingale theory: 

\begin{theorem}[Optional Stopping Theorem discrete time]\label{thm: optional stopping discrete time}
Let $ (X_{n}$ be a martingale. 
\begin{enumerate}
	\item If $ T$ is a stopping time, then the stopped process $ X^{T}$ is also a martingale. In particular for all $ t\geq 0$:
		\[
			\mathbb{E}[X_{T\land t}] =\mathbb{E}[X_{0}].
		\]
	\item It $ S\leq T$ are bounded stopping times, then 
		\[
			\mathbb{E}[X_{T}|\F_{S}] = X_{T}, \quad \text{a.s.}
		\]
		and hence $ \mathbb{E}[X_{T}] \mathbb{E}[X_{S}]$.
	\item It there exists an integrable random variable $ Y$ such that $ |X_{n}\leq Y|$ for all $ n \geq 0 $ and $ T$ is finite, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
	\item If there exists $ M\geq 0$ such that $ |X_{n+1}-X_{n}|\leq M$ for all $ n\in \N$ and $ T$ is a stopping time with $ \mathbb{E}[T]<\infty$, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
\end{enumerate}

\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item NTS: for all $ t\geq 0$, $ \mathbb{E}[X_{T\land t}|\F_{t-1}]=X_{T\land t}$ a.s.
		    Indeed, 
\[
\begin{array}{ll}
	\mathbb{E}[X_{T\land t}|\F_{t-1}] &=\displaystyle\sum^{t-1}_{s =0}\mathbb{E}[X_{s}\cdot \mathbf{ 1}(T=s)|\F_{t-1}] \mathbb{E}[X--t]\cdot \mathbf{1}(T\geq t)|\F_{t-1}] \\
					  &=\displaystyle\sum^{ t-1}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t) \quad \text{ a.s.}\\ 
&=\displaystyle\sum^{ t-2}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t-1) \quad \text{ a.s.}\\ 
&= X_{T\land t-1} \quad \text{a.s.}
\end{array}
\]
\item $S\leq T\leq n, n\in \N$ fixed. Let $ A\in \F_{S}$. \underline{NTS:} $ \mathbb{E}[X_{T}\cdot \mathbf{1}(A)] = \mathbb{e}[X_{s}\cdot \mathbf{1}(A)]$. We compute
	\[
	\begin{array}{ll}
	    X_{T}-X_{S} &= (X_{T}-X_{T-1})+\cdots + (X_{S+1}-X_{S}) \\
			&=\displaystyle\sum^{ n-1}_{k=0}(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T). 
	\end{array}
	\]
	Thus, 
	\[
		\mathbb{E}[X_{T}\cdot \mathbf{1}(A)] \stackrel{ (A\in \F_{S}) }{=}\mathbb{E}[X_{S}\cdot \mathbf{1}(A)]+ \displaystyle\sum^{ n-1}_{k=0}\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)]
	\]
	Have, $ A\cap \{S\leq k\}\in \F_{k}$ and $ A\cap \{T>k\}\in F_{k}$. Thus, $ \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)$ is $ \F_{k}-$measurable. Using $ \mathbb{E}[X_{k+1}|\F_{k}]=X_{k}$ a.s. we deduce that 
	\[
	\begin{array}{ll}
		\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)]
		&= \mathbb{E}[\smash{\cancelto{0}{\mathbb{E}[(X_{k+1}-X_{k})|\F_{k}]}}\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)] \\
		&= 0
	\end{array}
\]
Thus, $ \mathbb{E}[X_{T}|\F_{S}]=X_{S}$ a.s. 

\item By the Optional Stopping Theorem applied to $ (X_{T\land n})_{n\geq 0}$, we have 
	\[
		\mathbb{E}[X_{T\land n}] =\mathbb{E}[X_{0}] \quad \text{for all } n\geq 0.
	\]
Now, $ T$ being finite a.s. implies that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ a.s. By assumption, have $ |X_{T\land n}|\leq Y$ a.s. for all $ n\in \N$ and so can apply DCT to conclude.	

\item Observe that for all $ n\geq 1$
	\[
	X_{T\land n}-X_{0} =\displaystyle\sum^{n-1}_{k=0}(X_{k}-X_{0})\cdot \mathbf{1}(T=k)+(X_{n} -X_{0})\mathbf{1}(T\geq n) 
	\]
Hence, we have the bound (using that $ |X_{k+1}-X_{k}|\leq M$ a.s. for all $ k\geq 0$)
\[
\begin{array}{ll}
    \\
|X_{T\land n}-X_{0}|&\leq M\displaystyle\sum^{n-1}_{k=0} k\mathbf{1}(T=k) + n\mathbf{1}(T\geq n)\\
			&\leq\mathbb{E}[T]<\infty \quad \text{a.s.}
\end{array}
\]
Now, $\mathbb{E}[T]<\infty$ gives $ T<\infty$ a.s. and so can deduce as before that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ and use the DCT to conclude the argument. 


 \end{enumerate}
    
\end{proof}


\begin{boxcor}\label{cor: pos supermg bound}
Let $ X$ be a positive superartingale, $ T$ a stopping time such that $ T<\infty$ a.s., then 
\[
	\mathbb{E}[X_{T}]\leq\mathbb{E}[X_{0}].
\]

\end{boxcor}

\begin{proof}
	Use Fatou \ref{lemma: Fatou}: $\mathbb{E}[\displaystyle\liminf_{t\uparrow \infty}X_{T\land t}]\leq \displaystyle\liminf_{t\uparrow \infty}\mathbb{E}[X_{T\land t}]\leq\mathbb{E}[X_{0}]$.
\end{proof}


\begin{examplesblock}{Simple random walk on $ \Z$}\label{def: SRW on ints}
	Let $ (\xi_{i})_{i\geq 0}$ be iid Bernoulli random variables with success probability $ 1/2$. Define the process $(X_{n})_{n\geq 0}$ by setting $ X_{n} = \xi_{1}+\dots+\xi_{n}$ for all $ n\geq 1$ and $ X_{0}=0$. Furthermore, let $ T = \inf\{n\geq 0: X_{n}= 1\}$. Using the analysis below, we will see that $ \PP(T<\infty)=1$. The Optional Stopping Theorem gives $\mathbb{E}[X_{T\land t}]=0$ for all $ t\geq 0$. Yet, $1 = \mathbb{E}[X)_{T}]\neq 0$. We thus see that the condition $\mathbb{E}[T]<\infty$ in $ 4)$ is necessary, since $\mathbb{E}[T] = \infty$. 
	\begin{figure}[H]
	    \centering
	    \includesvg[width=0.6\linewidth]{images/SRW-on-Z.svg}
	    \caption{Illustration of simple random walk (first step) on $ \Z$.}
	    \label{fig: SRW on Z}
	\end{figure}
\end{examplesblock}


\mymark{Lecture 6} We consider again the example of the simple random walk \ref{def: SRW on ints} $ (X_{n})_{n\in \N}$ and define the stopping times 
\[
	T_{c} = \displaystyle\inf {n\geq 0: X_{n = c}}, \quad c\in \Z
\]
Set $ T = T_{-a}\land T_{b}$ for $ a b \in \Z$. We now ask what is $ \PP(T_{-a}\land T_{b})$?\\ 

To answer this, note first that $ X^{T}_{n} = X_{T\land n}$ is a martingale by the Optional Stopping Theorem and we also have the bounded differences $ |X_{n+1}-X_{n}|\leq 1$ for all $ n\geq 1$.\\ 

\underline{Claim:} $\mathbb{E}[T]<\infty$.\\ 
To show this, we will \textit{stochastically dominate} $ T$ be a geometric random variable, which automatically has a finite expectation and then conclude using the non-negativity of $ T$. Now we have that for the sequence $ \xi_{1}, \xi_{2}, \cdots, \xi_{a+b}$ the probability that they all are either $ +1$ or $ -1$ is $ 2\cdot 2^{-(a+b)}$ by independence, call this event $ A_1$. The same is true for the shifted sequence $ \xi_{k(a+b)+1}\cdots \xi_{(k+1)(a+b)}$ for all $ k\in \N$, where we call the corresponding event $ A_{k}$.  \\ 

Thus, we can bound $ T$ by the random variable
\[
	Z(\omega) =\displaystyle\inf\{n\geq 0: \omega \in A_{n}\}  
\]
which has the distribution $ Z\sim Geom(2\cdot2^{-(a+b)})$. Thus, $\mathbb{E}[T]<\mathbb{E}[Z]\leq (a+b)\cdot 2^{a+b-1}<\infty$. Thus, we conclude by the OST that $\mathbb{E}[X_{T}]=\mathbb{E}[X_{0}]=0$. Hence, $ -a\PP(T_{a}<T_{b})+b\PP(T_{b}<T_{-a}) = 0$ and so a quick computation yields that $ \PP(T_{-a}<T_{b}) = \frac{b}{a+b}$.\\ 

\section{Martingale Convergence Theorem}\label{sec: mg conv thm discrete case}

\begin{theorem}[Almost sure martingale convergence theorem]\label{thm: a.s. mg conv thm disc}
	Let $ X$ be a supermartingale bounded in $ \mathcal{L}^{1}$, i.e. satisfying $\displaystyle \sup_{n}\mathbb{E}[|X_{n}|]<\infty$. Then, there exists $ X_{\infty}\in \mathcal{ L}^{1}(\F_{\infty}), \F_{\infty} = \sigma(\F_{n}: n\geq 0)$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow} X_{\infty}$, a.s.
\end{theorem}

Before we embark on the proof of this theorem, we need so me supporting results. First we have a result from analysis and we set up some notation. Let $ x - (x_{n}_{n\in \N})$ be a real sequence and let $ a<b$ be reals. We proceed to define the \textit{number of upcrossings of the sequence } before time $ n\in \N$. Wec constructrecursively the sequence of times:
\[
	\begin{array}{ll}\label{eq: stopping times doob upcrossing}
    T_{0}(x) &= 0 \\
    S_{k+1}(x) &= \displaystyle\inf\{n\geq T_{k}(x): x_{n}\leq a\}\\
    T_{k+1}(x) &= \displaystyle\inf\{n\geq S_{k+1}(x): x_{n}\geq b\}\\
\end{array}
\]

and 
\[
	N_{n}([a,b], X) = \sup\{k\geq 0: T_{k}(x) \leq n\}
\]
Observe that as $ n\to \infty$, $ N_{n}([a,b], x)\uparrow N([a,b], x) = \sup\{k
geq 0: T_{k}(x)<\infty\}$ (see figure \ref{fig: doob upcrossing} for an illustration).

\begin{boxlemma}\label{lemma: upcrossing lemma}
	Let $ X = (X_{n})$ be a real sequence. Then $ X$ converges in $ \overline{\R} = \R \cup \{\pm \infty\}$ if and on ly if for all $ a<b$, $ a, b\in \Q$, $ N([a,b], X)<\infty$.
\end{boxlemma}

\begin{proof}
	\underline{$ \implies:$} Suppose $ x$ converges, if $ a<b$ such that $ N([a,b], x)=\infty$, then $ \displaystyle\liminf_{n}x_{n}\leq a < b \leq \displaystyle \limsup_{n}x_{n}$, a contradiction.\\ 
	\underline{$ \impliedby:$} if not, then $ \displaystyle\liminf_{n}x_{n}< \displaystyle \limsup_{n}x_{n}$ which implies that there exists $ a<b$ in $ \Q$ with $  \displaystyle\liminf_{n}x_{n}< a< b< \displaystyle \limsup_{n}x_{n}$, and hence $ N([a,n], x) = \infty$, a contradiction. 
\end{proof}

Now we state \text{it} Doob's upcrossing Inequality

\begin{boxlemma}[Doob's upcrossing inequality]\label{lemma: doob upcrossing}
Let $ X$ be a supermartingale, then for all $ n \in \N$: 
\[
	(b-a)\cdot\mathbb{E}[N_{n}([a,b], X)]\leq\mathbb{E}[(X_{n}-a)^{-}]
\]
\end{boxlemma}


\begin{proof}
	It is not hard to check that the sequences of times in \ref{eq: stopping times doob upcrossing} are stopping times. Now we have:

\[
\begin{array}{ll}
   &\displaystyle\sum^{n}_{k=1}(X_{T_{k}\land n}-X_{S_{k}\land n})\\
   &=\underbrace{\displaystyle\sum^{N_{n}}_{k=1} (X_{T_{k}}-X_{S_{k}})}_{\geq N_{n}\cdot (b-a)} + (X_{n}-X_{S_{N_{n}+1}}) \mathbf{1}(S_{N_{n}+1}\leq n)
\end{array}
\]
Since $ T_{k\land n}\geq S_{k\land n}$, the OST gives $\mathbb{E}[X_{T_{k}\land n}]\leq\mathbb{E}[X_{S_{k}\land n}]$. Note: 
\[
\underbrace{X_{n}-X_{S_{N_{n}}+1}}_{\geq (X_{n}-a) \land 0 = -(X_{n}-a)^{-}} \mathbf{1}(S_{N_{n}+1}\leq n).\]

Thus, taking expectations on both sides gives:	
	\[
		0 \geq (b-a)\cdot\mathbb{E}[N_{n}]-\mathbb{E}[(X_{n}-a)^{-}].
	\]
	thus concluding the proof.
\end{proof}


\begin{figure}[H]
    \centering
    \includesvg[width=0.8\linewidth]{images/Doob-crossing.svg}
    \caption{Illustration of upcrossings for the process $ (X_{n})_{n\in\N}$.}
    \label{fig: doob upcrossing}
\end{figure}

Now we proceed to the proof of the martingale convergence theorem:

\begin{proof}{(Theorem \ref{thm: a.s. mg conv thm disc})}
Fix $ a<b$, in $ \Q$. Have 
\[
\begin{array}{ll}
	\mathbb{E}[N_{n([a,b], X)}] &\leq(b-a)^{-}\underbrace{\mathbb{E}[(X_{n}-a)^{-}]}_{\leq\mathbb{E}[|X_{n}|+a]} \\
				    &\leq  (b-a)^{-} \left( \displaystyle\sup_{n\geq 0}\underbrace{\mathbb{E}[|X_{n}|]}_{<\infty}+a \right)
\end{array}
\]
Also have $ N_{n}([a,b], X)\uparrow N([a,b], X)$
as $ n\to \infty$. By monotone convergence: $ \mathbb{E}[N([a,b], X)]<\infty$. Set 
\[
	\Omega_{0} =\displaystyle\bigcap_{a<b \\ a,b, \in \Q}\{N([a,b], X)<\infty\}\in \F_{\infty} 
\]
and $ \PP(\Omega_{0})=1$. On $ \Omega_{0}$, $ X$ converges. set
\[
X_{\infty} = \left\lbrace
\begin{array}{@{}l@{}}
	\displaystyle \lim_{n\to \infty}X_{n} \quad \text{ on } \Omega_{0}  \\
	0, \quad \text{ on } \Omega\setminus \Omega_{0}.
    
\end{array}\right.
\]
So, $ X_{\infty}$ is $ \F_{\infty}-$measurable, $X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty} $ a.s. and 
\[
	\mathbb{E}[|X_{\infty}|] =\mathbb{E}[\displaystyle\liminf_{n}|X_{n}| ]\leq \displaystyle\liminf_{\mathbb{E}[X_{n}]}<\infty.  
\]
\end{proof}

\begin{boxcor}\label{cor: a.s. disc conv pos sup mg}
Let $ B$ be a upermaartingale. Then, $ X$ converges a.s.
\end{boxcor}

\begin{proof}
	$\mathbb{E}[|X_{n}|] =\mathbb{E}[X_{n}]\leq\mathbb{E}[X_{0}]$. Apply the martingale convergence theorem to conclude.
\end{proof}

\mymark{Lecture 7} \section{Doob's inequalities}


\begin{theorem}[Doob's maximal inequality]\label{thm: doob maximal ineq discrete}
Let $ X$ be a non-negative submartingale and set $ X^{*}_{n} = \displaystyle\sup_{ 0\leq k \leq n}X_{k}$ . Then for all $ \lambda \geq 0$, 
\[
\begin{array}{ll}
	\lambda \cdot \PP(X^{*}_{n}\geq \lambda ) &\leq\mathbb{E}[X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq \lambda)] \\
						  &\leq\mathbb{E}[X_{n}].
\end{array}
\]


\end{theorem}


\begin{proof}
	Let $ T = \displaystyle\inf\{k\geq 0 : X_{k}\geq \lambda\}$ (it is a stopping time). Then $ \{X^{*}_{n}\geq \lambda\} = \{T\leq n\}$. Also have that $ X_{T\land n}$ is a submartingale by the OST. Then $\mathbb{E}[X_{T\land n}]\leq\mathbb{E}[X_{n}]$. Now, 

	\[
	\begin{array}{ll}
		\mathbb{E}[X_{T\land n}] &= \mathbb{E}[X_{T}\cdot \mathbf{1}(T\leq n)] \\
					 &+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\geq \lambda \cdot \PP(T\leq n)+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\implies \lambda\cdot \PP(T\leq n) \leq \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(\underbrace{T\leq n}_{=\{X^{*}_{n}\geq \lambda\}}) \right]\\ 
					 &\leq \mathbb{E}\left[ X_{n} \right] 
	\end{array}
	\]
	
\end{proof}


\begin{theorem}[Doob's $  \mathcal{L}^{1}$ inequality]\label{thm: doob L1 ineq disc}
Lte $ p>1$ and let $ X$ be a martingale or a non-negative submartingale. Set $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n }|X_{k}|$. Then 

\[
	\norm{X^{8}_{n}}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}.\label{eq: doob L1 disc}
\]

\end{theorem}

\begin{proof}
	By Jensen, it is enough to prove  \ref{eq: doob L1 disc} for a non-negative submartingale. Now, observe that 

	\[
	\begin{array}{ll}
	     &= b \\
		(y\land k)^{p} &= \displaystyle\int_{k}^{0} px^{p-1} \mathbf{1(y\geq x)}\diff x  
			       =\mathbb{E}[ \displaystyle\int_{0}^{k}[x^{p-1} \mathbf{1}(X^{8}_{n}) \diff x  ]\\ 
			       &\stackrel{\text{Fubini}}{=} \displaystyle\int_{0}^{k} px^{p-1}\underline{\PP(X^{*}_{n}\geq x)}_{\leq \frac{1}{x}  \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq x) \right] }\diff x \\ 
			       &\leq \mathbb{E}\left[ \displaystyle\int_{0}^{k} px^{p-2}\cdot \mathbf{ 1}(X^{*}_{n}\geq x) \diff  x \cdot X_{n} \right] \\ 
			       &= \mathbb{E}\left[ \frac{p}{p-1}(X^{*}_{n}\land k)^{p-1}\cdot X_{n} \right]\\ 
			       &\stackrel{\text{H\"{o}lder}}{\leq} \frac{p}{p-1}\cdot \norm{X_{n}}_{p}\cdot \norm{X^{*}_{n}\land k}^{p-1}_{p}. 
	\end{array}
	\]
	So we proved $ \norm{X^{*}_{n}\land k}^{p}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}\cdot\norm{X^{*}_{n}\land k}^{p-1}_{p}$, which implies $ \norm{X^{*}_{n}\land k}_{p}\leq \frac{p}{p-1}\cdot \norm{X_{n}}_{p}$. Now take $ k\to \infty$ and use monotone convergence to conclude the argument.
\end{proof}



\begin{theorem}[$  \mathcal{L}^{p}$-convergence theorem]\label{thm: Lp convergence theorem discrete}
	Let $ X$ be a martingale and $ 1<p<\infty$, then the following are equivalent: 

	\begin{enumerate}
		\item $ X$ is bounded in $ \mathcal{L^{p}}$, i.e. $ \displaystyle\sup_{n\geq 0}\norm{X_{n}}_{p}<\infty $.
		\item $ X$ converges 'underline{almost surely} and in $ \mathcal{L}^{p}$ to a limit $ X_{\infty}\in \mathcal{L}^{p}$.
		\item There exists $ Z\in \mathcal{L}^{p} $ s.t. $ X_{n} = \mathbb{E}\left[Z | \F_{n}  \right]$ a.s.
	\end{enumerate}
	
\end{theorem}


\begin{proof}
\underline{$1) \implies 2)$:} $ X$ bounded in $ \mathcal{L}^{p}$ implies $ X$ is bounded in $ \mathcal{L}^{1} $. So there exists $ X_{\infty}$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ \underline{a.s.} \\ 
	Also, $ \mathbb{E}\left[ |X_{\infty}|^{p} \right] = \mathbb{E}\left[  \displaystyle\liminf_{n}|X_{n}|^{p} \right]\stackrel{\text{Fatou}}{\leq} \displaystyle\liminf_{ \mathbb{E}\left[ |X_{n}|^{p} \right]}<\infty$. Thus, $ X_{\infty}\in \mathcal{L}^{p} $.\\ 

	Now, let $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n}|X_{k}|$, $ X^{*}_{\infty} = \displaystyle\sup_{k\in \N}|X_{k}|$. Thus, 
	\[
		|X_{n}-X_{\infty}|\leq 2X^{*}_{\infty}
	\]
for all $ n\in \N$. Thus, it is enough to show by DCT that $ X^{*}_{\infty}\in \mathcal{L}^{p} $. By Doob's $ \mathcal{L}^{p}- $inequality,
	$\norm{X^{*}_{n}}_{p} &= \frac{p}{p-1}\cdot \displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty $
By MCT ($ X^{*}_{n}\uparrow X^{*}_{\infty}$):
$\norm{X^{*}_{\infty}}_{p}\leq \frac{p}{p-1}\displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty$
Thus, $ X^{*}_{\infty}\in \mathcal{L}^{p} $.

\underline{$2)\implies 3)$:} $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s. and in $ \mathcal{L}^{p} $. Set $ Z = X_{\infty}$. Need to show: $ X_{n} = \mathbb{E}\left[ X_{\infty} | \F_{n}\right]$ for all $ n\in \N$. 
\[
\begin{array}{ll}
	\norm{X_{n}- \mathbb{E}\left[ X_{oo}|\F_{n} \right]}_{p} &\stackrel{m\geq n}{=} \norm{\mathbb{E}\left[ X_{m}-X_{\infty} |\F_{n}\right]}_{p} \\ 
								 &\stackrel{\text{contraction (Jensen)}}{\leq} \norm{X_{m}-X_{\infty}}_{p}\to 0, \quad m\to \infty.
\end{array}
\]
\underline{$3)\implies  1)$:} By conditional Jensen, we can conclude.
\end{proof}

\begin{boxdef}\label{def: lp closed mg disc}
	A martingale of the form $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$, $ Z\in \mathcal{L}^{p} $ is called a martingale \underline{closed in $ \mathcal{L}^{p} $}.
\end{boxdef}

\begin{boxcor}\label{cor: lp closed a.s. conv}
	Let $ Z\in \mathcal{L}^{p} $, $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ Z|\F_{\infty} \right]$ a.s. and in $ \mathcal{L}^{p} $ where $ F_{\infty} = \sigma(X_{n}, n\geq 0)$.
\end{boxcor}

\begin{proof}
By theorem \ref{thm: Lp convergence theorem discrete}, we have $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s. And in $ \mathcal{L}^{p} $. Now, we need to show:
	\[
	X_{\infty} = \mathbb{E}\left[ Z|\F_{\infty} \right]\quad \text{a.s.}
	\]
Now, we have that $ X_{\infty}$ is $ \F_{\infty}-$measurable (being the point wise limit of $ X_{n}, n\geq 0$) and for all $ A\in \F_{\infty}$, $ \mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right]$. Fix $ A\in\displaystyle\bigcup_{n\geq 0}\F_{n} $, a $ \pi-$system generating $ \F_{\infty}$. There exists $ N\in \N$ such that $ A\in \F_{N}$. Let $ n\geq N$, now 
\[
	\mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right]\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right].
\]
\end{proof}

\begin{boxdef}[Uniform integrability]\label{def: UI}
A collection of variables $ (X_{i})_{i\in I}$ is called uniformly integrable (UI) if 

\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(|X_{i}|> M) \right]\stackrel{M\to \infty}{\longrightarrow}0.
\]
\end{boxdef}

Equivalently, $ (X_{i})_{i\in I}$ is UI if $ (X_{i})$ is bounded in $ \mathcal{L}^{1} $ and for all $ \epsilon>0$, there exists $ \delta >0$ such that for all $ A\in \F$ with $ \PP(A)<\delta$, 
\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(A_{i}) \right]<\epsilon.
\]
\begin{itemize}
	\item A UI family is bounded in $ \mathcal{L}^{1} $. 
	\item If a family $ (X_{i})$ is bounded in $ \mathcal{L}^{p} $, $ p>1$, then it is also UI.
\end{itemize}

\begin{boxlemma}\label{lemma: UI a.s. conv}
	Let $ (X_{n})_{n\in \N}$, $ X$ be in $ \mathcal{L}^{1} $ and $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow}$ in $ \mathcal{L}^{1} $ if and only if $ (X_{n})_{n\in \N}$ is UI.
\end{boxlemma}

\begin{theorem}\label{thm: cond exp UI family}
	Let $ X\in \mathcal{L}^{1}$. The family $ \{ \mathbb{E}\left[ X| \mathcal{G} : \mathcal{G} \subset \F  \right]\}$ is uniformly integrable (UI).

\end{theorem}
\begin{proof}
    Need to show for all $ \epsilon>0$, there exists $ 
    \lambda$ large enough such that for all $ \mathcal{G}\subset \F$
    \[
    \begin{array}{ll}
	    &\mathbb{E}\left[ || \mathbb{E}\left[ X \mathcal{G} \right] \cdot \mathbf{1}(|\mathbb{E}\left[ X \mathcal{ G} \right] |>\lambda)\right] <\epsilon\\
	    &\leq \mathbb{E}\left[  \mathbb{E}\left[ |X| | \mathcal{G} \right]\cdot \mathbf{1}(|\smash{\underbrace{\mathbb{E}\left[ X| \mathcal{G}|\right]}_{ \mathcal{G}-\text{measurable}}}|>\lambda) \right].
    \end{array}
    \]

    Since $ X\in \mathcal{L}^{1} $, for all $ \epsilon>0$, there exists $ m\delta >0$ such that if $ A\in \F$, $ \PP(A)<\delta$, then $ \mathbb{E}\left[ |X| \cdot \mathbf{1}(A)\right]<\epsilon$. Now, 
    \[
    \begin{array}{ll}
        \PP(|\mathbb{E}\left[ X \mathcal{G} \right]|>
	\lambda) &\stackrel{\text{Markov}}{\leq} \frac{\mathbb{E}\left[ |\mathbb{E}\left[ X  \mathcal{G} \right]| \right]}{\lambda}\\ 
		 &\leq \frac{\mathbb{E}\left[  \mathbb{E}\left[ |X|  \mathcal{G}\right] \right]
}{\lambda} = \frac{\mathbb{E}\left[ |X| \right]}{\lambda}.
    \end{array}
    \]
    Take $\lambda =  \frac{\mathbb{E}\left[ |X| \right]}{\lambda}$, then we are done.
\end{proof}

\begin{boxdef}\label{def: UI mg}
$ X = (X_{n})_{n\in \N}$ is called UI (super/sub) martingale if it is a (super/sub) martingale and $ (X_{n})_{n\geq 0}$ is UI.
\end{boxdef}


\begin{examplesblock}{Examples:}\label{examples: 4}
	Let $ X_{1}, X_{2}, \cdots$ be an iid sequence with $ \PP(X_{1}=0) = \PP(X_{1}=2)=1/2$. Set $ Y_{n} = X_{1}\cdotX_{2}\cdot \cdots \cdot X_{n}$, which can be seen to be a martingale. Also have $ \mathbb{E}\left[ Y_{n} \right]=1$ for all $ n\in \N$ and $ Y_{n}\stackrel{n\in \N}{\longrightarrow}Y_{\infty}=0$ a.s. by the martingale convergence theorem, not \underline{not} in $ \mathcal{L}^{1} $ (because it is not UI).
\end{examplesblock}


\begin{theorem}\label{thm: UI mg L1 conv them}
Let $ X$ be a martingale. Then the following are equivalent: 
\begin{enumerate}
	\item $ X$ is UI.
	\item $ X$ converges a.s. and in $ \mathcal{L}^{1} $ to $ X_{\infty}$ as $ n\to \infty$.
	\item There exists $ Z\in \mathcal{L}^{1} $ such that $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$ for all $ n\geq 0$.
\end{enumerate}


\end{theorem}

\begin{proof}
	\underline{$ 1)\implies 2)$:} $ X$ is bounded in $ \mathcal{L}^{1} $ implies (by the martingale convergence theorem), $ X_{n}\to $ a.s. Since $ X_{n}$ is UI, then $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $.\\ 

	\underline{$ 2)\implies 3)$:} Set $ Z= X_{\infty}$. Need to show: $ X_{n} = \mathbb{E}\left[ X_{\infty}|\F_{n} \right]$ a.s. Indeed, 
	\[
	\begin{array}{ll}
		\norm{X_{n}-\mathbb{E}\left[ X_{\infty}|\F_{n} \right]}_{1} &\stackrel{m\geq n}{=} \norm{ \mathbb{E}\left[ X_{m}-X_{\infty} |\F_{n}\right]}_{1} \\ 
									    &\leq \norm{X_{m}-X_{\infty}}_{1}\stackrel{m\to \infty}{\longrightarrow} 0. 
	\end{array}
	\]

	\underline{$ 3)\implies 1)$:} The tower property implies $ (X_{n})_{n\in \N}$ is a martingale and the previous theorem implies that $ (X_{n}_{n\in \N})$ is UI.
\end{proof}


\begin{remark}
    \begin{enumerate}
	    \item We get as before, $ X_{\infty} = \mathbb{E}\left[ Z|\F_{\inft} \right]$ a.s., where $ \F_{\infty} = \sigma(X_{n}:n\geq 0)$. 
	    \item It $ X$ were a UI super/sub martingale, then we would get $ \mathbb{E}\left[ X_{\infty}|\F_{n} \right]\stackrel{\geq \text{sub}}{\leq} X_{n}$ (check!).
    \end{enumerate}
    
\end{remark}


$ X$ is UI implies $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $ and a.s. Now let $ T$ be a stopping time. We can then define 
\[
X_{T} =\displaystyle\sum^{\infty}_{n=0}X_{n}\cdot \mathbf{1}(T=n) + X_{\infty}\cdot \mathbf{1}(T=\infty).
\]

\begin{theorem}[Optional stopping theorem for UI martingales]\label{thm: OST for UI mg}
Let $ X$ be a UI martingale and let $ S, T$ be stopping times with $ S\leq T$. Then 
\[\mathbb{E}\left[ X_{T}|\F_{S} \right] = X_{S} \quad \text{a.s.}
\]

\end{theorem}

\begin{proof}
    We know that $ X_{n} = \mathbb{E}\left[ X_{\infty}\F_{n} \right]$ a.s. since $ X$ is UI. It suffices to prove that for any stopping times $ T$, $ \mathbb{E}\left[ X_{\infty}|\F_{T} \right] = X_{T}$ a.s. Indeed, $ \mathbb{E}\left[ X_{T}|\F_{S} \right] = \mathbb{E}\left[ \mathbb{E}\left[ X_{\infty}|\F_{T} \right]|\F_{S} \right]$ and since $ S\leq T$we have $ \F_{S}\subseteq \F_{T}$ and hence the tower property would give: 
    \[
    \mathbb{E}\left[ X_{T}|\F_{S} \right] = \mathbb{E}\left[ X_{\infty}|\F_{S} \right] = X_{S}
    \]
    a.s. 
    Thus, we need to show: for all $ T$ stopping times, $ \mathbb{E}\left[ X_{\infty}|\F_{T} \right] = X_{T}$ a.s. 

    \begin{enumerate}
	    \item \underline{NTS:} $ X_{T}\in \mathcal{L}^{1} $:
		    \[
		    \begin{array}{ll}
		        \mathbb{E}\left[ |X_{T}| \right] &=\displaystyle\sum^{\infty}_{n=0}\mathbb{E}\left[ |X_{n}\cdot \mathbf{1}(T=n)| \right]+ \mathbb{E}\left[ |X_{\infty}|\cdot \mathbf{1}(T=\infty) \right]  \\
							 &\stackrel{\text{have $ X_{n}=\mathbb{E}\left[ X_{\infty}|\F_{n} \right]$}}{\leq}\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[  \mathbb{E}\left[ |X_{\infty}\F_{n}| \right]\cdot \overbrace{\in \F_{n}}^{\mathbf{1}(T=n)} \right]\\ 
							 &+ \mathbb{E}\left[ |X_{\infty}\cdot \mathbf{1}(T=\infty)| \right]\\ 
							 &=\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[ |X_{\infty}|\cdot \mathbf{1}(T=n) \right] + \mathbb{E}\left[ |X_{\infty}\cdot \mathbf{1}(T = \infty)| \right]\\ 
							 &= \mathbb{E}\left[ |X_{\infty}| \right]<\infty
		    \end{array}
		    \]
		    as $ X_{\infty}\in \mathcal{L}^{1} $. It is also not hard to check that $ X_{T}$ is $ \F_{T}-$measurable.
\item \underline{NTS:} for all $ B\in \F_{T}$: $ \mathbb{E}\left[  X_{\infty}\cdot \mathbf{1}(B) \right] = \mathbb{E}\left[ X_{T}\cdot \mathbf{1}(B) \right]$
	\[
	\begin{array}{ll}
		\mathbb{E}\left[ X_{T}\cdot \mathbf{1}(B) \right] &=\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[ X_{n}\cdot \underbrace{\mathbf{1}(T=n)\cdot \mathbf{1}(B)}_{\in \F_{n}} \right]  \\
								  &+ \mathbb{E}\left[ X_{\infty} \\\cdot \mathbf{1}(T=\infty)\cdot \mathbf{1}(B)\right]\\ 
								  &=\displaystyle\sum^{\infty}_{n=0}\mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(T=n) \cdot \mathbf{1}(B)\right]\\ 
								  &= \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(B) \right]
	\end{array}
	\]
	
    \end{enumerate}

\end{proof}


\begin{boxdef}[Backwards martinagles]\label{def: backwards mg}
Let $ \cdots \subseteq \mathcal{G}_{-2}\subseteq \mathcal{G}_{-1}\subseteq \mathcal{G}_{0} $ be a decreasing family of sub sigma algebras of $ \F$. We call $ X = (X_{n})_{n\leq 0}$ a \underline{backwards martingale} if $ X_{o}\in \mathcal{L}^{1} $ and for all $ n\leq -1$ $ \mathbb{E}\left[ X_{n+1}| \mathcal{G}_{n} \right] = X_{n}$ a.s. By the tower property, $ \mathbb{E}\left[ X_{0} | \mathcal{G}_{n} \right] = X_{n}$ for all $ n\leq 0 $. Since $ X_{0}\in \mathcal{L}^{1} $, a backwards martingale is automatically UI. 
\end{boxdef}


\begin{theorem}[ $ \mathcal{L}^{p}$/a.s. backwards martingale convergence theorem]\label{thm: backwards mg conv theorem}
	Let $ X$ be a backwards martingale with $ X_{0}\in \mathcal{L}^{p} $, $ 1\leq p <\infty$. Then $ X_{n}\to X_{-\infty}$ as $ m\to -\infty$ a.s. and in $ \mathcal{L}^{p} $ and $ X_{-\infty} = \mathbb{E}\left[ X_{o}| \mathcal{G}_{-\infty} \right]$ a.s., where $ \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\leq 0} \mathcal{G}_{n} $.
\end{theorem}


\begin{proof}
	Set $ \F_{k} = \mathcal{G}_{-n+k}$, $ 0\leq k \leq n$. This is an increasing filtration and $ (X_{-n+k})_{0\leq k \leq n}$ is $ \F_{k}-$martingale. Let $ N_{-n}([a,b], X)$ be the number of upcrossings of the interval $ [a,b]$ between $ -n$ and $ 0$. Doob's upcrossing inequality gives:
	\[
		(b-a)\cdot \mathbb{E}\left[ N_{-n}([a,b], X) \right]\leq \mathbb{E}\left[ (X_{n}-a)^{-} \right].
	\]
As before, we get that $ X_{n}\to X_{-\infty}$ as $ n\to -\infty$ a.s. We also have $ X_{-\infty} $is $ \mathcal{G}_{-\infty}-$measurable. Also observe that n$ X_{o}\in \mathcal{L}^{p} $ implies $ X_{n}\in \mathcal{L}^{p} $ for all $ n\leq 0$.\\ 

\mymark{Lecture 9} $ X_{n} = \mathbb{E}\left[ X_{n}| \mathcal{G}_{n} \right]$ a.s. (backwards martingale). If $ X_{n}\in \mathcal{L}^{p} $, $ p\in [1,\infty)$ $ X_{n\to X_{-\infty}}$ a.s. $ n\to -\infty$ a.s. and $ X_{-\infty}$ is $ \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\leq 0} \mathcal{G}_{n}-$measurable.

\end{proof}

Observe we have that $ X_{n}\in \mathcal{L}^{p} $ by conditional Jensen and using Fatou, we obtain
$ X_{-\infty}\in \mathcal{L}^{p} $. Now we need to show that $ X_{n}\to X_{-\infty}$ in $ \mathcal{L}^{p} $. Indeed, 

\[
\begin{array}{ll}
    |X_{n}-X_{-\infty}|^{p} &= |\mathbb{E}\left[ X_{0}| \mathcal{G}_{n} \right]-\mathbb{E}\left[ X_{-\infty}| \mathcal{G_{n}} \right]|^{p} \\
			    & = |\mathbb{E}\left[ X_{0]-X_{-\infty}| \mathcal{G}_{n}} \right]|^{p} \\ 
			    &\stackrel{\text{Jensen}}{\leq}\displaystyle\underbrace{\mathbb{E}\left[ |X_{0}-X_{-\infty}|^{p} | \mathcal{G }_{n}\right]}_{\text{UI family}}. 
\end{array}
\]
Hence, $ (|X_{n}-X_{-\infty}|^{p})_{n\leq 0}$ is UI, hence giving $ \mathcal{L}^{1} $ convergence.\\

\underline{NTS:} $ X_{-\infty} = \mathbb{E}\left[ X_{o}| \mathcal{G_{-\infty}} \right]$ a.s.\\ 

Let $ A\in \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\geq 0} \mathcal{G}_{n} $ implies that $A \in \mathcal{G}_{n}$ for all $ n\leq 0$. Hence, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{0}\cdot \mathbf{1}(A) \right]$, for all $ n\leq 0$. Take $ n\to -\infty$ and use $ \mathcal{L}^{1} $ convergence to get $ \mathbb{E}\left[ X_{-\infty}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{0}\cdot \mathbf{1}(A) \right]$ to conclude.


\section{Applications of martingales}\label[{sec: applications of mgs}

\begin{theorem}[Kolmogorov's $ 0-1$ law]\label{thm: Kolmogorov's 0-1}
	Let $ (X_{i})$ be iid and for all $ n\in \N$, $ \F_{n} = \sigma(X_{k}:k\geq n)$, $ \F_{\infty} =\displaystyle\bigcap_{n\geq 0}\F_{n} $. Then, $ \F_{\infty}$ is trivial, i.e. for all $ A'in 'F_{\infty}$, $ \PP(A)\in \{0,1\}$.
\end{theorem}


\begin{proof}
    Let $ A\in \F_{\infty}$. Define $ \mathcal{G_{n } = \sigma(X_{n }: k\leq n)}$ and $ \mathcal{G}_{\infty} = \sigma (\mathcal{G}_{n}, n\geq )$. Now, we have that $ \mathbb{E}\left[ \mathbf{1}(A)| \mathcal{G}_{n} \right]$ is a martingale and 
    \[
	    \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G}_{n}\right] \stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ \mathbf{1}(A)| \mathcal{G}_{\infty} \right] \quad \text{ a.s.}
    \]
    Now, $ A\in \F_{\infty}$ implies that $ A\in \F_{n+1}$ and also have $ \mathcal{G}_{n}\perp \F_{n+1}$ and $ \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G}_{n}\right] = \PP(A)$ a.s., $ \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G_{\infty}} \right] = \mathbf{1}(A)$ a.s. since $ \F_{\infty}\subseteq \mathcal{G}_{\infty}$ implies that $ A\in \mathcal{G}_{\infty}$. So $ \PP(A) = \mathbf{1}(A)$ a.s. finally giving $ \PP(A) \in \{0,1\}$.   
\end{proof}


\begin{theorem}[Strong law of large numbers]\label{thm: slln}
Let $ (X_{i})_{i\in I}$ be an iid sequence in $ \mathcal{L}^{1} $ with $ \mathbb{E}\left[ X_{1} \right]$. Define $ S_{n} = X_{1}+\cdots X_{n}$. Then $ \frac{S_{n}}{n}$ converges a.s. and in $ \mathcal{L}^{1} $ to $ \mu$ as $ n\to \infty$ a.s.
\end{theorem}

\begin{proof}
    Define $\mathcal{G} = \sigma(S_{n}, S_{n+1}\cdots)  = \sigma(S_{n}, X_{n+1}, \cdots)$. For $ n\leq -1$, $ M^{n} = \frac{S_{-n}}{-n}$. We will show that $ (M_{n})_{n\leq -1}$ is a backwards martingale with respect to $ (\mathcal{G}_{-n}_{n\leq -1} )$. Indeed, 
    \[
    \begin{array}{ll}
	    \mathbb{E}\left[ M_{m+1}|\mathcal{G}_{-m}  \right] &= M_{-m}  \quad \text{a.s. for } m\leq -1\\         &= \mathbb{E}\left[ \frac{S_{-m-1}}{-m-1} |\mathcal{G}_{-m}  \right] \stackrel{\tet{set }n = -m}{=} \mathbb{E}\left[ \frac{S_{n-1}}{n-1} | \mathcal{G}_{n}  \right]\\ 
							       & = \mathbb{E}\left[ \frac{S_{n-1}}{n-1}| S_{n-1}, X_{n+1}\cdots \right]\\ 
							       &= \mathbb{E}\left[ \frac{S_{n}-X_{n}}{n-1}S_{n} \right] \\ 
							       & = \frac{S_{n}}{n-1}-\mathbb{E}\left[ \frac{X_{n}}{n-1}|S_{n} \right].
    \end{array}
    \]
    Now since $ S_{n} = X_{1} +\stackrel{\text{iid}}{\cdots}+X_{n}$, we have that $ \mathbb{E}\left[ X_{k}|S_{n} \right]= \mathbb{E}\left[ X_{1}| \right]S_{n}$ and so $ \frac{S_{n}}{n-1}-\frac{1}{n-1} \left( \frac{S_{n}}{n} \right) = \frac{S_{n}}{n-1} \left( \frac{n-1}{n} \right) = \frac{S_{n}}{n}$. Hence $ \frac{S_{n}}{n} \stackrel{n\to \infty}{\longrightarrow} Y$ a.s. and in $ \mathcal{L}^{1} $measurable for all $ k\geq 0$. Thus $ Y$ is $\displaystyle\underbrace{\bigcap_{k} \sigma(X_{k+1}, \cdots )}_{\text{Kolmogorov 0-1 law}\implies \text{trivial}}-$measurable. So there exists $ c\in \R$ such that $ \PP(Y = c)  = 1$. So $ \frac{S_{n}}{n}\stackrel{n\to \infty}{\longrightarrow}$ in $ \mathcal{L}^{1} $ and hence $ c = \mathbb{E}\left[ Y \right] = \lim_{i \to \infty} \mathbb{E}\left[ \frac{S_{n}}{n} \right] = \mu$ and so finally $ c = \mu$. 
\end{proof}

\begin{theorem}[Radon-Nikodym Theorem]\label{thm: radon nokodym thm}
Let $ P$ and $ Q$ be two probability measures on the space $ (\Omega, \F, \PP)$. Suppose that $\F $ is countable generated, i.e. there exists a sequence $ (F_{n})_{n\in \N}$ such that $ \F = \sigma(F_{n}: n\in \N)$. Then the following are equivalent: 

\begin{enumerate}
	\item For all $ A\in \F$, $\PP(A) = 0$ implies $ Q(A) = 0$. $ (Q<<P)$.
	\item For all $ \epsilon >0$, there exists $ \delta>0$ such that if $ A\in \F$ with $ \PP(A)<\delta$, then $ Q(A)<\epsilon$. 
	\item There exists a non-negative random variable $ X$ such that $ Q(A) = \mathbb{E}\left[ X\cdot \mathbf{1}(A) \right]$, for all $ A\in \F$.
\end{enumerate}

\end{theorem}

\begin{remark}
$ X$ is called a version of the \underline{Radon-Nikodym derivative} of $ Q$ with respect to $ P$, or $ X = \frac{\diff  Q}{\diff  P}$ on $ \F $ a.s.
\end{remark}


\begin{proof}
	\underline{$ 1)\implies 2):$} Suppose $ 2)$ does not hold, then there exists an $ \epsilon>0$ such that for all $n\in \N$, there exist $ A_{n}$ with $ P(A_{n})\leq \frac{1}{n^{2}}$ and $ Q(A_{n})\geq \epsilon$. Now, since $\displaystyle\sum^{\infty}_{n=1}P(A_{n})<\infty$ Borel-Cantelli implies $ P(A_{n}\text{ i.o})=0 $ and so $ Q(A_{n}) = 0$. However, 
	\[
	\begin{array}{ll}
	\{A_{n} \text{i.o.} \} &= \bigcap_{n} \bigcup_{k\geq n} A_{k} \implies Q(A_{n}\text{ i.o})  \\
	     &= \lim_{n \to \infty} Q \left( \bigcup_{k\geq n } A_{n}\right)\\ 
	     &\geq  \lim_{n \to \infty} Q(A_{n})\geq \epsilon \,
	\end{array}
	\]
a contradiction.	

\underline{$ 3) \implies 1):$} trivial. \\ 

\underline{$ 2) \implies 3):$} Let $ \mathcal{A}_{n} = \{H_{1}\cap \cdots\cap H_{n}:H_{i} = F_{i} \text{ or } F_{i}^{c} \text{ for all } i \}$. In other words $ \mathcal{A}_{n} = \left\{F_{1}, F_{2}, \cdots, F_{n}, \bigcup_{k\geq n} F_{k}\right\}$. Let $ \F_{N} = \sigma (\mathcal{A}_{n})$, so $ \F_{n}$ is a filtration.\\ 

Now defined
\[
X_{n}(\omega) =\displaystyle\sum_{A\in \mathcal{A_{n}}}\frac{Q(A)}{P(A)} \cdot \mathbf{1}(\omega \in A). 
\]
Thus, for all $ A|in \F_{n}$, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right] = Q(A) = \stackrel{F_{n}\subseteq\F_{n+1}}{=} \mathbb{E}\left[ X_{n+1}\cdot \mathbf{1}(A) \right]
$. So $ (X_{n})_{n\in \N}$ is indeed a martingale. Furthermore $ \mathbb{E}\left[ X_{n} \right] = Q(\Omega)=1$ (and since $ X_{n}\geq 0$ for all $ n\geq 0$), we have that $ X_{n}$ is an $ \mathcal{L}^{1 }$ bounded martingale. Thus, $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s.\\ 

Now we show that $ (X_{n})_{n\in \N}$ is UI: 
\[
\begin{array}{ll}
	\PP(X_{n}\geq \lambda)&\leq 1/\lambda<\infty \\
     &\leq \delta
\end{array}
\]
using Markov's inequality and taking $ \lambda = 1/\delta$. Thus, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(X_{n}\geq \lambda) \right] = Q(X_{n}\geq \lambda)<
epsilon$. Thus $ (X_{n})_{n\in \N}$ is UI and so $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $.\\ 

Now define $ \tilde{Q}(A) = \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right]
$. Want to show: $ \tilde(Q)(A) = Q(A)$ for all $ A\in \F$. Indeed, we have $ X_{n} = X_{\infty}|\F_{n}$. Now if we let for a moment $ A \in \bigcup_{n\geq 0}\F_{n} $, there exists some $ N\in \N$ such that $ A\in \F_{N}$. Thus, 
\[
	\displaystyle\underbrace{\mathbb{E}\left[ X_{N}\cdot \mathbf{1}(A) \right] }_{= Q(A)} =\displaystyle\underbrace{\mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right] }_{=\tilde{Q}(A)}.
\]

Hence, $ Q = \tilde{Q}$ on a $\pi-$system, $ \left( \bigcup_{n} \F_{n} \right)$, that generates $ \F$, and by the extension theorem we have that $ Q \equiv \tilde{Q}$ everywhere.
\end{proof}

\mymark{Lecture 10}\section{Continuous Time processes}\label{sec: cont time processes}
Let $X  =(X_{n})_{n\in \N} $ be a process, that is for all $ n\in \N$ $ X_{n}$ is a random variable on some underlying probability space $ (\Omega, \F ,\PP))$. $ X$ can also be viewed as the map
\[
X:(\omega,n)\mapsto X_{n}(\omega).
\]
and observe that this map is $ \F\otimes \mathcal{P}(\N) = \sigma(\{A\times \{k\}: A\in \F, k\in \N\})$ as long as $ X_{n}$ is $ \F-$measurable for all $ n\in \N$. Now we consider random variables taking values in the spaces $ \R^{d}$, $ d\geq 1$. \\ 

\begin{boxdef}[Stochastic process]\label{def: stoch proces}
	The family $ (X_{t})_{t\in \R_{+}}$ is called a \underline{stochastic process} if for all $ t$ positive $ X_{t}$ is a random variable.
\end{boxdef}


\begin{remark}
    The map $ X:(\omega, t)\mapsto X_{t}(\omega)$ need not be $ \F\otimes  \mathcal{B}(\R_{+})-$measurable. 
\end{remark}

\underline{Claim:} If for all $ \omega \in \Omega$, $ \mapsto X_{t}(\omega)$ is a continuous function for $ t\in(0,1]$, then the map $ X:(\omega, t)\mapsto X_{t}(\omega)$ is $ \F\otimes  \mathcal{B}(\R_{+})-$measurable. \\ 

Indeed, by continuity we can write 

\[
	X_{t}(\omega) = \lim_{n \to \infty}\displaystyle\overbrace{\displaystyle\sum^{2^{n}-1}_{k=0}  \mathbf{1}(t\in (k\cdot 2^{-n}, (k+1)\cdot 2^{-n}])X_{k\cdot 2^{-n}}(\omega) }^{\text{for all }n \text{this sum is} \F\otimes \mathcal{B}((0,1])-\text{meas.} }
\]
Thus $ X$ is measurable with as a limit of measurable functions.\\ 

From now onwards, we will always (unless otherwise stated) assume that $ X$ is right-continuous and admits left limits, almost everywhere. \underline{We call such processes cadlag}.


We now revisit some of the earlier definition we have made in the discrete setting and extend the to the continuous case. A \underline{filtration} is an increasing family of sigma algebras $ (\F_{t})_{t\in \R_{+}}$ whenever $ t\leq t'$. We say $ X$ is adapted to the filtration above if $ X_{t}$ if $ \F_{t}-$measurable for all $t\in \R_{+}$. A random variable $ T:\Omega \to [0,\infty]$ is called a \underline{stopping time} if for all $ t$, $ \{T\leq t\}\in \F_{t}
$. Define $ \F_{T} = \{A|in \F: A\cap \{T\leq t\}\in \F_{t} \text{ for all } t\}$ and $ A|in \mathcal{B}(\R)$. Furthermore, $ T_{A} = \displaystyle\inf_{t\geq 0 : X_{t}\in A}$ is \underline{not} always a stopping time.\\ 
\[
	\{T_{A}\leq t\} = \bigcup_{s\leq t} \{X_{s}\in A\}\,
\]
an uncountable union so not immediately clear whether it in $ \F_{t}$.\\ 

\begin{examplesblock}{Examples: }\label{examples: 5}

		\begin{wrapfigure}{r}{0.3\textwidth}
		    \centering
		    \includesvg[width = 0.8\linewidth]{images/stopping time counterexample.svg}
		    \caption{Illustration of $ X$.}
		    \label{fig: counterexample stopping time}
		\end{wrapfigure}

	Let $ J= \left\lbrace
	\begin{array}{@{}l@{}}
		1, \text{ with probability} \frac{1}{2} \\
		-1, \text{ with probability} \frac{1}{2}
	\end{array}\right.$
	and 
	\[	X_{t}(\omega) = \left\lbrace
	\begin{array}{@{}l@{}}
		t, \quad t\in [0,1] \\
	1+ J(t-1), \quad t>1.
	\end{array}\right.
	\]
	Let $(\F_{t})_{t\geq 0} = (\F^{X}_{t})_{t\geq 0}$ and fix $ A\in (1,2)$. Then $ \{T_{A}\leq 1\}\not in \F_{1} = \{\emptyset, \Omega\}$, since $\{T_{A}\leq 1\} = \{J = 1\}$.
\end{examplesblock}

Again, we say $ X^{T}_{t} = X_{T\land t}$, $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ whenever $ T(\omega)<\infty$.

\begin{boxprop}\label{prop: stopping times continuous}
	Let $ S,T$ be stopping times and $ X$ a cadlag adapted process. Then
\begin{enumerate}
	\item If $ S\leq T$, then $ \F_{S}\subseteq \F_{T}$.
	\item $ S\land T$ is a stopping time.
	\item $ X_{T}\cdot \mathbf{1}(T<\infty)$ is $ \F_{T}-$measurable.
	\item $ X^{T}$ is adapted.
\end{enumerate}

\end{boxprop}


\begin{proof}
    $ 1), 2)$ are clear (check!) and $ 4)$ is immediate from $ 3)$, since $ X_{T\land t}$ if $ \F_{T\land t}-$measurable and $\F_{T\land t} \subseteq \F_{t}$.\\ 

\underline{proof of $ 3)$:} \underline{Claim:} a random variable $ Z$is $ \F_{T\land t}-$measurable if and only if $ Z\cdot \mathbf{1}(T\leq t)$ is $ \F_{t}-$measurable for all $ t\geq 0$. Indeed, \\ 

\underline{$ \impliedby$):} is true by definition. \\ 

\underline{$ \implies):$} if $ Z = c\cdot \mathbf{1}(A)$, $ A\in \F$, then $ A\in \F_{T}$ which means that $ Z$ is $ \F_{T}-$measurable. Now, if $ Z =\displaystyle\sum_{i}c_{i}\cdot \mathbf{1}(A_{i}) $, a finite sum with $ c_{i}>0$, $ A_{i}\in \F$, then $ Z$ is $ \F_{T}-$measurable.\\ 

\underline{$ Z$ general ($ \geq 0$)}: let $ Z_{n}\uparrow Z$, where 
\[
	Z_{n} = 2^{-n}\lfloor 2^n Z\rfloor \land n, \quad \text{ for all } n \in \N.
\]
Observe that $ Z_{n}$ are simple for all $ n$ and so by the previous steps $ Z_{n}$ is $ \F_{T}-$measurable and hence so is $ Z$, being an a.s. pointwise limit of measurable functions.\\ 

The case for completely general $ Z$ follows by decomposing $ Z = Z^{+}-Z^{-}$, $ Z^{+} = Z \lor, Z^{-} = (-Z)\lor 0$ and apply the previous case to $ Z^{+}, Z^{-}$.\\ 

Now, by the above claim, it suffice to show: $ X_{T}\cdot \mathbf{1}(T\leq t)$ if $ F_{t}$ measurable for all $ t$. We have $ X_{T} \mathbf{1}(T\leq t) = X_{T}\cdot \mathbf{1}(T<t)+X_{t}\cdot \mathbf{1}(T=t)$. Hence, it suffices to show  that  $ X_{T}\cdot \mathbf{1}(T< t)$ if $ F_{t}$ measurable for all $ t$.\\ 

Define $ T_{n} = 2^{-n}\lceil 2^n T \rceil $, stopping times since 
\[
\begin{array}{ll}
	\{T_{n}\leq t\} &= \{\lceil 2^n T\rceil \leq 2^n t\} \\
			&= \{2^n T\leq \lfloor 2^n t\rfloor\} = \{T\leq 2^{-n}\lfloor 2^n T\rfloor\}\\ 
			&\in \F_{2^{-n}\lfloor 2^n T\rfloor}\subseteq \F_{t}.
\end{array}
\]
Also, $ T_{n}\downarrow T$, as $ n\to \infty$. Now by the cadlag property of $ X$,\\ 
$ X_{T}\cdot \mathbf{1}(T<t) = \lim_{n \to \infty} X_{T_{n}\land t}\cdot \mathbf{1}(T<t)$.\\ 

Furthermore, $ T_{n}$ takes values in $ \mathcal{D}_{n}= \{k\cdot 2^{-n}, k\in \N\}$. Now, 
\[
\begin{array}{ll}
	X_{T_{n}\land t}\cdot \mathbf{1}(T<t) &=\displaystyle\sum_{d\in \mathcal{D}_{n}, d\leq t} \overbrace{X_{d}\cdot \mathbf{1}(T_{n} = d)\cdot \mathbf{1}(T<t)}^{\F_{t}-\text{meas.}} \\
     &+ X_{t}\cdot \mathbf{1}(T_{n}=t)\cdot \mathbf{1}(T<t).
\end{array}
\]
Hence, $ X_{T}\cdot \mathbf{1}(T<\infty)$ is $ \F_{t}-$measurable as a limit of $ \F_{t}-$measurable functions.


\end{proof}


\begin{boxprop}\label{prop: stopping time cont process}
	Let $ X$ be a continuous and adapted process and let $ A$ be a closed set. Then $ T_{A} = \{t\geq 0: X_{t}\in A\}$ is a stopping time.
\end{boxprop}


\begin{proof}
	\underline{Need to show:} $ \{T_{A}\leq t\} = \left\{ \displaystyle\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0 \right\}$.

	\underline{$ (\subseteq ):$} $ d(x,A)=$distance of $ x$ from $ A$. Let $ T_{A} = s\leq t$, then there exists a sequence $ s_{n}\downarrow s$, such that $ X_{S_{n}}\in A$. Since $ A$ is closed, we have $ d(X_{s}, A) = 0$ and $ X_{s_{n}}\to X_{s}$, as $ n \to \infty$. Again $ A$ being closed implies that $ d(X_{s}, A) = 0$. The continuity of $ X$ and $ d(\cdot, A)$ means that there exists another sequence $ (q_{n})_{n\in \N}\subseteq \Q$ such that $ q_{n}\uparrow s$ such that $ d(X_{q_{n}}, A)\to 0$ hence $\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0$. \\ 

	\underline{($\supseteq  $):} If $\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0$, then there exists a sequence $ (s_{n})_{n\in \N}$ such that $ s_{n}\leq t$ for all $ n$ and $ d(X_{s_{n}, A}\to 0)$ as $ n\to \infty$. Then by compactness, there exists a convergent subsequence of $ s_{n}\to s$ (without relabelling), such that $ s\leq t$ and $ d(X_{s_{n}, A})\to 0$ as $ n\to \infty$ and by continuity we obtain $ d(X_{s}, A) =0$, hence $ X_{s}\in A$ and so $ T_{A}\leq t$.

\end{proof}

\begin{boxdef}\label{def: future sigma algebra}
	Given a filtration $ (\F_{t})_{t\geq 0}$, we define $ \F_{t^{+}}= \bigcap_{s>t} \F_{s}$, for all $ t\geq 0$. Observe that $ (\F_{t^{+}})_{t\geq 0}$ is a filtration. If for all $ t\geq 0$, $ \F_{t^{+}}$, we say $ (\F_{t})_{t\geq 0}$ is right-continuous. 
\end{boxdef}


\mymark{Lecture 11}\begin{boxprop}\label{prop: open set stopping time wrt future filtration}
	Let $ X$ be a continuous process, and $ A$ be an \underline{open} set. Then 
	\[
		T_{A} = \displaystyle\inf\{t\geq 0: X_{t}\in A\}
	\]
	is a stopping time with respect to the filtration $ (\F_{t^{+}})_{t\geq 0}$.
\end{boxprop}


\begin{proof}
	\underline{Need to show: } for all $ t\geq 0$, $ \{T_{A}\leq t\}\in \F_{t^{+}}$. Have, 
\[
\begin{array}{ll}
	\{T_{A}<s\} &= \displaystyle\bigcup_{q\in \Q, q<s}\displaystyle\underbrace{X_{q}\in A}_{\in \F_{s}}\in \F_{s}  \\
	\{T_{A}\leq t\} &= \bigcap_{n}\displaystyle\underbrace{\{T_{A}<t+\frac{1}{n}\}}_{\in \F_{t+\frac{1}{n}}} \in \F_{t^{+}}.
\end{array}
\]
\end{proof}
Let $ (X_{t})_{t\geq 0}
$ be a stochastic process. It can be viewed, as a random element in the space of functions $ \{f:\R_{+}\to E\}$ endowed with the product sigma-algebra making all projections measurable. Further, let $ \mathcal{C}(\R_{+}, E)$ be the space of all continuous functions and $ \mathcal{D}(\R_{+}, E)$ the space of all cad lag functions. Endow the spaces $ \mathcal{C}, \mathcal{D}$ with the sigma algebra that makes all projections $ \pi_{t}: f\mapsto f_{t}$ measurable for all $ t\geq 0$. This sigma algebra is generated by the cylinder sets 
\[
	\left\{ \displaystyle\bigcap_{s\in J} \{f_{s}\in A_{s}: \text{for all } T\subseteq \R_{+}, \text{ finite}, A_{s}\in \mathcal{B}(E)\} \right\}.
\]
For $ A$ in the product sigma algebra, we write $ \mu(A) = \PP(X\in A)$ and we call $ \mu$ the law of $ X$. (``$ X_{*}\PP = \mu$``). For every $ J$ finite subset of $ \R_{+}$, write $ \mu_{J}$ for the law of $ (X_{t})_{t\in J}$. The measures $ (\mu_{J})$ are called the finite dimensional marginals of $ X$. The $ \mu_{J}$ completely characterise the law of $ \mu$. This follows because the sets above form a $\pi-$system that generates the sigma fields previously mentioned.


\begin{examplesblock}{Examples: }\label{examples: 6}
	Let $ X = 0$ for all $ t\in [0,1]$ and $ U\sim [0,1]$ (uniform) and $ X_{t'} = \mathbf{1}(U = t)$ for $ t\in [0,1]$. Both of them have the same finite dimensional distributions which are Dirac masses at zero, but the processes are not \underline{equal}. 
	\[
	\begin{array}{ll}
		\PP(X_{t} = 0 \text{ for all } t\in [0,1])) &= 1 \\
							   \PP(X'_{t} = 0 \text{ for all } t\leq 1) &= 0. \quad \text{ But, }\\ 
 \PP(X_{t } = X'_{t}) &= 1 \quad \text{ for all } t\in [0,1]. 
	\end{array}
	\]
\end{examplesblock}

\begin{boxdef}\label{def: version processes}
Let $ X$ and $ X'$ be two processes on $ (\Omega, \F, \PP)$, we say $ X'$ is a version of $ X$ if ($X_{t} = X'_{t}$ a.s.) for all $ t$. That is 
\[
\text{For all} t\geq 0: \PP(X_{t} = X'_{t})=1.
\]
\end{boxdef}


\begin{boxdef}\label{def: sigma null sets}
Fix a filtered probability space $ (\Omega, \F, (\F_{t})_{t\geq 0}, \PP)$. Set $ \mathcal{N}$ to be the collection of sets of measure zero. Furthermore, set 
\[
	\tilde{\F}_{t} = \sigma(\F_{t}, \mathcal{N})
\]
for all $ t\geq 0$. If for all $ t$, $ \F_{t} = \tilde{\F}_{t}$, we say that $ (\F_{t})_{t\geq 0}$ satisfies the usual conditions. 
\end{boxdef}


\begin{theorem}[Martingale regularisation theorem]\label{thm: mg reg thm}
	Let $ (X_{t})_{t\geq 0 }$ be a martingale wrt $ (\F_{t})_{t\geq 0}$. Then, there exists a cadlag process $ (\tilde{X}_{t})_{t\geq 0}$ satisfying for all $ t\geq 0$:
	\[
		X_{t} = \mathbb{E}\left[ \tilde{X}_{t}| \F_{t} \right] \quad \text{a.s.}
	\]
	and $ X$ is a martingale with respect to the augmented filtration $ (\tilde{\F}_{t})_{t\geq 0}$. If $ (\F_{t)_{t\geq 0}}$ satisfies the usual conditions, then $ \tilde{X}$ is a version of $ X$. 
\end{theorem}

We start with a Lemma
\begin{boxlemma}\label{lemma: mg reg lemma}
Let $ f: \Q_{+}\to \R$ such that for all $ I\subseteq \Q_{+}$ bounded, $ f$ is bounded on $ I$ and for any $ a<b, a,b,\in \Q_{+}$, for all $ I$ bounded and suppose 
\[
\begin{array}{ll}
	\mathcal{N}([a,b], I, f) = \displaystyle\sup\left\{ n\geq 0 : \text{ there exists } 0<s_{1}<t_{1}<\cdots <s_{n}<t_{n},\right. \\ 
	\left.s_{i}, t_{i}\in I \text{ s.t. } f(s_{i})<a, f(t_{i}>b)\right\}<\infty.
\end{array}
\]
Then, for all $ t\geq 0$, the limits 
\[
\lim_{s\uparrow t, s\in \Q_{+}}f(s), \lim_{s\downarrow t, s\in \Q_{+}}f(s)  
\]
exist and are finite.
\end{boxlemma}

\begin{proof}
	Let $ s_{n}\downarrow t$, the sequence $ (f(s_{n}))$ will converge by the finite upcrossing property (see lemma \ref{lemma: upcrossing lemma}). Now suppose $ t_{n}\downarrow t$ is another such sequence, then combining them (by selecting elements from each sequence in an alternating fashion exploiting convergence) we get a decreasing sequence converging to $ t$ to conclude $ \lim_{n \to \infty} f(s_{n}) = \lim_{n \to \infty}f(t_{n})  $. Finally, $ f$ being bounded gives that both limits are equal and finite.
\end{proof}

\underline{Goal:} To define $ \tilde{X}_{t} = \lim_{s\downarrow t, s\in \Q_{+}} X_{s}$ on a set of measure $ 1$, and zero otherwise. We now outline below the main steps in the proof of Theorem \ref{thm: mg reg thm}. 

\underline{Steps:}
\begin{enumerate}
	\item Show that the limit exists and is finite on a set of measure one. 
	\item Show that $ \tilde{X}$ is $ \tilde{\F}_{t}-$measurable and satisfies $ \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ a.s. for all $ t\geq 0$.
	\item $ \tilde{X}$ is a $ (\tilde{\F}_{t})_{t\geq 0}$ martingale. 
	\item $ \tilde{X}$ is cadlag.
\end{enumerate}

\begin{proof}{(Theorem \ref{thm: mg reg thm})}
   \begin{enumerate}
	   \item Let $ I$ be a bounded subset of $ \Q_{+}$. Need to show that $ \PP\left(\displaystyle\sup_{t\in I}|X_{t}|<\infty\right)=1$. Observe that 
		   \[
		   \displaystyle\sup_{t\in I}|X_{t}| = \displaystyle\sup_{J \subseteq I, J \text{ finite}}\displaystyle\sup_{t\in J}|X_{t}|.
		   \]
		   Now, let $ J = \{j_{1}, \cdots, j_{n}\}\subseteq I$ with $ j_{1}<\cdots j_{n}$ and $ k>\displaystyle\sup I$. Then $ (X_{t})_{t\in J}$ is a discrete time martingale. Hence the maximal inequality in \ref{thm: doob maximal ineq discrete} gives
		   \[
		   \lambda \cdot \PP(\displaystyle\sup_{t\in J}|X_{t}|\geq \lambda)\leq \mathbb{E}\left[ |X_{j_{n}}| \right]\leq \mathbb{E}\left[ |X_{k}| \right]
		   \]	
		    by the martingale property and Jensen. Now taking the limit as $ J\uparrow I$, 
	\[
	 \lambda \cdot \PP\left(\displaystyle\sup_{t\in I}|X_{t}|\geq \lambda\right)\leq \mathbb{E}\left[ |X_{j_{n}}| \right]\leq \mathbb{E}\left[ |X_{k}| \right]
	\]
	So, $ \PP \left(\displaystyle\sup_{t\in I}|X_{t}|\geq \lambda \right) = 1$. Now for $ M\in \N$ define $ I_{M} = \Q_{+}\cap [0,M]$, then by the above, 
	\[
	\PP \left( \displaystyle\bigcap_{M\in \N} \left\{ \displaystyle\sup_{t\in I_{M}}|X_{t}|<\infty \right\} \right) = 1
	\]
	and on the above event, $ X_{t}$  is bounded on bounded intervals of $ \Q_{+}$.

	\mymark{Lecture 12} Let $ a<b$, $ a,b \in \Q_{+}$, $ I\subseteq \Q_{+}$, bounded. Observe that
	\[
		\mathcal{N}([a,b], I, X) = \displaystyle\sup_{I \subseteq I, J \text{ finite}} \mathcal{N}([a,b], J, X).
	\]
	Now, let $ J = \{j_{1}, \cdots, j_{n}\}\subseteq I$ with $ j_{1}<\cdots j_{n}$ and $ k>\displaystyle\sup I$. Then $ (X_{t})_{t\in J}$ is a discrete time martingale. Now, Doob's upcrossing inequality from \ref{lemma: doob upcrossing} gives
	\[
	\begin{array}{ll}
		(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b], J, X) \right] &\leq \mathbb{E}\left[ (X_{j_{n}}-a)^{-} \right] \\
	     &\leq \mathbb{E}\left[ (X_{k}-a)^{-} \right].
	\end{array}
	\]
	By monotone convergence, we get 
\[
(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b], I, X) \right]<\infty.
	\]
	Let $ M\in \N$, $ I_{M} = \Q_{+}\cap [0,M]$ and 
 \[
	 \Omega_{0} = \displaystyle\bigcap_{m\in\N} \left( \displaystyle\bigcap_{a<b, a,b\in\Q} \{ \mathcal{N}([a,b], I_{M}, X)<\infty\} \bigcup \left\{ \displaystyle\sup_{t\in I_{m}} |X_{t}|<\infty \right\}\right).
 \]
 On $ \Omega_{0}$, from lemma \ref{lemma: mg reg lemma}, $ \lim_{s \downarrow tX_{s}} $ exists and we have $\PP(\Omega_{0})=1$. Now, define 
 \[
	 \tilde{X}_{t} = \left\lbrace
\begin{array}{@{}l@{}}
	\lim_{s\downarrow t, s\in \Q_{+}} X_{s}, \quad \text{ on }\Omega_{0}    \\
	0 , \quad\quad\quad\quad\quad \text{ otherwise}. 
\end{array}\right. 
 \]
 Recall $ \tilde{\F}_{t} = \sigma(\F_{t}, \mathcal{N})$ for all $ t\geq 0$. From the definition definition, we see that $ \tilde{X}$ is $ \tilde{\F}-$adapted.\\ 

 It remains to show that $X_{t} =  \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ a.s. and $ \tilde{X}$ is cadlag and a martingale. 

\item Let $ t_{n\downarrow} t$, $ t_{n\in \Q_{+}}$, then 
	\[
		\tilde{X}_{t} = \lim_{n\to \infty}X_{t_{n}} 
	\]
	a.s. Observe that $ (X_{t_{n}})$ is a backwards martingale with respect to the filtration $ (\F_{t_{n}})_{n\in \N}$. SO $ (X_{t_{n}})$ converges a.s. and in $ \mathcal{L}^{1} $. In other words, $ X_{t } = in $ $\mathcal{L}^{1} $. So $ X_{t} = \mathbb{E}\left[ \tilde{X}_{t}|\F_{t}\right]$ a.s.

\item We now prove that $ \tilde{X}$ is a martingale. Let $ s<t$, we need to show that $ \mathbb{E}\left[ \tilde{X}_{t}|\tilde{\F}_{s} \right] = 'tilde{X}_{s}$ a.s.\\ 

	\underline{Claim:} $ \mathbb{E}\left[ X_{t}|\F_{t^{+}} \right] = \tilde{X}_{s}$ a.s. Indeed, first observe that for $ Y$ any random variable and $ \mathcal{G}$ a sigma algebra it follows that
	\[
	\mathbb{E}\left[ Y| \sigma \mathcal{G}, \mathcal{N}) \right] = \mathbb{E}\left[ X | \mathcal{G} \right]
	\]
	which is clear because the conditional expectation is defined almost surely and $ \mathcal{N}$ only contains sets of measure zero.\\ 

Now, fix $ s<t$ and let  $ s_{n}\downarrow s$, $ s_{n}\in \Q_{+}$, $ s_{0}<t$. We have by the  tower property that $ (\mathbb{E}\left[ X_{t}|\F_{s_{n}} \right])_{n\in \N}$ is a backwards martingale and so it converges a.s. and in $ \mathcal{L}^{1} $ to $ \mathbb{E}\left[ X_{t}|\F_{t^{+}} \right]$. But $ \mathbb{E}\left[ X_{t}|\F_{s_{n}} \right] = X_{s_{n}}$ a.s. and $ X_{s_{s}}\to \tilde{X}_{s}$ a.s. as $ n'to \infty$. So $ \tilde{X}_{s} = \mathbb{E}\left[ X_{t}|\F_{s^{+}} \right]$.
	
\item Finally, we show that $\tilde{X}$ is a cadlag. First we show that $ \tilde{X}
	$ is right continuous. Suppose \underline{not}. Then, there exists $ \omega \in \Omega_{0}$ and some $ t\geq 0$ such that $ \tilde{X}(\omega)$ is not right continuous at $ t$. That is there exists a sequence $ s_{n}\downarrow t$ such that $ |\tilde{X}_{s_{n}}-\tilde{X}_{t}|\geq \epsilon >0$ (for some positive $ \epsilon$). By the definition of $ \tilde{X}$, there exists another sequence $ s'_{n}>s_{n}$, for all $ n\in \N$ and $ s^{'}_{n}\downarrow t$, $ s'_{n}\in \Q_{+}$ such that $ |\tilde{X}_{s_{n}}-X_{s_{n}'}|\leq \frac{\epsilon}{2}$. So $ |X_{s^{'}_{n}}-\tilde{X}_{t}|\geq \frac{\epsilon}{2}$, a contradiction since $ s'_{n}\downarrow t$, $ s'_{n}\in \Q_{+}$. The argument for left continuity is entirely analogous. 
	
   \end{enumerate}
    
\end{proof}

\begin{examplesblock}{Examples: }\label{examples: 7}
Let $ \xi , \eta$ be independent iid  symmetric Bernoulli with success probability $ 1/2$. Define
\[
X_{t = }\left\lbrace
\begin{array}{@{}l@{}}
    0, \quad t<1 \\
    \xi, \quad t =1 \\ 
    \xi+\eta, \quad t>1.
\end{array}\right.
\]
and let $ \F_{t} = \sigma(X_{s}, s<\leq)$ for all $ t\geq 0$. Observe that $ X$ is an $ (\F_{t})_{t\geq 0}$ martingale. Also, $ \tilde{X}$ satisfies $ X_{t} = \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ where 
\[
	\tilde{X}_{t} = \left\lbrace
	\begin{array}{@{}l@{}}
	    0, \quad t<1 \\
	    \xi+\eta, \quad t\geq 1.
	\end{array}\right.
\]
Furthermore, $ \F_{1} = \sigma(\xi)$ and $ \F_{t} = \sigma(\xi, \eta)$ for all $ t>1$, $ \tilde{X}$ is cadlag with respect to $ \tilde{F}$. Observe finally that $ \F_{1^{+}} = \sigma(\xi, \eta
)$ and so the filtration $ \F$ is not right continuous and $ \tilde{X}$ is not a version of $ X$. We thus see that the right-continuity of $ (\F_{t})_{t\geq 0}$ is necessary in Theorem \ref{thm: mg reg thm}. 
\end{examplesblock}

\begin{theorem}[Almost sure martingale convergence theorem]\label{thm: a.s. mg conv thm cont time}
Let $ X$ be a cadlag martingale bounded in $ \mathcal{L}^{1} $. Then $ X_{t}\to X_{\infty}$ a.s. with $ X_{\infty}\in \mathcal{L}^{1}(\F_{\infty}) $. 
\end{theorem}

\begin{proof}
	Let $ I_{M} = \Q_{+}\cap[0,M]$. Then Doob's upcrossing inequality \ref{lemma: doob upcrossing} from the discrete setting and a monotone convergence argument give for $ a<b, a,b \in \Q_{+}$
	\[
		(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b] , I_{M}, X) \right]\leq a + \displaystyle\sup_{t\geq 0}\mathbb{E}\left[ |X_{t}| \right].
	\]
	Taking $ M\to \infty$ gives $ \mathcal{N}([a,b], \Q_{+}, X)<\infty$ a.s. Hence, for the event
	\[
		\Omega_{0} = \displaystyle\bigcap_{a<b, a,b\in \Q+} \left\{ \mathcal{N}([a,b], \Q_{+}, X)<\infty \right\}
	\]
we have $ \PP(\Omega_{0})=1$ and on $ \Omega_{0}$, $ \lim_{q\to \infty, q\in \Q_{+}} X_{q}$ exists and is finite. We thus have $ X_{\infty} = \lim_{q\to \infty, q\in \Q_{+}} X_{q}$ on $ \Omega_{0}$.  Now for all $ \epsilon>0$, there exists $ q_{0}$ such that $ |X_{q_{0}}-X_{\infty}|\leq \frac{\epsilon}{2}$ for all $ q>q_{0}$, $ q\in \Q_{+}$. Now let $ t>q_{0}$. Then there exists some $ q>t$, $ q\in \Q_{+}$ such that $ |X_{t}-X_{q}|\leq \frac{\epsilon}{2}$ by right continuity of $ X$. So $ |X_{t}-X_{\infty}|\leq\epsilon$.

\end{proof}


\begin{theorem}[Doob's maximal inequality]\label{thm: doob maximal ineq cont time}
Let $ X$ be a cadlag martingale, $ X^{*}_{t} = \displaystyle\sup_{s\leq t}|X_{s}|$. Then for all $ \lambda>0$, 
\[
\lambda\cdot \PP(X^{*}_{t}\geq \lambda)\leq \mathbb{E}\left[ |X_{t}|\cdot \mathbf{1}(X^{*}_{t}\geq \lambda) \right] \leq \mathbb{E}\left[ |X_{t}| \right].
\]

\end{theorem}

\begin{proof}
    Have 
    \[
	    \displaystyle\sup_{s\leq t}|X_{s}| = \displaystyle\sup_{s\in \{t\}\cup(\Q_{+}\cap[0,t])}|X_{s}|
    \]
    and use the beginning of the proof of theorem \ref{thm: mg reg thm}.
\end{proof}


\begin{theorem}[Optional stopping theorem for cadlag UI martingales]\label{thm: ost for UI }
Let $ X$ be a cadlag UI martingale, then for all $ S\leq T$ stopping times 
\[
	\mathbb{E}\left[  X_{T}|\F_{S} \right] = X_{S} \quad \text{ a.s.}
\]
\end{theorem}


\begin{proof}
	Let $ T_{n} = 2^{-n}\rceil 2^nT\rceil$ and $ S_{n} = 2^{-n}\lceil 2^n S\rceil$. Both are stopping times and $ T_{n}'downarrow T$, $S_{n}\downarrow S $ as $ n\to \infty$. \underline{need to show:} for $ A'in \F_{S}$, then $ \mathbb{E}\left[ X_{T}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{S}\cdot \mathbf{1}(A) \right]$. Indeed, $ X_{T_{n}}\to X_{T}$ and $ X_{S_{n}}\to X_{S}$ a.s. as $ n\to \infty$ ($ X$ is right continuous).\\ 

	Now, by the discrete optional stopping theorem applied to the martingale $ (X_{k\cdot 2^{-n}})_{k\in \N}$ with respect to the filtration $ (\F_{K\cdot 2^{-n}})_{k\in \N}$, $ X_{T_{n}}= \mathbb{E}\left[ X_{\infty}|\F_{T_{n}} \right]$, so $ X_{T_{n}}$ is UI  (since $ T_{n}$ take values in $ 2^{-n}\cdot \N$). Thus, $ X_{T_{n}}\to X_{T}$ in $ \mathcal{L}^{1} $, and the same holds for $ X_{S_{n}}\to X_{S}$ using the exact same argument. By the discrete optional stopping theorem, we have that $ \mathbb{E}\left[ X_{T_{n}}|\F_{S_{n}} \right] = X_{S_{n}}$ a.s. Now for $ A\in \F_{S}$, we have that $ A\in \F_{S_{n}}$ for all $ n\in \N$ since $ S_{n}\geq S$. So $ \mathbb{E}\left[ X_{T_{n}}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{S_{n}}\cdot \mathbf{1}(A) \right]$.
\end{proof}


\mymark{Lecture 13}\begin{theorem}[Kolmogorov's continuity criterion]\label{thm: kolmogorov continuity criterion}
	Let $ \mathcal{D}_{n} = \{K\cdot 2^{-n}: 0\leq k \leq 2^{n}\}$ and $ \mathcal{D} = \displaystyle\bigcup_{n\geq 0} \mathcal{D}_{n}$. Let $ (X_{t})_{t\in \mathcal{D}}$ be a stochastic process taking real values. Suppose there exists some $ \epsilon>0$ $ p>0$, such that
	\[
	\mathbb{E}\left[ |X_{t}-X_{s}|^{p} \right]\leq c\cdot |t-s|^{1+\epsilon}, \quad \text{ for all }s,t\in \mathcal{D}
	\]
	where $ c$ is a positive constant. Then for all $ \alpha \in (0, \epsilon/p)$, the process is $ \alpha-$H\"{o}lder continuous, that is there exists a random variable $ K_{\alpha}<\infty$ such that
	\[
	|X_{t}-X_{s}|\leq K_{\alpha}\cdot |t-s|^{\alpha}, \quad \text{ for all } s,t\in \mathcal{D}.
	\]
\end{theorem}



\begin{proof}
\[
	\PP \left( |X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \geq 2^{-n\alpha}\right)\stackrel{\text{Markov + assumption}}{\leq} c\cdot 2^{-n\alpha}p\cdot2^{-n(1+\epsilon)}. 
\]
Thus, 
\[
	\PP \left( \max_{0\leq k \leq 2^n}|X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \geq 2^{-n\alpha} \right)\stackrel{\text{union bound}}{\leq} c\cdot 2^{n\alpha p n\epsilon}, \quad (\alpha \in (0, \frac{\epsilon}{p}).   
\]
By Borel-Cantelli, 
\[
\max_{0\leq k \leq 2^n}|X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \leq 2^{-n\alpha}
\]
for all $ n\in \N$ sufficiently large. Thus, 
\[
 \displaystyle\sup_{ n\geq 0}\max_{0\leq k \leq 2^n} \frac{ |X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}|}{2^{-n\alpha}} \leq 2^{-n\alpha}\leq M(\omega)<\infty
\]
a.s. For some random variable $ M$. \\ 

\underline{Need to show: } there exists some $ M'$ such that $ |X_{t}-X_{s}|\leq M'\cdot |t-s|^{\alpha}$ for all $ s,t \in \mathcal{D}$.\\ 

Let $ s<t$, $ s,t\in \mathcal{D}$ and let $ r$ be the unique integer such that $ 2^{-(r+1)}<t-s\leq 2^{-r}$. Then there exists some $ k\in \N$ such that $ s<k\cdot 2^{-(r+1)}<t$. Now, observe that $ t-\alpha\leq 2^{-r}$ so 
\[
	t-\alpha = \displaystyle\sum^{\infty}_{j=r+1} \frac{x_{j}}{2^j}, \quad x_{j}\in \{0,1\}
\]
and 
\[
	\alpha-s = \displaystyle\sum^{\infty}_{j=r+1} \frac{y_{j}}{2^j}, \quad y_{j}\in \{0,1\}.

\]
Observe that $ [s, t)$ is a disjoint union of dyadic intervals each of them having length $ 2^{-n}$ with $ n\geq r+1$ and each interval of length will appear at most twice. Thus, we get the bound
\[
\begin{array}{ll}
|X_{t}-X_{s}| &\leq\displaystyle\overbrace{\sum_{d,n}\displaystyle\underbrace{|X_d-X_{d+2^{-n}}|}_{\leq 2^{-n\alpha\cdot M}}}^{d,n \text{ is the endpoint of a dyadic interval in the decomposition of }[s,t) \text{ of length } 2^{-n}}  \\
	      &\leq 2\cdot M \cdot\displaystyle\sum^{\infty}_{n =  r+1} 2^{-n\alpha} = \frac{2M\cdot 2^{-(r+1)\alpha}}{1-2^{-\alpha}} < \frac{2M}{1-2^{-\alpha}}|t-s|^{\alpha}.
\end{array}
\]
\end{proof}

\section{Weak Convergence}\label{sec: weak convergence}

We fix $  (\mathcal{M}, d)$ a metric space endowed with its Borel sigma algebra.

\begin{boxdef}\label{def: weak convergence of measures}
Let $ (\mu_{n})_{n\in \N}$ be a sequence of probability measures on $ \mathcal{M}$. We say $ (\mu_{n})_{n\in \N}$ converges weakly to $ \mu$ and write $\mu_{n}\implies \mu$ as $ n\to \infty$ if 
\[
	\mu_{n}(f) \coloneqq \int_{\mathcal{M}} f(x) \mu_{n}(\diff  x) \stackrel{n\to \infty}{\longrightarrow} \int_{ \mathcal{M}} f(x) \mu(\diff  x) \coloneqq \mu(f)
\]
for any $ f$ continuous and bounded.

\end{boxdef}


\begin{examplesblock}{Examples: }\label{examples: 8}
\begin{enumerate}
	\item Let $ x_{n}\to x$ as $ n\to \infty$ in $ ( \mathcal{M}, d)$ then $ \delta_{x_{n}}\stackrel{n\to \infty}{\longrightarrow}\delta_{x}$ ,since $ \delta_{x_{n}}(f)= f(x_{n})\stackrel{n\to \infty}{\longrightarrow}f(x) = \delta_{x}(f)$. 
	\item Let $  \mathcal{M} = [0,1]$, with the Euclidean metric and its Borel sigma algebra. Let $ \mu_{n} = \frac{1}{n}\displaystyle\sum_{0\leq k\leq n}\delta_{k/n} $. Then $ \mu_{n}$ converges weakly to the Lebesgue measure. Indeed, $ \mu_{n}(f) = \frac{1}{n}f(k/n)\stackrel{n\to \infty}{\longrightarrow}\int f(x) \diff x$, being Riemann sums.
	\item $ \mu_{n} = \delta_{\frac{1}{n}}\implies \delta_{0}$, as $ n\to \infty$. Notice however that for $ A = (0,1)$, $ \mu_{n}(A) = $ for all $ n\geq 0$ and so $ \nu_{n}(A) \cancel{\rightarrow} \delta_{0}(A) = 0$.
\end{enumerate}

\end{examplesblock}


\begin{theorem}\label{thm: weak convergence equivalence}
Let $ (\mu_{n})_{n\in \N}$ be a sequence of probability measures on $ ( \mathcal{M}, d)$. Then the following are equivalent: 
\begin{enumerate}
	\item $ \mu_{n}\implies \mu$.
    \item For all $ G$ open, $ \displaystyle\liminf_{n}\mu_{n}(G)\geq \mu(G)$.
    \item For all $ A$ closed, $ \displaystyle\limsup_{ n} \mu_{n}(A)\leq \mu(A)$.
    \item For all $ A$ with $ \mu(\partial A) = 0$, then $ \mu_{n}(A)\to \mu(A)$. 
\end{enumerate}

\end{theorem}


\begin{proof}
	\underline{$ 1\implies 2$:} Let $ G$ be open with $ G^{c}\neq \emptyset$. Let $ M>0$ and set $ f_{M}(x) = \mathbf{1}(M d(x, G^{c}))\leq \mathbf{1}(x\in G)$. Observe that 	$ f_{M}(x)\uparrow \mathbf{1}(x\in G)$ as $ M\to \infty$, $ f_{M}$ is bounded and continuous for all $ M$. So $ \mu_{n}(f_{M})\to \mu(f_{M})$ as $ n\to \infty$ for all $ M$. Thus, 
	\[
		\displaystyle\liminf_{n}\mu_{n}(G)\geq \displaystyle\liminf_{n}\mu_{n}(f_{M}) = \mu(f_{M})\stackrel{\text{monotone convergence}}{\to}\mu(G).
	\]
	\underline{$2\implies 3$:} follows from the previous case by taking complements. 
	\underline{$ 2,3\implies 4$:} $ 0 = \mu(\partial A) = \mu(A\setminus \intr{A})$, hence $ \mu(\overline{A}) = \mu(A) = \mu(\intr{A})$.\ \ 
	\underline{$ 2:$} $ \displaystyle\liminf_{n}\mu(\int{A})\geq \mu(\intr{A})=\mu(A)$.
	\underline{$ 3:$} $ \displaystyle\limsup_{n} \mu_{n}(\overline{A})\leq \mu(\overline{A}) = \mu(A)$.

	\underline{$ 4\implies 1$:} Need to show for any $ f$ continuous and bounded, $ \mu_{n}(f)\to \mu(f)$. We can assume further that $ f\geq 0$. Fix $ K> \displaystyle\sup f$. Have, 
	\[
	\begin{array}{ll}
		\displaystyle\int_{ \mathcal{M}}f(x) \mu_{n}(\diff x)    &= \displaystyle\int_{ \mathcal{M}} \left( \int^{K}_{0} \mathbf{1}(t\leq f(x)) \diff t \right) \mu_{n}(\diff  x)  \\
									 &\stackrel{\text{Fubini}}{=} \int^{K}_{0}\mu_{n}(f\geq t) \diff t.  
	\end{array}
	\]
	It suffices to show $ \mu_{n}(f\geq t)\to \mu(f \geq t)$ as $ n\to \infty$. Since then we can conclude using dominated convergence. Thus it suffices to show that $ \mu(\partial \{f\geq t\}) = 0$. Indeed, 
	\[
		\partial\{f\geq t \} \subset \{f = t\}.
	\]
	since $ f$ is continuous and $ \{f>t\}$ is open and $ \subset \int\{f\geq t\}$. Also observe that there exists an at most countable number of $ t$ such that $ \mu(f = t)>0$. Thus, 
	\[
	\{t: \mu(f=t)>0\} = \displaystyle\bigcup_{n}\displaystyle\underbrace{\{t: \mu(\{f = t\})\geq \frac{1}{n}\}}_{\#\leq n} .
	\]
	Thus, $ \partial \{f\geq t\}$ is countable and has Lebesgue measure zero.
\end{proof}

Now, let $ \mathcal{M} = \R$. Let $ \mu$ be a probability measure on $ \R$. We define the distribution function of $ \mu$ to be the function $ F_{\mu}:x\mapsto \mu((-\infty,x] )$, $ F_{\mu}\R\to [0,1]$. 


\begin{boxprop}\label{prop: weak convergence distribution function}
	Let $ (\mu_{n})_{n\in \N}$. be a sequence of probability measures on $ \R$. Then the following are equivalent: 
	\begin{enumerate}
	    $ \mu_{n}\implies \mu$, as $ n\to \infty$.
	    \item $F_{\mu_{n}}(x)\stackrel{n\to \infty}{\longrightarrow}F_{\mu}(x)$ for all $ x\in \R$ continuity points of $ F_{\mu}$.
	\end{enumerate}
\end{boxprop}

\begin{proof}
	\underline{$ 1\implies 2$:}  Let $ x$ be a continuity point of $ F_{\mu}$. Have $ F_{\mu_{n}}(x) = \mu_{n}((-\infty, x])$ and 
	\[
	\begin{array}{ll}
		\mu(\partial(-\infty, x]) &= \mu(\{x\}) \\
					  &= \mu((-\infty, x])- \lim_{n \to \infty} \mu((-\infty, x-\frac{1}{n}])\\ 
					  & = F_{\mu}(x)- \lim_{n \to \infty} F_{\mu}(d-\frac{1}{n}) = 0
	\end{array}
	\]
	since $ x$ is a continuity point of $ F_{\mu}$. 
\end{proof}

\underline{$ 2\implies 1$:} Let $ G$ be an open set in $ \R$. Then $ G = \displaystyle\bigcup_{n} (a_{k}, b_{k})$, a union of disjoint open intervals. Now, 

\[
\begin{array}{ll}
    \displaystyle\liminf_{n}\mu_{n}(G) &= \displaystyle\liminf_{n}\displaystyle\sum_{k}\mu_{n}(a_{k}, b_{k})  \\
				       &\stackrel{\text{Fat}}{\geq}\displaystyle\sum_{k} \displaystyle\liminf_{n} \mu_{n}(a_{k}, b_{k}). 
     &= +d
\end{array}
\]
So it suffices to show that $ \displaystyle\liminf_{n} \mu_{n}(a,b)\geq \mu(a,b)$ for all $ a<b\in \R$.\\ 

Indeed, We have $ \mu_{n}((a,b)) = F_{\mu_{n}}(b-)-F_{\mu_{n}}(a)$ and since $ F_{\mu}$ is non-decreasing and has at most countably many discontinuities, there exist $ a', b'$ continuity points of $ \F_{\mu}$. Hence, $ F_{\mu_{n}}(a') \stackrel{n\to \infty}{\longrightarrow} F_{\mu}(a')$ and $ F_{\mu_{n}}(b') \stackrel{n\to \infty}{\longrightarrow} F_{\mu}(b')$. This means that
\[
    \displaystyle\liminf_{n}\mu_{n}((a,b)) \geq F_{\mu}(b')-F_{\mu}(a').
\]
By the density of continuity points, there exist $(b'_{m})_{m\in \N}$, such that $ b'_{m}\uparrow b'$ and $ (a'_{m})_{m\in \N}$, $ a'_{m} \downarrow a'$ all continuity points. Thus, 
\[
\begin{array}{ll}
	\displaystyle\liminf_{n} \mu_{n}((a,b)) &\geq \displaystyle\sup_{n}F_{\mu_{n}}(b'_{m})-F_{\mu}(a'_{m})\\
     &= F_{\mu}(b-)-F_{\mu}(a) = \mu((a,b)).
\end{array}
\]
\begin{boxdef}\label{def: weak conv law}
Let $ (X_{n})_{n\in \N}$ be a sequence of random variables taking values in $ ( \mathcal{M}, d)$, defined on probability spaces $ (\Omega_{n}, \F_{n}, \PP_{n})$. We say that $ (X_{n})_{n\in \N}$ converges weakly (or in distribution) to a random  variable $ X$ defined on $ (\Omega, \F, \PP)$ if $  \mathcal{L}(X_{n})\implies \mathcal{L}(X)$ (i.e. the laws converge weakly). 
\end{boxdef}


\begin{remark}
	Equivalently, $ X_{n} \stackrel{w/d}{\implies}X$ if for all $ F$ continuous and bounded, $  \mathbb{E}_{\PP_{n}}\left[ f(X_{n}) \right]\to \mathbb{E}_{\PP}\left[ f(X) \right]$, as $ n\to \infty$.
\end{remark}

\begin{boxprop}\label{prop: conv in prob conv in dist}
\begin{enumerate}
	\item If $ X_{n}\stackrel{\PP}{\implies} X$ as $ n\to \infty$, then $ X_{n}\stackrel{d}{\implies} X$ as $ n\to \infty$. 
	\item If $  X_{n}\stackrel{d}{\implies} c$, $ c$ a constant, then $  X_{n}\stackrel{\PP}{\implies} c$
\end{enumerate}

\end{boxprop}


\begin{examplesblock}{Examples: (CLT)}\label{examples: 8}
	Let $(X_{n})_{n\in \N}$ be iid and $ \mathbb{E}\left[ X_{1} \right] = m$ and $ \sigma^{2} = \text{Var}(X_{1})$. Then with $ S_{n} =\displaystyle\sum^{n}_{i=1}X_{i} $
	\[
		\frac{S_{n}-n\cdot m}{\sqrt{n\sigma^{2}}} \stackrel{d}{\longrightarrow} \mathcal{N}(0,1)
	\]
	as $ n\to \infty$.
\end{examplesblock}

\begin{boxdef}[Tightness]\label{def: waek conv tightness}
	Let $ ( \mathcal{M}, d)$ be a metric space. A sequence of probability measures $ (\mu_{n})_{n\in \N}$ on $ \mathcal{M}$ is called \underline{tight} if for all $ \epsilon>0$, there exists a compact set $ K\subseteq \mathcal{M}$ such that 
\[
 \displaystyle\sup_{n\geq 0}\mu( \mathcal{M}\setminus K)\leq \epsilon.
\]
\end{boxdef}


\begin{remark}
    It $ \mathcal{M}$ is compact, then all sequences of probability measures are tight. 
\end{remark}

\begin{theorem}[Prohorov]\label{thm: prohorov}
Let $ (\mu_{n})_{n\in\N}$ be a tight sequence of probability measures, then there exists a subsequence $ (\mu_{n_{k}})_{k\in \N}$ and a probability measure $ \mu$ such that 
\[
	\mu_{n_{k}} \stackrel{d}{\implies} \mu, \quad \text{ as }k\to \infty.
\]
\end{theorem}

\begin{proof}   

We focus on the case $ \mathcal{M} = \R$. Let $ \Q = \{x_{1}, x_{2}, \cdots\}$ be an enumeration of $ \Q$ and $ F_{n} = F_{\mu_{n}}$. Then, the sequence $ (F_{n}(x_{1}))_{n\in \N}$ in $ [0,1]$ has a convergent subsequence $F_{n^{(1)}_{k}}(x_{1})\stackrel{k\to \infty}{\longrightarrow} F(x_{1})$ by compactness. So does $ (F_{n^{(1)}_{k}}(x_{2}))_{k\in \N}$. Thus, continuing so inductively, we obtain for all $ i\in \N$ that there exist sequences $ (n^{(i)}_{k})_{k\in \N}$ such that 
\[
F_{n^{(i)}_{k}}(x_{j}) \stackrel{k\to \infty}{\longrightarrow}F(x_{j}), \quad \text{ for all } 1\leq j\leq i.
\]
Thus, we can extract a diagonal sequence $ (m_{k})_{k\in \N}$, where $ m_{k} = n^{(k)}_{k}$ for all $ k\in \N$ and Have 
\[
F_{m_{k}}(x) \stackrel{k\to \infty}{\longrightarrow} F(x), \quad \text{ for all } x\in \Q.
\]
Observe now that the functions $ F_{m_{k}}$ are non-decreasing, and so $ F$ is non-decreasing, so for $ x\in \R$ define $ F(x) = \lim_{q\downarrow x, q\in \Q} F(q)$. Thus, $ F$ is right continuous, non-decreasing and so $ F$ has left-limits.\\ 

Let $ x\in \R$ be a continuity point of $ F$. We need to show that $ F_{m_{k}}(x)\stackrel{k\to \infty}{\longrightarrow}F(x)$. Indeed, for any $ \epsilon >0$, there exist $ s_{1}<x<s_{2}$, $ s_{i}\in \Q$ such that  $F(s_{i})-F(x)|<\epsilon/2$ (since $ F$ is continuous at $ x$). We now have the chain of inequalities
\[
	F(x)-\epsilon\leq F(s_{1})-\frac{\epsilon}{2}\leq F_{m_{k}}(s_{1})\leq F_{m_{k}}(x)\leq F_{m_{k}}(s_{2}\stackrel{\text{conv. in } \Q}{\leq} F(s_{2})+\frac{\epsilon}{2}\leq F(x)+\epsilon
\]
for all $ k\in \N$ sufficiently large.\\ 

Finally, it remains to show that  there exists some probability measure $ \mu$ such that $ F = F_{\mu}$. Indeed, by tightness, we have that for all $ \epsilon>0$, there exists $ N\in \R$ large enough so that (with $ \pm N$ being continuity points of $ F$)
\[
	\displaystyle\sup_{n\geq 0 }\mu_{n}([-N, N]^{c})\leq \epsilon.
\]
Thus, $ F(-N)\leq \epsilon$ and $ 1-F(N)\leq \epsilon$. This guarantees that 
\[
\lim_{x\to -\infty} F(x) = 0, \quad \lim_{x \to \infty} F(x) = 1.
\]
Finally, define define $ \mu((a,b]) = F(b)-F(a)$. Then, $\mu$ can be extended to the Borel sigma algebra by Calathea dory's extension theorem.
\end{proof}

\begin{boxdef}\label{def: charateristic function}
Let $ X$ be a random variables with values in $ R^{d}$. The characteristic function of $ X$ is defined as 
\[
	\phi_{X}(u) = \mathbb{E}\left[ e^{i\bracket{u}{X}} \right], \quad u\in \R^{d}.
\]
\end{boxdef}
\underline{Properties of $ \phi_{X}$:}
\begin{enumerate}
	\item $ \phi_{X}$ is continuous on $ \R^{d}$ and $ \phi_{X}(0)=1$.
	\item $ \phi_{X}$ completely determines the law of $ X$, that is if $ \phi_{X}(u) = \phi_{Y}(u)$ for all $ u\in \R^{d}$, then $  \mathcal{L}(X) = \mathcal{L}(Y)$.
\end{enumerate}

\mymark{Lecture 15} \begin{theorem}[L\'{e}vy's convergence theorem]\label{thm: levy conv thm}
Let $ (X_{n})_{n\in \N}$, $ X$ be random variables taking values in $ \R^{d}$. Then 
\begin{enumerate}
	\item $ \mathcal{L}(X_{n})\implies \mathcal{L}(X)$  as $ k\to \infty$, then $ \phi_{X_{n}}(u) \stackrel{n\to \infty}{\longrightarrow}\phi_{X}(u)$ for all $ u\in \R^{d}$. 
	\item Suppose there exists $ \psi:\R^{d}\to \C$ such that $ \psi(0) = 1$, $ \psi$ is continuous at zero and  $ \phi_{X_{n}}(u) \stackrel{n\to \infty}{\longrightarrow}\psi(u)$ for all $ u\in \R^{d}$. Then there exists a random variable $ X$ with characteristic function $ \psi = \phi_{X}$ and $ \mathcal{L}(X_{n})\implies \mathcal{L}(X)$. 
 
\end{enumerate}

\end{theorem}

Before we proceed with the proof of the theorem, we state a Lemma
\begin{boxlemma}\label{lemma: characteristic function bound}
Let $ X$ be a random variable in $ \R^{d}$. Then, for all $ K>0$, 
\[
	\PP(\norm{X}_{\infty})\leq C\cdot \left( \frac{K}{2} \right)^{d} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X}(u))  \diff  u, 
\]
where $ C = (1-sin(1))^{-1}$.
\end{boxlemma}

\begin{proof}
    Fix $ \lambda>0$ and let $ \mu = \mathcal{L}(X)$. Then,
    \[
    \begin{array}{ll}
	    \displaystyle\int_{[-\lambda, \lambda]^{d}}\phi_{X}(u) \diff u   &= 
 \displaystyle\int_{[-\lambda, \lambda]^{d}} \left( \displaystyle\int_{\R^{d}} \prod^d_{j=1}e^{iu_{j}\cdot x_{j}}\mu(\diff x)   \right) \diff   u \\
									     &\stackrel{\text{Fubini}}{=} \displaystyle\int_{\R^{d}} \mu(\diff  x) \displaystyle\prod^d_{j=1} \left( \displaystyle\int_{[-\lambda,\lambda]}e^{iu_{j}\cdot x_{j}} \diff u_{j} \right) \\ 
									     &= \displaystyle\int_{\R^{d}} \mu(\diff  x) \displaystyle\prod^d_{j=1} \left( \frac{e^{ix_{j}\lambda}-e^{-ix_{j}\lambda}}{ix_{j}} \right) \\ 
									     &= \displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \frac{2\cdot \sin(\lambda x_{j})}{x_{j}} \mu( \diff  x)\\ 
									     & = (2\lambda)^{d}\displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \left(\frac{2\cdot \sin(\lambda x_{j})}{\lambda x_{j}}\right) \mu( \diff  x).

    \end{array}
    \]
     Thus, 
     \[
     \displaystyle\int_{[-\lambda, \lambda]^{d}}(1-\phi_{X}(u)) \diff u  = (2\lambda)^{d}\displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \left(1- \frac{2\cdot \sin(\lambda x_{j})}{\lambda x_{j}}\right) \mu( \diff  x)
\]
     Now, let $ f(u) = \displaystyle\prod^d_{j=1} \left(\frac{2\cdot \sin(u_{j})}{u_{j}}\right)$, $ f:\R^{d}\to \R$. \\ 
	     \underline{Claim: } not hard to see that if $ x\geq 1$, then $ \left| \frac{\sin(x)}{x} \right|\leq \sin(1)$. Hence, if $ \norm{u}_{\infty}\geq 1$, then $ |f(u)|\leq \sin(1)$. So $ \mathbf{1}(\norm{u}_{\infty}\geq 1)\leq C\cdot(1-f(u))$, where $ C = (1-\sin(1))^{-1}$. Hence, 
	     \[
		     \PP(\norm{X}_{\infty}\geq k)\leq C\cdot \mathbb{E}\left[ 1- f \left( \frac{X}{K} \right) \right]
	     \]
and by simple scaling, one can conclude for the general case.
\end{proof}

\begin{proof}{(Theorem \ref{thm: levy conv thm})}
   \begin{enumerate}
	   $ f(x) = e^{i\bracket{u}{x}}$ is continuous and bounded so by bounded convergence, have 
	   \[
	   \phi_{X_{n}}(u) = \mathbb{E}\left[ f(X_{n}) \right]\to \mathbb{E}\left[ f(X) \right]
	   \]
	   as $ n\to \infty$. 
	   \item First we prove that $  \mathcal{L}(X_{n}))_{n\in \N}$ is tight. By Lemma \ref{lemma: characteristic function bound}, have that 
		   \[
		   	\PP(\norm{X_{n}}_{\infty})\leq C\cdot \left( \frac{K}{2} \right)^{d} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X_{n}}(u))  \diff  u
		   \]
and $ |1-\phi_{X_{n}}(u)|\leq 2$ for all $ u\in \R^{d}, n\in \N$. Thus, by dominated convergence, 
\[
 \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X_{n}}(u))  \diff  u \stackrel{n\to \infty}{\longrightarrow} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\psi(u))  \diff  u. 
\]
Since $ \psi$ is continuous at zero and $ \psi(0) = 1$, taking $ K$ large enough we get 
\[
\displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\psi(u))  \diff  u <\frac{\epsilon}{2^d Cd}(2K^{-1})^{d}.
\]
Thus, $ \PP(\norm{X_{n}}_{\infty}\geq K)\leq \epsilon$ for all $ n\in \N$ sufficiently large. Taking $ K$ possibly even larger, we conclude that 
\[
	\displaystyle\sup_{n\geq 0}\PP(\norm{X}_{\infty}\geq K)\leq \epsilon,
\]
hence showing that $ ( \mathcal{L}_{n})_{n\in \N}$ is tight. By Pro horror, there exists a subsequence $ (n_{k})_{k\in \N}$ such that
\[
 \mathcal{L}(X_{n_{k}}) \stackrel{n\to \infty}{\implies} \mathcal{L}(X)
\]
and so $ \phi_{X_{n_{k}}}(u) \to \phi_{X}(u)$ for all $ u\in \R^{d}$. Thus, $ \psi \equiv \phi$.\\ 

Suppose for a contradiction that $ \mathcal{L}_{X_{n}
}$ does not converge. Then there exists $ f$ continuous and bounded and a subsequence $ m_{k}$ such that  
\[
 \left|\mathbb{E}_{m_{k}}\left[ f(X_{m_{k}}) \right] - \mathbb{E}\left[ f(X) \right]\right|\geq \epsilon
\]
for all $ k\ni \N$. Now, since $ (\mathcal{L}(X_{m_{k}}))_{k\in \N}
$ is tight, there exist a subsequence, without relabelling, such that $ ( \mathcal{L}(X_{m_{k}}))$ converges weakly, a contradiction. Thus, the limit must also be $ X$. 
   \end{enumerate}
    
\end{proof}

Now, we briefly embark on a discussion of the theory of \textit{large deviations}.

\section{Large deviations}\label{sec: large deviations}


Let $ X_{1}, X_{2}, \cdots$ be iid $\sim \mathcal{N}(0,1) $ random variables. Let $ \widehat{S}_{n} = \frac{1}{n}\displaystyle\sum^{n}_{i=1} X_{i} \sim \mathcal{N}(0, 1/n)$. Let $ \delta >0$, we by the weak law of large numbers that 
\begin{enumerate}
	\item 
\[
	\PP(|\hat{S}_{n}|\geq \delta)\stackrel{n\to \infty}{\longrightarrow} 0.
\]
\item \[ \PP(\sqrt{n}|\hat{S}_{n}|\in \stackrel{\text{interval}}{A})$ \stackrel{\text{CLT}}{\longrightarrow} \displaystyle\int_{A} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\diff x. 
	\item \[ 
			\PP(|\hat{S}_{n}|\geq \delta)$ = 1- \displaystyle\int^{\delta\sqrt{n}}_{-\delta \sqrt{n}} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\diff x. 
	\]
In other words, 
\[
 \frac{\log \PP(|\hat{S}_{n}|\geq \delta)}{n} \stackrel{n\to \infty}{\longrightarrow} -\frac{\delta}{2}.
\]
\end{enumerate}

Observe that $ \hat{S}_{n}$, the ``typical `` value is of the order $ \frac{1}{\sqrt{n}}$ and it can take relatively large values $ (\geq \delta>0)$ with very small probability $ \tilde e^{-\frac{\delta^{2}n}{2}}$. Furthermore, $ 1, 2$ are universal but $ 3$ depends on the distribution. We shall focus on quantifying $ 3$ for an appropriate class of random variables.\\ 

Let $ X_{1}, X_{2}, \cdots$ be an iid family of random variables, such that $ \mathbb{E}\left[  X_{1}\right] = \overline{x}$, $S_{n} = X_{1}+X_{2}+\cdots+X_{n}$. Let $ a\in \R$. Now 
\[
	\PP(S_{n+m}\geq a(n+m))\stackrel{\text{independence}}{\geq}\PP(S_{n}\geq a_{n})\cdot \PP(S_{m\geq a_{m}}). 
\]
Now, with $ b_{n} = -\log \PP(S_{n}\geq an)$ for all $ n\in \N$, have that $ b_{n+m}\leq b_{n}+b_{m}$. This is called  sub-additive sequence. Actually, for such sequences one has 
\[
\lim_{n \to \infty} \frac{b_{n}}{n} = \displaystyle\inf_{n}\frac{b_{n}}{n}.
\]
\begin{examplesblock}{Sub-additive sequences}\label{aside: subbadtive sequences}
To quickly see this, suppose first that $ \displaystyle\inf_{n}\frac{b_{n}}{n}>-\infty$. Fix any $ \epsilon>0$, then there exists some $ m\in \N$ such that $ \frac{b_{m}}{m}< \displaystyle\inf_{n}\frac{b_{n}}{n}+\epsilon$. Hence, for any $ k \geq m$, we have by Euclidean division that there exists some $ q\in \Z_{+}$ and $ r\in [0,m)\cap \N$ such that $ k = qm + r$. Thus, the sub-additivity of $ (b_{n})_{n\in \N}$ implies that 
\[
\begin{array}{ll}
	\frac{b_{k}}{k} = \frac{b_{qm+r}}{qm+r}&\leq \frac{q \cdot b_{m} + b_{r}}{qm+r} \\
					       &\leq \cancelto{1}{\frac{qm}{qm+r}}\displaystyle\inf_{n}\frac{b_{n}}{n} + \cancelto{0}{\epsilon\cdot mq + \frac{b_{r}}{qm+r}}
\end{array}
\]
as $ k\to \infty$. The case where  $ \displaystyle\inf_{n}\frac{b_{n}}{n}=-\infty$ can be dealt with similarly.

\end{examplesblock}

So, we have that 
\[
-\frac{1}{n}\log\PP(S_{n}\geq a_{n}) \stackrel{n\to \infty}{\longrightarrow} I(a).
\]
Also, 
\[
\begin{array}{ll}
	\PP(S_{n}\geq an) &\stackrel{\lambda >0}{=} \PP(e^{\lambda S_{n}}\geq e^{\lambda an}) \\
			  &\stackrel{\text{Markov}}{\leq} \mathbb{E}\left[ e^{\lambda S_{n}} \right]\cdot e^{-n\lambda a} = \mathbb{E}\left[ e^{\lambda X_{1}} \right]\cdot e^{-\lambda an}.
\end{array}
\]
Define $ M(\lambda) = \mathbb{E}\left[ e^{\lambda \cdot X_{1}} \right]$, $ \psi(\lambda) = \log M(\lambda)$, $ \lambda \in \R$. In other words, we have 
\[
\PP(S_{n}\geq an)\leq \exp(-n(\lambda a-\psi(\lambda))).
\]
Furthermore, let $ \psi^{*}(a) = \displaystyle\sup_{ \lambda\geq 0}(\lambda a-\psi(\lambda))\geq 0$. So $ \PP(S_{n}\geq an)\leq \exp(-n\psi^{*}(a))$ and so have obtained 
\[
 \frac{-\log \PP(S_{n\geq an})}{n} \geq \psi^{*}(a).
\]

\mymark{Lecture 16} 
\begin{theorem}[Cramer's Theorem]\label{thm: cramer}
	Let $ X_{1}, X_{2}, \cdots$ be an iid sequence of random variables with $ \mathbb{E}\left[ X_{1} \right] = \overline{x}$. Let $ S_{n} =\displaystyle\sum^{n}_{i=1}X_{i} $. Then, 
	\[
	-\frac{1}{n}\log\PP(S_{n}\geq an) \stackrel{n\to \infty}{\longrightarrow} \psi*(a)
\]
for all $ a\geq \overline{x}$ where $ \psi^{*}(a) = \displaystyle\sup_{\lambda \geq 0}(\lambda alpha-psi(\lambda))$, $ \psi(\lambda) = \log \mathbb{E}\left[ e^{\lambda \cdot X_{1}} \right]$ ($ \psi^{*}$ is known as the \textit{Legendre transform}).
\end{theorem}

We collect some basic facts about the function $ M(\lambda) = \mathbb{E}\left[ e^{\lambda X_{1}} \right]$, $ \lambda \in \R$.

\begin{boxlemma}\label{lemma: cramer log mgf}
	The functions $ M$ and $ \psi$ are continuous on $ \mathcal{D} = \{\lamda: M(\lambda)<\infty\}$ and differentiable in $ \intr{\mathcal{D}}$ with $ M'(\lambda) = \mathbb{E}\left[ X_{1\cdot e^{\lambda X_{1}}} \right]$ and $ \psi'(\lambda) = \frac{M'(\lambda)}{\lambda}$, $ \lambda \in \mathcal{D}$. 
\end{boxlemma}

\begin{proof}
	\underline{Continuity:} Fix a sequence $ \lambda_{n} \stackrel{n\to \infty}{\longrightarrow}\lambda\in \mathcal{D}$. Then, pointwise, $ e^{\lambda_{n}X_{1}}\stackrel{n\to \infty}{\longrightarrow} e^{\lambda X_{1}}$ and take $ n\in \N$ such that for all $ n\geq N$, $ e^{\lambda_{n}X_{1}}\leq e^{\lambda_{N}X_{1}}+e^{\lambda X_{1}}\in \mathcal{L}^{1}$ (which holds by since $ \lambda_{N}\leq \lambda_{n}\leq \lambda$ for $ n$ possible larger). Thus, can conclude by dominated convergence that $ \psi(\lamnda_{n})\stackrel{n\to \infty}{\longrightarrow}\psi(\lambda)$.\\ 

	\underline{Differentiability:} Fix $ \eta \in \intr{ \mathcal{D}}$. We can now bound 
	\[
	\begin{array}{ll}
	    \left|\frac{M(\eta + \epsilon)-M(\eta)}{\epsilon}\right| &= \left| \mathbb{E}\left[\frac{ e^{(\eta+\epsilon)\cdot X_{1}}-e^{\eta\cdot X_{1}}}{\epsilon} \right] \right|\\
	     &\leq e^{\eta\cdot X_{1}} \left| \frac{e^{\epsilon\cdotX_{1}}-1}{\epsilon} \right|.
	\end{array}
	\]
	Now, let $ \delta>0$ sufficiently small such that $ (\eta-\delta, \eta+\delta)\subseteq \intr{ \mathcal{D}}$. Now, for all $ \epsilon \in (-\delta, \delta)$
	\[
		\left| \frac{e^{\epsilonX_{1}}-1}{\epsilon} \right| &\stackrel{\text{comparing power series}}{\leq} \frac{e^{\delta|X_{1}|}-1}{\delta}.
	\]
	So 
	\[
\left| \frac{e^{(\eta+\epsilon)X_{1}}-e^{\eta X_{1}}}{\epsilon} \right|	\leq e^{\eta X_{1}}\cdot \frac{e^{\delta|X_{1}|}-1}{\delta}. 
	\]
	Now, since $ e^{\eta X_{1}}\cdot e^{\delta |X_{1}|}\leq e^{\eta X_{1}}\cdot(e^{\delta X_{1}}+e^{-\delta X_{1}})\in \mathcal{L}^{1}$ since $ \eta \in \intr{ \mathcal{D}}$ and we can thus conclude by dominated convergence.
\end{proof}


\begin{proof}{(Theorem \ref{thm: cramer})}
    From the previously derived Chernoff bound, we have 
    \[
    \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)\geq \psi^{*}(a).
    \]
    It suffices to show now that 
    \[
	    \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)\leq \psi^{*}(a), \quad \text{ for all }a\geq \overline{x}.
    \]
    Observe that we can replace each $ X_{i}$ by $ \tilde{X}_{i} = X_{i}-a$ and define $ \tilde{S}_{n} =\displaystyle\sum^{n }_{i=1}\tilde{X}_{i}$ and \\ 
    $ \tilde{M}(\lambda) = \mathbb{E}\left[ e^{\lambda \tilde{X}} \right] = e^{-a\lambda}M(\lambda)$, where $ \tilde{\psi}(\lambda) = \psi(\lambda)-a\lambda$, $ \lambda \in \R$. 
\end{proof}

Thus we can restate the original inequality as follows
\[
\lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)= \lim_{n \to \infty} -\frac{1}{n}\log \PP(\tilde{S}_{n}\geq 0)\leq \tilde{\psi}^{*}(0)\,
\]
where $ \tilde{\psi}^{*}(\lambda) = \displaystyle\sup_{\lambda \geq 0}(-\tilde{\psi}(\lambda))$. Thus, without loss of generality, it suffices to show that 
\[
 \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq 0)\geq \displaystyle\inf_{\lambda \geq 0}\psi(\lambda),
\]
when $ \overline{x}\leq 0$.\\ 


For the remainder of the proof, we let $ \mu = \mathcal{L}(X)$ and break the proof into several cases.\\ 

\underline{Case 1: } $ M(\lambda)<\infty$ for all $ \lambda \in \R$.\\ 

Define a new measure $ \mu_{\theta}$ for all $ \theta \geq 0$, absolutely continuous with respect to $ \mu$ and radon-Nikodym derivative 
\[
	\frac{\diff \mu_{\theta}}{\diff  \mu} = \frac{e^{\theta X_{1}}}{M(\theta)}.
\]
We compute
\[
 \mathbb{E}_{\theta}\left[ f(X_{1})\right] = \displaystyle\int_{\R} \frac{e^{\theta x}f(x)}{M(\theta)}\mu (\diff  x).    
\]
Now, if $X_{1}, \cdots, X_{n}$ are iid $ \sim \mu$. Then 
\[
	\mathbb{E}_{\theta}\left[ F(X_{1}, \cdots, X_{n}) \right] = \displaystyle\int F(X_{1}, \cdots, X_{n}) \displaystyle\prod^{n}_{i=1}\frac{e^{\theta x_{i}}}{M(\theta)}\mu(\diff  x_{i}).   
\]
Set $ g(\theta) = \mathbb{E}_{\theta}\left[ X_{1} \right] = \int x \frac{e^{\theta x}}{M(\theta)}\diff  \mu = \frac{M'(\theta)}{M(\theta)} = \psi'(\theta)$.\\ 

\underline{Seek:} $ \theta$ such that $ g(\theta) = \psi'(\theta) = 0$.\\ 

If $ \PP(X_{1}>0) = 0$, then $ \PP(S_{n}\geq 0) = (\PP(X_{1}=0))^{n}$ by independence. Thus, 
\[
\frac{1}{n}\log \PP(S_{n}\geq 0) = \PP(X_{1} = 0)
\]
and\[
\displaystyle\inf_{\lambda\geq 0} \leq \lim_{\lambda \to \infty}\psi(\lambda) = \lim_{\lambda \to \infty}\mathbb{E}\left[ e^{\lambda X_{1}} \right] \stackrel{\text{DCT}}{=} \lim_{\lambda \to \infty}\mathbb{E}\left[ e^{\lambda X_{1}} \mathbf{1}(X_{1}=0) \right] =  \PP(X_{1}=0).
\]

We can now focus on the case where $ \PP(X_{1}>0)>0$. Now, there exists an $ N\in \N$ such that $ \PP(X_{1}>\frac{1}{N})>0$. We deduce that 
\[
	\lim_{\theta\to \infty}\psi(\theta) = \lim_{\theta \to \infty}\mathbb{E}\left[ e^{\theta X_{1}} \right] \geq \lim_{\theta \to \infty} \mathbb{E}\left[ e^{ \frac{\theta}{N}} \mathbf{1}\left(X_{1}>\frac{1}{N}\right)\right] = \infty. 
\]
Thus, there exists some $ \eta \geq 0$ such that $ \displaystyle\inf_{\lambda \geq 0}\psi(\lambda) = \psi(\eta)$ and $ \psi'(\eta) = 0$. Now, 
\[
\begin{array}{ll}
	\PP(S_{n}\geq 0) &\geq \PP(S_{n}\in [0, \epsilon n])\geq \mathbb{E}\left[ e^{\eta S_{n}-\eta\epsilon n} \mathbf{1}(S_{n}\in [0,\epsilon n]) \right] \\
			 & = e^{-\eta \epsilon n}(M(\eta))^{n}\cdot\PP_{\eta}(S_{n}\in [0,\epsilon n])
\end{array}
\]
where $ \PP_{\eta}(X_{1}\in \cdot) = \mu_{\eta}(\cdot)$. Now, since $ \mathbb{E}_{\eta}\left[ X_{1} \right] = 0$, we claim that we can use the CLT on iid copies of $ X_{1}$ with law $ \mu_{\eta}$ to deduce 
\[
	\PP(S_{n}\in[0,\epsilon n])\stackrel{n\to \infty}{\longrightarrow}\frac{1}{2}.
\]
\begin{examplesblock}{Proof of claim}\label{aside: cramer clt}
This is a little messy, be warned! Fix any $ \epsilon'>0$. We have by the triangle inequality
\[
	\left| \PP_{\eta}(S_{n}\in [0,\epsilon n])-\frac{1}{2} \right|\leq \left| \PP_{\eta}(S_{n}\in [0,\epsilon n])- \PP_{\eta}(S_{n}\in [0,\infty)) \right| + \left| \PP_{\eta}(S_{n}\in [0,\infty))-\frac{1}{2} \right|.
\]
for all $ n\in \N$. Now, by the CLT and Theorem \ref{thm: weak convergence equivalence} we have that 
\[
\PP(S_{n}\in[0,\infty)) \stackrel{n\to \infty}{\longrightarrow} \frac{1}{2}. 
\]

Thus, for all n sufficiently large, we have that $ |\PP(S_{n}\in[0,\infty))-1/2|<\epsilon'/3$. Furthermore, there exists some $ N\in \N$ such that $ \PP( \mathcal{N}\in(\epsilon \sqrt{N}, \infty))<\epsilon'/3$ where $ \mathcal{N}$ denotes a standard normal random variable. Thus, for all $ n\in \N$ sufficiently large 
\[
\begin{array}{ll}
	\left| \PP_{\eta}(S_{n} \in [0,\epsilon n])-\frac{1}{2} \right|&\leq \frac{\epsilon'}{3}+ \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{n},\infty))\right| \leq \frac{\epsilon'}{3} + \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{n},\infty))\right| \\
								       &\leq \frac{\epsilon'}{3} + \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right| \leq  \frac{\epsilon'}{3} + \PP( \mathcal{N}\in(\epsilon \sqrt{N}, \infty))\\ 
								       &+ \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))- \PP_{\eta}(\frac{ \mathcal{N}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right|\\  
								       &\leq \frac{\epsilon'}{3}+ \frac{\epsilon'}{3} + \cancelto{\text{(CLT) }\leq \frac{\epsilon'}{3}}{\left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))- \PP_{\eta}(\frac{ \mathcal{N}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right|}\\
								       &\leq \epsilon'
\end{array}
\]
as required.
\end{examplesblock}

Thus, 
\[
	\frac{\log \PP(S_{n}\geq 0)}{n}\geq -\eta \epsilon +\log M(\eta) + \frac{\log\PP_{\eta}(S_{n}\in[0,\epsilon n])}{n}.
\]
Now, for all $ \epsilon> 0$, 
\[
 \displaystyle\liminf_{ n}\frac{1}{n}\log \PP(S_{n}\geq 0 )\geq \log M(\eta) -\eta \epsilon= \psi(\eta)\geq \displaystyle\inf_{\lambda \geq 0}\psi(\lambda).
\]
Sending $ \epsilon \to 0$ gives the desired inequality.\\ 

\underline{General Case:}\\ 

Without loss of generality, (arguing as in the previous case), let $ K>0$ sufficiently large so that $ \mu([0, K])>0$. Then define the conditional laws $ \nu = \mathcal{L}(X_{1}| |X_{1}|\leq K)$, $ \nu_{n} = \mathcal{L}\left(S_{n}\Big| \displaystyle\bigcap^n_{i = 1} \{|X_{i}|\leq K\}\right)$. Have 
\[
	\mu_{n([0,\infty)}\geq \nu_{n}([0,\infty))\cdot (\mu([-K,K]))^{n}
\]
and 
\[
\log \mu_{n}([0,\infty))\geq \frac{\log \nu_{n}([0,\infty)
}{n} +\mu([-K, K]).
\]
Let $ \psi_{K}(\lambda) = \log \displaystyle\int^{K}_{-K} e^{\lambda x}\diff\mu(x)$. Then, 
$ \log \displaystyle\int^{\infty}_{-\infty}e^{\lambda x} \diff \nu(x) = \psi_{K}(\lambda) -\log\mu([-K,K]) $. So, 
\[
	\overbrace{\lim_{n \to \infty}\frac{1}{n}\log\mu_{n}([0,\infty))]}^{ \text{exists again by sub-additivity}} \stackrel{\text{first step}}{\geq} \displaystyle\inf_{\lambda\geq 0} \left( \log \displaystyle\int^{\infty}_{-\infty}e^{\lambda x} \diff \nu(x)   \right)+ \log\mu([-K,K]) = \displaystyle\inf_{\lambda \geq 0}\psi_{K}(\lambda) \coloneqq J_{K}>-\infty.\\ 
\]
Now, as observe that $ \psi_{K}$ is a non-decreasing family of continuous functions. Hence, the $(J_{k})_{k\in \N}$ are non-decreasing and so one has $ J_{k}\uparrow J>-\infty$ $ K\to \infty$. Furthermore, the sets $ \{\lambda: \psi_{K}(\lambda)\leq J\}$ are compact by the continuity of the $ \psi_{K}$and the fact that $ \mu([0,K])>0$ implies $ \displaystyle\lim_{\lambda  \to \infty}\psi_{K}(\lambda) = \infty $, as well as nested. Thus, there exists some $ \lambda_{0}\in \displaystyle\bigcap_{k} \{\lambda: \psi_{K}(\lambda)\leq J\}$. hence, $ \psi(\lambda_{0}) = \displaystyle\lim_{k\to\infty}\psi_{K}(\lambda)\leq J $ by monotone convergence. So, 
	\[
\lim_{n \to \infty}\frac{1}{n}\log\mu_{n}([0,\infty)) \geq J\geq \psi(\lambda_{0})\geq \displaystyle\inf_{\lambda\geq 0 }\psi(\lambda)
	\] 
as required.\\

\section{Brownian Motion}\label{Brownian motion}
\mymark{Lecture 17} 
\begin{boxdef}[]\label{def: }

\end{boxdef}


\end{document}

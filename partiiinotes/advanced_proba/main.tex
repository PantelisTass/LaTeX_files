\documentclass{article}
\input{preamble}  % Include the preamble from an external file

%\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}\addtocounter{page}{-1}}
\fancyhead{}
\fancyhead[L]{\text{Advanced Probability}}
\fancyhead[R]{\text{Pantelis Tassopoulos}}

\title{\Huge Part III Advanced Probability \\ 
\huge Based on lectures by P. Sousi}
\author{\Large Notes taken by Pantelis Tassopoulos}
\date{\Large Michaelmas 2023}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage 

\section{Conditional Expectation}
\mymark{Lecture 1}\subsection{Basic definitions}
Let $ (\Omega, \F, \PP) $ be a probability space. Remember the following definitions 
\begin{boxdef}[Sigma algebra]\label{def: sigma algebra}
	$ \F $ is a sigma algebra if and only if: ($ \F\in \mathcal{P}{\Omega} $)
	\begin{enumerate}
		\item $ \Omega \in \F $
		\item $ A\in \F \implies A^c\in \F$
		\item $ (A_n)_{n\in \N}\subseteq \F \implies \displaystyle\bigcup_{n\in\N}A_n\in\F $
	\end{enumerate}
\end{boxdef}
\begin{boxdef}[Probability measure]\label{def: prob measure}
	$ \PP $ is a probability measure if
	\begin{enumerate}
		\item $ \PP:\F\to [0,1] $ (i.e. a set function)
		\item $ \PP(\Omega) = 1 $, and $ \PP(\emptyset) = 0 $
		\item $ (A_n)_{n\in \N} $ pairwise disjoint $ \implies \PP\left(\displaystyle\bigcup_{n\in\N}A_n\right) =\displaystyle\sum^{\infty}_{n=1}\PP(A_n)$.
	\end{enumerate}
	
\end{boxdef} 
\begin{boxdef}[Random Variable]\label{def: rv}
	$ X:\Omega\to \R $ is a \underline{random variable} if for all $ B $ open in $ \R $, $ X^{-1}(B)\in \F $.
\end{boxdef}
\begin{remark}
	Observe that the sigma algebra $ \mathcal{G}=\{B\subseteq\R: X(B)\in\F\}\supseteq \mathcal{O} \implies \mathcal{G}\supseteq \mathcal{B}(\R) $, the former being the collection of open sets in $ \R $ and the latter the Borel sigma algebra on $ \R $ with the usual topology, namely, $ \sigma(\mathcal{O})$ (see below for the notation).
\end{remark}

Let $ \mathcal{A} $ be a collection of subsets of $ \Omega $. We define 
\[\begin{array}{ll}
	\sigma(\mathcal{A}) &= \text{smallest sigma algebra containing $ \mathcal{A} $} \\
     &=\displaystyle \bigcap \{\mathcal{T}:\mathcal{T} \text{ sigma algebra containing }\mathcal{A}\}.
\end{array}
\]

\begin{boxdef}[Borel sigma algebra on $ \R $]\label{def: borel sigma alg}
	Let $ \mathcal{O} = \{\text{open sets} \R\} $. Then, the Borel sigma algebra $ \mathcal{B}(\R)( \coloneq \mathcal{B} ) $ is defined as above, namely, 
	\[\mathcal{B}(\R)\coloneq \sigma(\mathcal{O}).\]
\end{boxdef}

Let $ (X_i)_{i\in I} $ be a family of random variables, then $ \sigma(X_{i}:i\in I) =$ the smallest sigma algebra that makes them all measurable. We also have the characterisation 
$ \sigma(X_{i}: i\in I) = \sigma(\{\underbrace{\{\omega\in \Omega: X_{i}(\omega)\in B\}}_{X^{-1}_{i}(B)}, i\in I, B\in \mathcal{B}(\R)\})$.

\subsection{Expectation}

Note we use the following for the indicator function on some event $ A $
\[
    \mathbf{1}(A)(x) = \mathbf{1}(x\in A) 
     \coloneqq \left. \begin{array}{@{}l@{}}
    1, \quad x\in A \\
    0, \quad x\notin A
     \end{array}\right\rbrace, \quad A\in \F.
\]


We now begin the construction of the expectation of generic random variables.\\

\underline{Positive simple random variables:} $X = \displaystyle\sum^{
n}_{i=1}\mathbf{1}(A_{i}), c_{i}\geq 0, A_{i}\in\F.  $.
\[
	\mathbb{E}[X]\coloneqq\displaystyle\sum^{n}_{i=1}c_{i}\PP(A_{i}). 
\]


\underline{Non-negative random variables:} $ (X\geq 0). $
We proceed by approximation. Namely, let $ X_{n}(\omega)\coloneqq 2^{-n}\lfloor 2^{-n}\cdot X(\omega)\rfloor \land n \uparrow X(\omega) , n\to \infty$. Now, by monotone convergence, 
\[
	\mathbb{E}[X]\coloneqq \uparrow \displaystyle\lim_{n\to\infty}\mathbb{E}[X_{n}]=\displaystyle\sup\mathbb{E}[X].
\]

\underline{General random variables:} Have the decomposition $ X = X^{+}-X^{-} $, where $ X^{+} = X\lor 0$, $X^{-}=-X\land 0 $. If one of $ \mathbb{E}[X^{+}], \mathbb{E}[X^{-}]  <\infty $ then set 
\[
	\mathbb{E}[X]\coloneqq \mathbb{E}[X^{+}]-\mathbb{E}[X^{-}].  
\]

\begin{boxdef}\label{def: integrable rv}
	$ X $ is called \underline{integrable} if $ \mathbb{E}[|X|]<\infty $.
\end{boxdef}

\begin{boxdef}\label{def: cond prob event}
Let $ B\in \F $ with $ \PP(B)>0 $. Then for all $ A\in \F $, set 
\[
\PP(A|B)\coloneqq \frac{\PP(A\cap B)}{\PP(B)}
\] 

\end{boxdef}


Now for an integer-valued random variable $ X $, we set:
\[
	\mathbb{E}[X|B]\coloneqq \frac{\mathbb{E}[X\cdot \mathbf{1}_{B}]}{\PP(B)}
\]


\subsection{Conditional expectation with respect to countably generated sigma algebras}

\mymark{Lecture 2}We now extend the definition of the conditional expectation for a \underline{countably generated sigma algebra}. Let $ (\Omega, \F, \PP) $ be a probability space. We call the sigma algebra $\mathcal{G} $ countably generated if there exists a collection $ (B_{n})_{n\in \N} $ of pairwise disjoint events such that $\displaystyle\bigcup_{n\in I}B_{n} = \Omega$ with ($ I $ countable) and $\mathcal{ G} = \sigma(B_{i}:i\in I)$.\\ 

Let $X$ be an integrable random variable. We want to define $\mathbb{E}[X|\mathcal{G}]$.\\ 

Define $X'(\omega) = \mathbb{E}[X|B_{i}]$, whenever $w\in B_{i}$, i.e. 
\[
	X' =\displaystyle\sum_{i\in I}\mathbf{1}(B_{i})\cdot\mathbb{E}[X|B_{i}]. 
\]

We make the convention that $\mathbb{E}[X|B_{i}] = 0$ if $\PP(B_{i}) = 0$. It is easy to check that $X'$ is $\mathcal{G}-$measurable. We also have that 
\[
\mathcal{G}  = \left\{\displaystyle\bigcup_{j\in } B_{j}: J\subseteq I \right\}
\]
and $X'$ satisfies for all $ G\in\mathcal{G}$:$\mathbb{E}[X\cdot\mathbf{1}_{G}]=\mathbb{E}[X'\cdot\mathbf{1}_{G}] $ and 
\[\begin{array}{ll}
	\mathbb{E}[|X'|] &\leq\mathbb{E} \left[\displaystyle\sum_{i\in I}|\mathbb{E}[X|B_{i}]\mathbf{1}(B_{i})  \right] \\
			 &=\displaystyle\sum_{i \in I}\PP(B_{i})\cdot \left|\mathbb{E}[X|B_{i}] \right|\\ 
			 &\leq\displaystyle\sum_{i\in I}\PP(B_{i})\cdot \underbrace{\mathbb{E}[X\cdot\mathbf{1}(B_{i})]}_{\PP(B_{i})}\\ 
			 &=\mathbb{E}[|X|]<\infty.
\end{array}
\]

\subsection{General case}

We say $ A\in \F$ happens \underline{a.s.} if $ \PP(A) = 1$. \underline{Recall} (from measure theory and basic functional analysis): 
\begin{theorem}[Monotone Convergence Theorem (MCT)]\label{thm: MCT}
	Let $(X_{n})_{n\in \N}$ be such that $ X_{n}\geq 0, X$ be random variables such that $ X_{n}\uparrow X$ as $ n\to \infty$. Then, $\mathbb{E}[X_{n}]\uparrow\mathbb{E}[X]$ as $ n\to \infty$.
\end{theorem}
 
\begin{theorem}[Dominanted Convergenec Theorem (DCT)]\label{thm: DCT}
	Let $ (X_{n})_{n\in \N}$ be random variables such that $ X_{n}\to X$ a.s. as $ n\to \infty$ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$, where $ Y$ is integrable, then $\mathbb{E}[X_{n}]\to\mathbb{E}[X]$, as $ n\to \infty$.
\end{theorem}

Let $ 1\leq p <\infty$ and $ f $ a measurable function, then set $ \norm{f}_{p}\coloneqq \left(\mathbb{E}[\norm{f}^{p}]\right)^{\frac{1}{p}}$. If $ p =\infty$, then set $ \norm{f}_{\infty}\coloneqq \displaystyle \inf \{\lambda: |f|\leq \lambda \text{ a.s.}\}$. Recall for all $ p$, the Lebesgue spaces, $\mathcal{L}^{p}(\Omega, \F, \PP)=\{f: \norm{f}_{p}<\infty\}$.

\begin{theorem}\label{thm: orthog proj hilbert}
	$ \mathcal{L}^{2}(\Omega, \F, \PP) $ is a Hilbert space, with inner product $  \bracket{u}{v}_{2}=\mathbb{E}[u\cdot v]$. Furthermore, for any closed subspace $\mathcal{H}$, if $ f\in\mathcal{L}^{2}$, there exists a unique $ g\in\mathcal{H}$ s.t. $ \norm{f-g}_{\mathcal{L}^{2}}=\displaystyle\inf_{h\in\mathcal{H}}\norm{f-h}_{\mathcal{L}^{2}}$ and $ \bracket{f-g}{h}=0$, for all $ h\in\mathcal{H}$. We say that $ g$ is the \underline{orthogonal projection} of $ f$ in $\mathcal{H}$.
\end{theorem}


We now construct the conditional expectation in the general case, for any integrably random variable with respect to an arbitrary sigma algebras.

\begin{theorem}[Conditional Expectation]\label{thm: cond exp}
Let $ (\Omega, \F, \PP)$ be a probability space, $\mathcal{G}\subseteq \F$ a sub-sigma algebra, $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$. Then there exists an integrable random variable $ Y$ satisfying:
\begin{enumerate}
	\item $ Y$ is $\mathcal{G}-$measurable
	\item for all $ G\in\mathcal{G},\mathbb{E}[X\cdot\mathbf{1}(G)]= \mathbb{E}[Y\cdot\mathbf{1}(G)]$.
\end{enumerate}
Moreover, $ Y$ unique in the sense that if $ Y'$ also satisfies the above $ 1),2)$, then $ Y = Y'$ a.s.. We call $ Y$ a version of the conditional expectation of $ X$ given $ G$. We write $ Y =\mathbb{E}[X\mathcal{G}]$ a.s. If $\mathcal{G} = \sigma(Z)$, where $ Z$ is a random variable, then we write $\mathbb{E}[Z] =\mathbb{E}[X|\mathcal{G}]$.

\end{theorem}

\begin{remark}
	$ 2)$ could be replaced by $\mathbb{E}[X\cdot Z] =\mathbb{E}[Y\cdot Z]$ for all $ Z$ bounded $\mathcal{G}-$measurable random variables. 
\end{remark}

We now state and prove the main theorem of this section:

\begin{proof}{(Theorem \ref{thm: cond exp})}
	\underline{Uniqueness:} Let $ Y, Y'$ satisfy $ 1), 2)$. Let $ A = \{Y > Y'\}\in\mathcal{G}$. Then $ 2) $  
	\[\begin{array}{ll}
	&\implies \mathbb{E}[Y\cdot\mathbf{1}(A)] =\mathbb{E}[Y'\cdot\mathbf{1}(A)]=\mathbb{E}[X\cdot\mathbf{1}(A)] \\
	&\implies\mathbb{E}[(Y-Y')\cdot\mathbf{1}(A)] = 0\\ 
	&\implies \PP(A) = \PP(Y>Y') = 0\\ 
	&\implies Y\leq Y' \text{ a.s.}.
	\end{array}
	\]
	We similarly obtain $ Y\geq Y'$ a.s., hence we deduce that $ Y = Y'$ a.s.

	\underline{Existence:} three steps. 
	\begin{enumerate}
		\item Assume that $ X \in\mathcal{L}^{2}(\Omega, \F, \PP)$. Observe that $\mathcal{L}^{2}(\Omega,\mathcal{G},\PP)$ is a closed subspace of $\mathcal{L}^{2}(\Omega, \F, \PP)$. Hence, Theorem \ref{thm: orthog proj hilbert}, we have the decomposition  $\mathcal{L}^{2}(\Omega, \F, \PP) =\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)\oplus\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.
Then, we have the corresponding decomposition $ X = Y+Z$, where $ Y\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)$ and $ Z\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP) $ respectively. Define $\mathbb{E}[X\mathcal{G}]\coloneqq Y$, $ Y$ is $\mathcal{G}-$measurable and for all $ A\in\mathcal{G}$, $\mathbb{E}[X\cdot\mathbf{1}(A)]\mathbb{E}[Y\cdot\mathbf{1}(A)]=\mathbb{E}[Z\cdot\mathbf{1}(A)]$ since $ Z\in  \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.

\underline{Claim:} If $ X \geq 0$, a.s. then $ Y \geq 0$ a.s.
Indeed, let $ A = \{Y< 0\}\in\mathcal{G}$. Then observe that $ 0\leq\mathbb{E}[X\cdot\mathbf{1}(A)]=\mathbb{E}[Y\cdot\mathbf{1}(A)]\leq 0$. Hence $\mathbb{E}[Y\cdot\mathbf{1}(A)]=0$ and so $ \PP(A) = 0$, gibing $ Y = 0$ a.s.

\item Assume $ X\geq 0 $.\\ 
	Define $ X_{n} = X\land n\leq n $, meaning $ X_{n}$ is bounded for all $ n\in \N$. So $ X_{n}\in\mathcal{L}^{2}(\Omega, \F, \PP)$. Let $ Y_{n} =\mathbb{E}[X_{n}]$ a.s.. $ (X_{n})_{n\in \N}$ is an increasing sequence. By the claim above, so is $ (Y_{n})_{n\in \N}$ a.s.\\
	Define $ Y = \displaystyle \limsup_{n}Y_{n}$ meaning $ Y$ is $\mathcal{G}-$measurable and $ Y = \uparrow \displaystyle \lim_{n\to \infty}Y_{n} $ a.s. Now, we have that for all $ A\in\mathcal{G}$, $\mathbb{E}[X_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]$. Thus, by theorem \ref{thm: MCT} (MCT), $\mathbb{E}[X\cdot\mathbf{1}(A)]= \displaystyle \lim_{n\to \infty} \mathbb{E}[X_{n}\cdot\mathbf{1}(A)] = \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y\cdot\mathbf{1}(A)]$.

\item $ X$ general in $\mathcal{L}^{1}$.\\ 
	Decompose as before $ X = X^{+}-X^{-}$. Define, $\mathbb{E}[X\mathcal{G}] =\mathbb{E}[X^{+}|\mathcal{G}]-\mathbb{E}[X^{-}|\mathcal{G}]$.
\end{enumerate}

\end{proof}

\mymark{Lecture 3}
\begin{remark}
From the second step of the proof of Theorem \ref{thm: cond exp} we see that we can define $\mathbb{E}[X|\mathcal{G}]$ for all $ X\geq 0$, not necessarily integrable. It satisfies all conditions $ 1) , 2)$ except for the integrability one.
\end{remark}

\begin{boxdef}\label{def: independence of sigma algebras}
$\underbrace{\mathcal{G}_{1},\mathcal{G}_{2}, \dots}_{\text{sigma algebras}} \subset \F$. We call them \underline{independent} if whenever $ G_{i}\in \mathcal{G}_{i}$ and $ i_{1}<\dots i_{k}$ for some $ k \in \N$, then $ \PP(G_{i_{1}}\cap \dots\cap G_{i_{k}}) = \displaystyle \prod^{k}_{j=1}\PP(G_{i_{j}})$.\\ 

Moreover, let $ X$ be a random variable and $\mathcal{G}$ a sigma algebra, then they are said to be int if $ \sigma(X)$ is independent of $\mathcal{G}$.
\end{boxdef}

\underline{Properties of conditional expectations:}
Fix $ X,y \in\mathcal{L}^{1}$, $ G\in \F$.
\begin{enumerate}
	\item $\mathbb{E}[\mathbb{E}[X\mathcal{G}]]=\mathbb{E}[X]$ (take $ A  = \Omega$)
	\item If $ X$ is $\mathcal{G}-$measurable, then $\mathbb{E}[X\mathcal{G}]=X$ a.s.
	\item If $ X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X\mathcal{G}]=\mathbb{E}[X]$
	\item If $ X\geq 0$ a.s., then $\mathbb{E}[X\mathcal{G}]\geq 0 $ a.s. 
	\item For $ \alpha, \beta \in \R$ $\mathbb{E}[\alpha X + \beta Y |\mathcal{G}] = \alpha\mathbb{E}[X]+\beta\mathbb{E}[Y]$
	\item $\mathbb{E}[X|\mathcal{G}]|\leq\mathbb{E}[|X| |\mathcal{G}]$ a.s. 
\end{enumerate}

Below we proved:we expansions of useful measure theoretic results for the expectation to their corresponding conditional counterparts. First recall:
\begin{boxlemma}[Fatou's Lemma]\label{lemma: Fatou}
Let $  X_{n}\geq 0$ for all $ n\in \N$. Then 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s}
\]


\end{boxlemma}


\begin{theorem}[Jensen's Inequality]\label{thm: jensen}
If $ X$ is integrable and $ \phi: \R \to \R$ is a convex function, then 
\[
	\phi(\mathbb{E}[X])\leq\mathbb{E}[\phi(X)]\quad   \text{ a.s.}
\]
\end{theorem}
Now the results themselves:

\begin{theorem}[Conditional Monotone Convergence theorem (MCT)]\label{thm: cond MCT}
Let $\mathcal{G}\subset \F$ be sigma algebras, $ X_{n}\geq 0$ a.a. and $ X_{n}\uparrow X$, as $ n\to \infty$ a.s. Then 
\[
	\mathbb{E}[X_{n}|\mathcal{G}]\uparrow\mathbb{E}[X|\mathcal{G}] \quad \text{ a.s.}
\]

\end{theorem}

\begin{proof}{Theorem \ref{thm: cond MCT}}
	Set $ Y_{n} =\mathbb{E}[X_{n}\mathcal{G}]$ a.s. Observe that $ Y_{n}$ is a.s. increasing. Set $ Y = \displaystyle\limsup_{n}Y_{n}$. $ Y_{n}$ is $\mathcal{G}-$measurable, hence, so is $ Y$ (as a $ \displaystyle \limsup $ of $\mathcal{G}-$measurable random variables) is also $\mathcal{G}-$measurable. Also, $ Y = \displaystyle \lim_{n\to \infty}Y_{n} $ a.s.\\ 

	\underline{Need to show:} $\mathbb{E}[Y\cdot\mathbf{1}(A)]\mathbb{E}[X\cdot\mathbf{1}(A)]$ for all $ A\in\mathcal{G}$.	Indeed,
	\[\begin{array}{ll}
	    \\
	    \mathbb{E}[Y\cdot\mathbf{1}(A)] &=\mathbb{E}[ \displaystyle \lim_{n\to \infty }Y_{n}\cdot\mathbf{1}(A) ] \stackrel{\text{MCT}}{=} \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]\\
																	      &=\displaystyle \lim_{n\to \infty }\mathbb{E}[X_{n}\cdot\mathbf{1}(A)]  =\mathbb{E}[X\cdot\mathbf{1}(A)].
	\end{array}
	\]
	
\end{proof}

\begin{proof}{Theorem \ref{lemma: Fatou}}
$ \displaystyle \liminf_{n}X_{n} = \displaystyle \lim_{n\to \infty }\left( \displaystyle\inf_{k\geq n}X_{k} \right) $, the limit of an increasing sequence. By Theorem \ref{thm: MCT}, we have 
\[
	\displaystyle \lim_{n\to \infty}\mathbb{E}[\displaystyle\inf_{k\geq n}X_{n}|\mathcal{G}] =\mathbb{E}[\displaystyle \liminf_{n}X_{n}|\mathcal{G}]
\]
and 
\[
	\mathbb{E}[\displaystyle \inf_{k\geq n}X_{k}|\mathcal{G}]\stackrel{\text{a.s.}}{\leq } \displaystyle\inf_{k\geq n}\mathbb{E}[X_{k}|\mathcal{G}]\footnote{\text{can take the infinum due to countability that preserves a.s.}}
\]
which gives the result 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s.}
\]

		
\end{proof}

\begin{theorem}[Conditional Dominated Convergence Theorem]\label{thm: cond DCT}
	SUppose $ X_{n}\to X$ a.s. $ n\to \infty $ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$ with $ Y$ integrable. Then $\mathbb{E}[X_{n}\mathcal{G}]\to\mathbb{E}[X\mathcal{G}]$ a.s. as $n\to \infty.$
\end{theorem}

\begin{proof}
	From $ -Y\leq X_{n}\leq Y$, we have $ X_{n}+Y\geq 0$ for all $ n\in \N$ and $ Y-X_{n}\geq 0 $a.s. By Theorem \ref{lemma: Fatou},
\[
		\begin{array}{ll}
		\mathbb{E}[X+Y\mathcal{G}] &=\mathbb{E}[\displaystyle\liminf_{n}(X_{n}+Y)|\mathcal{G}] \\
					   &\leq \displaystyle\liminf_{n}\mathbb{E}[X_{n}+Y|\mathcal{G}] = \displaystyle\liminf_{n}\mathbb{E}[X_{n}\mathcal{G}]+\mathbb{E}[X]
	\end{array}
\]
Thus, 
\[\begin{array}{ll}
	\mathbb{E}[|X-Y| |\mathcal{G}]&=\mathbb{E}[Y-\displaystyle\liminf_{n}X_{n}|\mathcal{G}]  \\
				     &\leq\mathbb{E}[Y]+\displaystyle\liminf_{n}  \mathbb{E}[X_{n}|\mathcal{G}] 
\end{array}
\]
Hence, 
\[
	\displaystyle\limsup_{n} \mathbb{E}[X_{n}|\mathcal{G}] \leq\mathbb{E}[X|\mathcal{G}]
\]
and 
\[
	\displaystyle\liminf_{n} \mathbb{E}[X_{n}|\mathcal{G}] \geq\mathbb{E}[X|\mathcal{G}]
\]
a.s., concluding the proof.

\end{proof}

\begin{theorem}[Conditional Jensen]\label{thm: cond jensen}
Let $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$, $ \phi:\R\to \R$ be a convex function s.t. $ \phi(X)$ is integrable or $ \phi(X)\geq 0 $
\[
	\phi(\mathbb{E}[X|\mathcal{G}])\leq\mathbb{E}[\phi(X)|\mathcal{G}] \quad \text{a.s.}
\]
\end{theorem}

\begin{proof}
	\underline{Claim:} (true for any convex function, no proof given) $ \phi(x)=\displaystyle\sup_{i\in\N}(a_{i}x+b_{i})$, $ a_{i}b_{i}\in\R$. 
Thus, 
\[
	\mathbb{E}[\phi(X)|\mathcal{G}]\geq a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i} \quad \text{ for all } i\in \N.
\]
Taking the supremum gives \footnote{can take the supremum due to countability which again preserves a.s.}
\[
\begin{array}{ll}
      
	\mathbb{E}[\phi(X)|\mathcal{G}]&\geq \displaystyle\sup_{i\in \N} \left(  a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i}  \right)\\ 
				       &= \phi(\mathbb{E}[X|\mathcal{G}]) \quad \text{ a.s.}
\end{array}
\]

\end{proof}

\begin{boxcor}\label{cor: norm contraction cond exp}
	For all $ 1\leq p <\infty \norm{\mathbb{E}[X|\mathcal{G}]}_{p}\leq \norm{X}_{p}$.
\end{boxcor}

\begin{proof}
    Apply conditional Jensen.
\end{proof}

\begin{boxprop}[Tower Property]\label{prop: tower ppty}
Let $ X$ be integrable and $\mathcal{H}\subseteq\mathcal{G}$ sigma algebras. Then 
\[
	\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]=\mathbb{E}[X|\mathcal{H}] \quad \text{ a.s.}
\]

\end{boxprop}

\begin{proof}
	\begin{enumerate}[(a)]
		\item $\mathbb{E}[X|\mathcal{H}] $ is $\mathcal{H}-$measurable.
		\item For all $ A\in\mathcal{H}$ NTS: 
			\[
				\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot\mathbf{1}(A)] =\mathbb{E}[\mathbb{E}[X|\mathcal{H}]\cdot\mathbf{1}(A)]
			\]
			Indeed, both terms above are equal to $\mathbb{E}[X\cdot\mathbf{1}(A)]$ since $ A\in\mathcal{G}\subseteq\mathcal{H}$.
			
    \end{enumerate}
    
\end{proof}

\begin{boxprop}\label{prop: meas factorisation cond exp}
Let $ X\in\mathcal{L}^{1}$, $\mathcal{G}\subseteq \F$, $ Y$ bounded $\mathcal{G}-$measurable. Then 
\[
	\mathbb{E}[X\cdot Y|\mathcal{G}] =  Y\cdot\mathbb{E}[X|\mathcal{G}].
\]

\end{boxprop}


\begin{proof}
	\begin{enumerate}[(a)]
		\item RHS is clearly $\mathcal{G}-$measurable.
		\item For all $ A\in\mathcal{G}$: 
			\[
			\begin{array}{ll}
				\mathbb{E}[X\cdot Y\cdot \mathbf{1}(A)] &=\mathbb{E}[Y\cdot\mathbb{E}[X\mathcal{G}]\cdot\mathbf{1}(A)] \\
				\mathbb{E}[X\cdot (\smash{\underbrace{Y\cdot\mathbf{1}(A)}_{\makebox[0pt]{$\mathcal{G}$-\text{meas. and bounded}}}})]&=\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot Y\cdot\mathbf{1}(A)]=RHS.
			\end{array}
			\]
			
    \end{enumerate}
    \vspace{1em} 
(Also observe that by a monotone class argument, we have for any integrable function $ f:\Omega \to \R$, $\mathbb{E}[X\cdot f] =\mathbb{E}[\mathbb{E}[X|\mathcal{ G}]\cdot f] $ ) 
\end{proof}


\mymark{Lecture 4}

We are building towards the Theorem
\begin{theorem}\label{thm: cond expectation sigma indep}
$ X\in \mathcal{L}^1, \mathcal{G}, \mathcal{H} \subseteq \F$. Assume $ \sigma( \mathcal{G}, \mathcal{H})\perp \mathcal{H}$, Then
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]

\end{theorem}


We begin with a definition
\begin{boxdef}\label{def: pi system}
	Let $ \mathcal{A} $ be a collection of sts. It is called a \underline{$ \pi-$system} if for all $ A,B\in \mathcal{A}$, we also have $ A\cap B\in \mathcal{A}$.
\end{boxdef}


\begin{theorem}[Uniquenes of extension]\label{thm: uniqueness meas extension}
Let $ (E, \xi)$be a measurable space and let $ \mathcal{A}$ be a $ \pi-$system generating the sigma algebra $ \xi$. Let $ \mu, \nu$ be two measures on $ (E, \xi)$ with $ \mu(E)=\nu(E)<\infty$. If $ \mu = \nu$ on $ \mathcal{A}$, then $ \mu = \nu$ on $ \xi$.
\end{theorem}

\begin{proof}{(Theorem \ref{thm: cond expectation sigma indep})}
    NTS: for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$
    \[
	    \mathbb{E}[X\cdot \mathbf{1}_{F}] =\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}_{F}]
    \]
    Now, set $  \mathcal{ A} = \{A\cap B : A\in \mathcal{ G}, B\in \mathcal{ H}\}$. It is easy to check that $  \mathcal{A}$ is a $ \pi-$system generating $ \sigma( \mathcal{G}, \mathcal{H})$. If $ F = A\cap B$ for some $ A\in \mathcal{G}$ and $ B \in \mathcal{H}$, Then 
    \[
    \begin{array}{ll}
	    \mathbb{E}[X\cdot \mathbf{1}(A\cap B)] &=\mathbb{E}[X\cdot \mathbf{1}(A)\cdot \mathbf{1}(B)] \\
						  & =\mathbb{E}[X\cdot \mathbf{1}(A)]\cdot\mathbb{E}[ \mathbf{1}(B)] \stackrel{H\perp \sigma( \mathcal{G}, \mathcal{H})}{=}\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}(A\cap B)].
    \end{array}
    \]

    Now assume $ X\geq 0$; in the general case, decompose $ X = X^{+}- X^{-}$ and apply same argument to both $ X^{\pm}$. Define the measures $ \mu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ and $ \nu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$. Observe that $ \mu(\Omega) = \nu(\Omega) =\mathbb{E}[X]<\infty$ and we have shown that $ \mu = \nu$ on $ \mathcal{A}$. Thus, $ \mu=\nu$ on $ \sigma( \mathcal{G}, \mathcal{H})$ which finally implies the result 
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]


\end{proof}


\begin{examplesblock}{Examples: }\label{examples: 1}

\begin{enumerate}
	\item 
\begin{boxdef}[Gaussian]\label{def: gaussian dist}
$ (X_{1}, X_{2}, \cdots, X_{n})\in \R^{n}$ has the Gaussian distribution if and only if for all scalars $ a_{1}, a_{2}, \cdots, a_{n}\in \R$, $ a_{1}X_{1}+\cdots a_{n}X_{n}$ has the Gaussian distrubition in $ \R$.
\end{boxdef}

A stochastic process (more on that later) $ (X_{t})_{t\geq 0}$ is a \underline{Gaussian process} if for all $ t_{1}<t_{2}<\cdots t_{n}$ the vector $ (X_{t_{1}}, X_{t_{2}}, \cdots, X_{t_{n}})$ is Gaussian.

Let $ (X,Y)$ be a Gaussian vector in $ \R^{2}$. We compute $\mathbb{E}[X|Y]$.\\ 
Let $ X' =\mathbb{E}[X|Y]$. Looking for $ f$ a Borel measurable function s.t. $ \mathbb{E}[X|Y] = f(Y)$ a.s. Let $ f(y) = ay+b$ for some $ a,b\in \R$ to be determined. Now, $ X' = aY+b$, $\mathbb{E}[X'] =\mathbb{E}[X] = a\mathbb{E}[Y]+b$ and $\mathbb{E}[X'\cdot Y] =\mathbb{E}[X\cdot Y]\implies\mathbb{E}[(X-X')\cdot Y]=0$. Thus $ \text{Cov}(X-X', Y)=0\implies \text{Cov}(X,Y) = a^{2}\text{Var}(Y)$.\\ 

\underline{Need to check:} that for all $ Z$ bounded $ \sigma(Y)-$measurable, $ \mathbb{E}[(X-X')\cdot Z] = 0$.\\ 
Indeed, observe that $ (X-X', Y)$ is a Gaussian vector and since $ \text{Cov}(X-X', Y) = 0\implies X-X'\perp Y\implies (X-X')\perp Z$.

\item Let $ (X,Y)$ be a random vector with density in $ \R^{2}$ with joint density function $ f_{X,Y}:\R^{2}\to \R$. Let $ h:\R\to \R$ be a Borel function such that  $ h(X)$ is integrable. We now compute $\mathbb{E}[h(X)| Y]$.\\ 
We have for all $ g$ bounded $ \sigma{Y}-$measurable functions.
	
\[
\begin{array}{ll}
	\displaystyle\int_{\R^{2}}h(x)g(y)f_{X,Y}(x,y) \diff x \diff y &=  \mathbb{E}[h(X)g(Y)]\\ 
								       &=\mathbb{E}[\mathbb{E}[h(X)|Y]g(Y)] =\mathbb{E}[\phi(Y)g(Y)]\\ 
								       &= \displaystyle\int_{\R^{2}}\phi(y)g(y)f_{Y(y)} \diff y  
\end{array}
\]
where $ f_{Y}(y) = \int_{\R}f_{X,Y}(x,y) \diff x  $ and $ \phi:\R\to \R$ is some Borel measurable function. Hence, 

\[
\phi(y) = \left\lbrace
\begin{array}{@{}l@{}}
    \displaystyle\int_{\R} h(x)\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \diff dx, \quad f_{Y}(y)>0   \\
    0, \quad \text{otherwise}
\end{array}\right.
\]
can be seen to work. Thus, we obtain 
\[
	\mathbb{E}[h(X)|Y] = \phi(Y) \quad \text{a.s.}
\]
	
\end{enumerate}
\end{examplesblock}


\section{Discrete Time Martingales}

\begin{boxdef}[Filtration]\label{def: filtration}
	Let $ (\Omega, \F, \PP)$ be a probability space. A \underline{filtration} is a sequences of increasing sigma sub-algebras of $ \F$, $ (\F_{n})_{n\in \N}$, $ \F_{n}\subseteq \F_{n+1}$ for all $ n\in \N$. We call $ (\Omega, \F, (\F_{n})_{n\in\N})$ a \underline{filtered probability space}.\\ 

	Let $X =  (X_{n})_{n\in \N}$ be a sequence of random variables/a stochastic process. Then it induces $ (\F^{X}_{n})_{n\in N}$, where $ \F^{X}_{n}\coloneqq \sigma(X_{:k\leq n})$ for all $ n\in \N$: the canonical filtration associated to $ X$. We call $ X$ \underline{adapted} to a filtration $ (\F_{n})_{n\in \N}$ if $ X$is $ \F_{n}-$measurable for all $ n\in \N$.$ X$ is called \underline{integrable} if $ X_{n}$ is integrable for all $ n\in \N$.
\end{boxdef}

\begin{boxdef}[Martingale discrete time]\label{def: martingale}
Let $ (\Omega, \F, (\F_{n})_{n\in\N}, \PP)$ be a filtered probability space. Let $ X = (X_{n})_{n\in \N}$be an integrable and adapted process. 
\begin{itemize}
	\item $ X$ is called a \underline{martingale} if $ \mathbb{E}[X_{n}| \F_{m}]=X_{m}$ a.s. for all $ n\geq m$.
	\item $ X$ is called a \underline{super-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\leq X_{m}$ a.s. for all $ n\geq m$.
        \item $ X$ is called a \underline{sub-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\geq X_{m}$ a.s. for all $ n\geq m$.
\end{itemize}

\end{boxdef}

\begin{remark}
	If $  X$ is a (super/sub)martingale with respect to $ (\F_{n})_{n\in\N}$, then it is also a martingale with respect to $ (\F^{X}_{n})_{n\in \N}$. To see this, one has to use the tower property \ref{prop: tower ppty}: $ \F^{X}_{n}\subseteq \F_{n}$ for all $ n\in\N$ implies $ \mathbb{E}[X_{n}|\F^{X}_{m}]= \mathbb{E}[ \mathbb{E}[X_{n}|\F_{m}]|\F^{X}_{m}]$ (since $ \mathbb{E}[X_{n}|\F_{m}]$ a.s.).
\end{remark}

\begin{examplesblock}{Examples: }\label{examples: 2}
\begin{enumerate}
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=0$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \xi_{1}+\cdots +\xi_{n}$ for all $ n\in\N$, $ X_{0} = 0$. $ X$ is a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=1$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \displaystyle\prod^{n}_{i=1}\xi_{i}$ for all $ n\in\N$, $ X_{0} = 1$. $ X$ is again a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.

\end{enumerate}

\end{examplesblock}

\mymark{Lecture 5}Let $ (\Omega, \F, (\F_{n})_{n\in \N}, \PP)$ be a filtered probability space.

\begin{boxdef}[Stopping time discrete time]\label{def: stopping time discrete}
	A \underline{stopping time} $ T$ is a random variable $ T:\Omega\to \Z_{+}\cup \{\infty\}$ s.t. $ \{T\leq n\}\in \F_{n}$ for all $ n\in \N$. Equivalently, if $ \{f=n\}\in \F_{n}$ for all $ n\in \N$ since 
	\[
		\{T=n\}=\underbrace{\{T\leq n\}}_{\F_{n}}\setminus \underbrace{\{T\leq n-1\}}_{\F_{n-1}\subset \F_{n}}\in \F_{n}.
	\]
and 
\[
	\{T\leq n\} =\displaystyle\bigcup^{n}_{k=1}\{T=k\}\in \F_{k}\subset \F_{n}.
\]

\end{boxdef}

\begin{examplesblock}{Examples: }\label{examples: 3}
\begin{enumerate}
	\item Constant time are trivially stopping times.
	\item Let $ X = (X_{n})_{n\in\N}$ be a stochastic process taking values in $ \R$ and $ A\in  \mathcal{B}(\R)$ ($ X$ adapted). Define 
		\[
			T_{A} = \{n\geq 0: X_{n\in A}\}.
		\]
		Then $ \{T_{A}\leq n \} =\displaystyle\bigcup^{n}_{k=0}\{X_{k\in A}\}\in \F_{n} $ for all $ n\in\N$ (with convention $ \inf \emptyset = \infty$).
	\item $ L_{A} = \sup\{n\geq 0: X_{n\in A}\}$ is \underline{NOT} a stopping time.	
\end{enumerate}
\end{examplesblock}

\underline{Properties:} $ S,T, (T_{n})_{n\in\N}$ stopping times. Then $ S\land T, S\lor T$, $ \displaystyle\inf_{n}T_{n}, \displaystyle\sup_{n}T_{n}$, $ \displaystyle\liminf_{n}T_{n}$, $ \displaystyle\limsup_{n}T_{n}$ are also stopping times.

\begin{boxdef}[Stopping time sigma algerbra]\label{def: stopping time sigma algebra}
It $ T$is a stopping time, define 
\[
	\F_{T}=\{A\in \F: A\cap \{T\leq t \}\in \F_{t}\}
\]
Let $ (X_{n})_{n\geq 0}$ be a process. Write $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ for $ \omega \in \Omega$ whenever $ T(\omega)<\infty$. Define the \underline{stopped process:} $ X^{T}_{t}\coloneqq X_{T\land t}$.
\end{boxdef}


\begin{boxprop}\label{prop: stopping time discrete}
	Let $ S$ and $ T$ be stopping times, and let $ X$ be an adapted process, then:
	\begin{enumerate}
		\item If $ S\leq T$, then $ \F_{S}\subseteq\F_{T}$.
		\item $ X_{T}\cdot$ is $ \F_{T}-$measurable.
		\item $ X^{T}$ is adapted. 
		\item If $ X$ is integrable, then the stopped process iss integrable.
	\end{enumerate}
	
\end{boxprop}
\begin{proof}
    \begin{enumerate}
	    \item Immediate from definition.
	    \item Let $ A\in \mathcal{B}(\R)$. Need to show: 
		    \[
			    \{X_{T} \mathbf{1}(T<\infty)\}\cap \{T\leq t\} \in A, \quad \text{ for all }t\geq 0.
		    \]
Indeed, we have that 
\[
	\{X_{T} \mathbf{1}(T<\infty)\} =\displaystyle\bigcup^{t}_{s=0}\underbrace{\{X_{s}\in A\}}_{\F_{s}\subseteq \F_{t}}\cap \underbrace{\{T = s\}}_{\F_{s}}\in \F_{t}.
\]

\item $ X^{T}_{t} = X_{T\land t}$, this being $ \F_{T\land t}-$measurable $ \subseteq\F_{t}-$measurable by $ 1)$, so we conclude it is $ \F_{t}-$measurable.

\item 
	\[
	\begin{array}{ll}
		\mathbb{E}[|X_{t}^{T}|] &=\mathbb{E}[|X_{T\land t}|] \\
					&=\displaystyle\sum^{t-1}_{s=0}\mathbb{E}[|X_{s}|\cdot \mathbf{1}(T = s)]+\mathbb{E}[|X_{t}|\cdot \mathbf{1}(T\geq t)]\\ 
					&\leq\displaystyle\sum^{ t}_{s=0}\mathbb{E}[|X_{s}|]\underbrace{<\infty}_{X_{t} \text{ is integrable}}.
	\end{array}
	\]
	
    \end{enumerate}
    
\end{proof}

We now state and prove a fundamental theorem in Martingale theory: 

\begin{theorem}[Optional Stopping Theorem discrete time]\label{thm: optional stopping discrete time}
Let $ (X_{n}$ be a martingale. 
\begin{enumerate}
	\item If $ T$ is a stopping time, then the stopped process $ X^{T}$ is also a martingale. In particular for all $ t\geq 0$:
		\[
			\mathbb{E}[X_{T\land t}] =\mathbb{E}[X_{0}].
		\]
	\item It $ S\leq T$ are bounded stopping times, then 
		\[
			\mathbb{E}[X_{T}|\F_{S}] = X_{T}, \quad \text{a.s.}
		\]
		and hence $ \mathbb{E}[X_{T}] \mathbb{E}[X_{S}]$.
	\item It there exists an integrable random variable $ Y$ such that $ |X_{n}\leq Y|$ for all $ n \geq 0 $ and $ T$ is finite, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
	\item If there exists $ M\geq 0$ such that $ |X_{n+1}-X_{n}|\leq M$ for all $ n\in \N$ and $ T$ is a stopping time with $ \mathbb{E}[T]<\infty$, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
\end{enumerate}

\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item NTS: for all $ t\geq 0$, $ \mathbb{E}[X_{T\land t}|\F_{t-1}]=X_{T\land t}$ a.s.
		    Indeed, 
\[
\begin{array}{ll}
	\mathbb{E}[X_{T\land t}|\F_{t-1}] &=\displaystyle\sum^{t-1}_{s =0}\mathbb{E}[X_{s}\cdot \mathbf{ 1}(T=s)|\F_{t-1}] \mathbb{E}[X--t]\cdot \mathbf{1}(T\geq t)|\F_{t-1}] \\
					  &=\displaystyle\sum^{ t-1}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t) \quad \text{ a.s.}\\ 
&=\displaystyle\sum^{ t-2}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t-1) \quad \text{ a.s.}\\ 
&= X_{T\land t-1} \quad \text{a.s.}
\end{array}
\]
\item $S\leq T\leq n, n\in \N$ fixed. Let $ A\in \F_{S}$. \underline{NTS:} $ \mathbb{E}[X_{T}\cdot \mathbf{1}(A)] = \mathbb{e}[X_{s}\cdot \mathbf{1}(A)]$. We compute
	\[
	\begin{array}{ll}
	    X_{T}-X_{S} &= (X_{T}-X_{T-1})+\cdots + (X_{S+1}-X_{S}) \\
			&=\displaystyle\sum^{ n-1}_{k=0}(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T). 
	\end{array}
	\]
	Thus, 
	\[
		\mathbb{E}[X_{T}\cdot \mathbf{1}(A)] \stackrel{ (A\in \F_{S}) }{=}\mathbb{E}[X_{S}\cdot \mathbf{1}(A)]+ \displaystyle\sum^{ n-1}_{k=0}\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)]
	\]
	Have, $ A\cap \{S\leq k\}\in \F_{k}$ and $ A\cap \{T>k\}\in F_{k}$. Thus, $ \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)$ is $ \F_{k}-$measurable. Using $ \mathbb{E}[X_{k+1}|\F_{k}]=X_{k}$ a.s. we deduce that 
	\[
	\begin{array}{ll}
		\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)]
		&= \mathbb{E}[\smash{\cancelto{0}{\mathbb{E}[(X_{k+1}-X_{k})|\F_{k}]}}\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)] \\
		&= 0
	\end{array}
\]
Thus, $ \mathbb{E}[X_{T}|\F_{S}]=X_{S}$ a.s. 

\item By the Optional Stopping Theorem applied to $ (X_{T\land n})_{n\geq 0}$, we have 
	\[
		\mathbb{E}[X_{T\land n}] =\mathbb{E}[X_{0}] \quad \text{for all } n\geq 0.
	\]
Now, $ T$ being finite a.s. implies that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ a.s. By assumption, have $ |X_{T\land n}|\leq Y$ a.s. for all $ n\in \N$ and so can apply DCT to conclude.	

\item Observe that for all $ n\geq 1$
	\[
	X_{T\land n}-X_{0} =\displaystyle\sum^{n-1}_{k=0}(X_{k}-X_{0})\cdot \mathbf{1}(T=k)+(X_{n} -X_{0})\mathbf{1}(T\geq n) 
	\]
Hence, we have the bound (using that $ |X_{k+1}-X_{k}|\leq M$ a.s. for all $ k\geq 0$)
\[
\begin{array}{ll}
    \\
|X_{T\land n}-X_{0}|&\leq M\displaystyle\sum^{n-1}_{k=0} k\mathbf{1}(T=k) + n\mathbf{1}(T\geq n)\\
			&\leq\mathbb{E}[T]<\infty \quad \text{a.s.}
\end{array}
\]
Now, $\mathbb{E}[T]<\infty$ gives $ T<\infty$ a.s. and so can deduce as before that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ and use the DCT to conclude the argument. 


 \end{enumerate}
    
\end{proof}


\begin{boxcor}\label{cor: pos supermg bound}
Let $ X$ be a positive superartingale, $ T$ a stopping time such that $ T<\infty$ a.s., then 
\[
	\mathbb{E}[X_{T}]\leq\mathbb{E}[X_{0}].
\]

\end{boxcor}

\begin{proof}
	Use Fatou \ref{lemma: Fatou}: $\mathbb{E}[\displaystyle\liminf_{t\uparrow \infty}X_{T\land t}]\leq \displaystyle\liminf_{t\uparrow \infty}\mathbb{E}[X_{T\land t}]\leq\mathbb{E}[X_{0}]$.
\end{proof}


\begin{examplesblock}{Simple random walk on $ \Z$}\label{def: SRW on ints}
	Let $ (\xi_{i})_{i\geq 0}$ be iid Bernoulli random variables with success probability $ 1/2$. Define the process $(X_{n})_{n\geq 0}$ by setting $ X_{n} = \xi_{1}+\dots+\xi_{n}$ for all $ n\geq 1$ and $ X_{0}=0$. Furthermore, let $ T = \inf\{n\geq 0: X_{n}= 1\}$. Using the analysis below, we will see that $ \PP(T<\infty)=1$. The Optional Stopping Theorem gives $\mathbb{E}[X_{T\land t}]=0$ for all $ t\geq 0$. Yet, $1 = \mathbb{E}[X)_{T}]\neq 0$. We thus see that the condition $\mathbb{E}[T]<\infty$ in $ 4)$ is necessary, since $\mathbb{E}[T] = \infty$. 
	\begin{figure}[H]
	    \centering
	    \includesvg[width=0.6\linewidth]{images/SRW-on-Z.svg}
	    \caption{Illustration of simple random walk (first step) on $ \Z$.}
	    \label{fig: SRW on Z}
	\end{figure}
\end{examplesblock}


\mymark{Lecture 6} We consider again the example of the simple random walk \ref{def: SRW on ints} $ (X_{n})_{n\in \N}$ and define the stopping times 
\[
	T_{c} = \displaystyle\inf {n\geq 0: X_{n = c}}, \quad c\in \Z
\]
Set $ T = T_{-a}\land T_{b}$ for $ a b \in \Z$. We now ask what is $ \PP(T_{-a}\land T_{b})$?\\ 

To answer this, note first that $ X^{T}_{n} = X_{T\land n}$ is a martingale by the Optional Stopping Theorem and we also have the bounded differences $ |X_{n+1}-X_{n}|\leq 1$ for all $ n\geq 1$.\\ 

\underline{Claim:} $\mathbb{E}[T]<\infty$.\\ 
To show this, we will \textit{stochastically dominate} $ T$ be a geometric random variable, which automatically has a finite expectation and then conclude using the non-negativity of $ T$. Now we have that for the sequence $ \xi_{1}, \xi_{2}, \cdots, \xi_{a+b}$ the probability that they all are either $ +1$ or $ -1$ is $ 2\cdot 2^{-(a+b)}$ by independence, call this event $ A_1$. The same is true for the shifted sequence $ \xi_{k(a+b)+1}\cdots \xi_{(k+1)(a+b)}$ for all $ k\in \N$, where we call the corresponding event $ A_{k}$.  \\ 

Thus, we can bound $ T$ by the random variable
\[
	Z(\omega) =\displaystyle\inf\{n\geq 0: \omega \in A_{n}\}  
\]
which has the distribution $ Z\sim Geom(2\cdot2^{-(a+b)})$. Thus, $\mathbb{E}[T]<\mathbb{E}[Z]\leq (a+b)\cdot 2^{a+b-1}<\infty$. Thus, we conclude by the OST that $\mathbb{E}[X_{T}]=\mathbb{E}[X_{0}]=0$. Hence, $ -a\PP(T_{a}<T_{b})+b\PP(T_{b}<T_{-a}) = 0$ and so a quick computation yields that $ \PP(T_{-a}<T_{b}) = \frac{b}{a+b}$.\\ 

\section{Martingale Convergence Theorem}\label{sec: mg conv thm discrete case}

\begin{theorem}[Almost sure martingale convergence theorem]\label{thm: a.s. mg conv thm disc}
	Let $ X$ be a supermartingale bounded in $ \mathcal{L}^{1}$, i.e. satisfying $\displaystyle \sup_{n}\mathbb{E}[|X_{n}|]<\infty$. Then, there exists $ X_{\infty}\in \mathcal{ L}^{1}(\F_{\infty}), \F_{\infty} = \sigma(\F_{n}: n\geq 0)$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow} X_{\infty}$, a.s.
\end{theorem}

Before we embark on the proof of this theorem, we need so me supporting results. First we have a result from analysis and we set up some notation. Let $ x - (x_{n}_{n\in \N})$ be a real sequence and let $ a<b$ be reals. We proceed to define the \textit{number of upcrossings of the sequence } before time $ n\in \N$. Wec constructrecursively the sequence of times:
\[
	\begin{array}{ll}\label{eq: stopping times doob upcrossing}
    T_{0}(x) &= 0 \\
    S_{k+1}(x) &= \displaystyle\inf\{n\geq T_{k}(x): x_{n}\leq a\}\\
    T_{k+1}(x) &= \displaystyle\inf\{n\geq S_{k+1}(x): x_{n}\geq b\}\\
\end{array}
\]

and 
\[
	N_{n}([a,b], X) = \sup\{k\geq 0: T_{k}(x) \leq n\}
\]
Observe that as $ n\to \infty$, $ N_{n}([a,b], x)\uparrow N([a,b], x) = \sup\{k
geq 0: T_{k}(x)<\infty\}$ (see figure \ref{fig: doob upcrossing} for an illustration).

\begin{boxlemma}\label{lemma: upcrossing lemma}
	Let $ X = (X_{n})$ be a real sequence. Then $ X$ converges in $ \overline{\R} = \R \cup \{\pm \infty\}$ if and on ly if for all $ a<b$, $ a, b\in \Q$, $ N([a,b], X)<\infty$.
\end{boxlemma}

\begin{proof}
	\underline{$ \implies:$} Suppose $ x$ converges, if $ a<b$ such that $ N([a,b], x)=\infty$, then $ \displaystyle\liminf_{n}x_{n}\leq a < b \leq \displaystyle \limsup_{n}x_{n}$, a contradiction.\\ 
	\underline{$ \impliedby:$} if not, then $ \displaystyle\liminf_{n}x_{n}< \displaystyle \limsup_{n}x_{n}$ which implies that there exists $ a<b$ in $ \Q$ with $  \displaystyle\liminf_{n}x_{n}< a< b< \displaystyle \limsup_{n}x_{n}$, and hence $ N([a,n], x) = \infty$, a contradiction. 
\end{proof}

Now we state \text{it} Doob's upcrossing Inequality

\begin{boxlemma}[Doob's upcrossing inequality]\label{lemma: doob upcrossing}
Let $ X$ be a supermartingale, then for all $ n \in \N$: 
\[
	(b-a)\cdot\mathbb{E}[N_{n}([a,b], X)]\leq\mathbb{E}[(X_{n}-a)^{-}]
\]
\end{boxlemma}


\begin{proof}
	It is not hard to check that the sequences of times in \ref{eq: stopping times doob upcrossing} are stopping times. Now we have:

\[
\begin{array}{ll}
   &\displaystyle\sum^{n}_{k=1}(X_{T_{k}\land n}-X_{S_{k}\land n})\\
   &=\underbrace{\displaystyle\sum^{N_{n}}_{k=1} (X_{T_{k}}-X_{S_{k}})}_{\geq N_{n}\cdot (b-a)} + (X_{n}-X_{S_{N_{n}+1}}) \mathbf{1}(S_{N_{n}+1}\leq n)
\end{array}
\]
Since $ T_{k\land n}\geq S_{k\land n}$, the OST gives $\mathbb{E}[X_{T_{k}\land n}]\leq\mathbb{E}[X_{S_{k}\land n}]$. Note: 
\[
\underbrace{X_{n}-X_{S_{N_{n}}+1}}_{\geq (X_{n}-a) \land 0 = -(X_{n}-a)^{-}} \mathbf{1}(S_{N_{n}+1}\leq n).\]

Thus, taking expectations on both sides gives:	
	\[
		0 \geq (b-a)\cdot\mathbb{E}[N_{n}]-\mathbb{E}[(X_{n}-a)^{-}].
	\]
	thus concluding the proof.
\end{proof}


\begin{figure}[H]
    \centering
    \includesvg[width=0.8\linewidth]{images/Doob-crossing.svg}
    \caption{Illustration of upcrossings for the process $ (X_{n})_{n\in\N}$.}
    \label{fig: doob upcrossing}
\end{figure}

Now we proceed to the proof of the martingale convergence theorem:

\begin{proof}{(Theorem \ref{thm: a.s. mg conv thm disc})}
Fix $ a<b$, in $ \Q$. Have 
\[
\begin{array}{ll}
	\mathbb{E}[N_{n([a,b], X)}] &\leq(b-a)^{-}\underbrace{\mathbb{E}[(X_{n}-a)^{-}]}_{\leq\mathbb{E}[|X_{n}|+a]} \\
				    &\leq  (b-a)^{-} \left( \displaystyle\sup_{n\geq 0}\underbrace{\mathbb{E}[|X_{n}|]}_{<\infty}+a \right)
\end{array}
\]
Also have $ N_{n}([a,b], X)\uparrow N([a,b], X)$
as $ n\to \infty$. By monotone convergence: $ \mathbb{E}[N([a,b], X)]<\infty$. Set 
\[
	\Omega_{0} =\displaystyle\bigcap_{a<b \\ a,b, \in \Q}\{N([a,b], X)<\infty\}\in \F_{\infty} 
\]
and $ \PP(\Omega_{0})=1$. On $ \Omega_{0}$, $ X$ converges. set
\[
X_{\infty} = \left\lbrace
\begin{array}{@{}l@{}}
	\displaystyle \lim_{n\to \infty}X_{n} \quad \text{ on } \Omega_{0}  \\
	0, \quad \text{ on } \Omega\setminus \Omega_{0}.
    
\end{array}\right.
\]
So, $ X_{\infty}$ is $ \F_{\infty}-$measurable, $X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty} $ a.s. and 
\[
	\mathbb{E}[|X_{\infty}|] =\mathbb{E}[\displaystyle\liminf_{n}|X_{n}| ]\leq \displaystyle\liminf_{\mathbb{E}[X_{n}]}<\infty.  
\]
\end{proof}

\begin{boxcor}\label{cor: a.s. disc conv pos sup mg}
Let $ B$ be a upermaartingale. Then, $ X$ converges a.s.
\end{boxcor}

\begin{proof}
	$\mathbb{E}[|X_{n}|] =\mathbb{E}[X_{n}]\leq\mathbb{E}[X_{0}]$. Apply the martingale convergence theorem to conclude.
\end{proof}

\mymark{Lecture 7} \section{Doob's inequalities}


\begin{theorem}[Doob's maximal inequality]\label{thm: doob maximal ineq discrete}
Let $ X$ be a non-negative submartingale and set $ X^{*}_{n} = \displaystyle\sup_{ 0\leq k \leq n}X_{k}$ . Then for all $ \lambda \geq 0$, 
\[
\begin{array}{ll}
	\lambda \cdot \PP(X^{*}_{n}\geq \lambda ) &\leq\mathbb{E}[X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq \lambda)] \\
						  &\leq\mathbb{E}[X_{n}].
\end{array}
\]


\end{theorem}


\begin{proof}
	Let $ T = \displaystyle\inf\{k\geq 0 : X_{k}\geq \lambda\}$ (it is a stopping time). Then $ \{X^{*}_{n}\geq \lambda\} = \{T\leq n\}$. Also have that $ X_{T\land n}$ is a submartingale by the OST. Then $\mathbb{E}[X_{T\land n}]\leq\mathbb{E}[X_{n}]$. Now, 

	\[
	\begin{array}{ll}
		\mathbb{E}[X_{T\land n}] &= \mathbb{E}[X_{T}\cdot \mathbf{1}(T\leq n)] \\
					 &+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\geq \lambda \cdot \PP(T\leq n)+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\implies \lambda\cdot \PP(T\leq n) \leq \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(\underbrace{T\leq n}_{=\{X^{*}_{n}\geq \lambda\}}) \right]\\ 
					 &\leq \mathbb{E}\left[ X_{n} \right] 
	\end{array}
	\]
	
\end{proof}


\begin{theorem}[Doob's $  \mathcal{L}^{1}$ inequality]\label{thm: doob L1 ineq disc}
Lte $ p>1$ and let $ X$ be a martingale or a non-negative submartingale. Set $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n }|X_{k}|$. Then 

\[
	\norm{X^{8}_{n}}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}.\label{eq: doob L1 disc}
\]

\end{theorem}

\begin{proof}
	By Jensen, it is enough to prove  \ref{eq: doob L1 disc} for a non-negative submartingale. Now, observe that 

	\[
	\begin{array}{ll}
	     &= b \\
		(y\land k)^{p} &= \displaystyle\int_{k}^{0} px^{p-1} \mathbf{1(y\geq x)}\diff x  
			       =\mathbb{E}[ \displaystyle\int_{0}^{k}[x^{p-1} \mathbf{1}(X^{8}_{n}) \diff x  ]\\ 
			       &\stackrel{\text{Fubini}}{=} \displaystyle\int_{0}^{k} px^{p-1}\underline{\PP(X^{*}_{n}\geq x)}_{\leq \frac{1}{x}  \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq x) \right] }\diff x \\ 
			       &\leq \mathbb{E}\left[ \displaystyle\int_{0}^{k} px^{p-2}\cdot \mathbf{ 1}(X^{*}_{n}\geq x) \diff  x \cdot X_{n} \right] \\ 
			       &= \mathbb{E}\left[ \frac{p}{p-1}(X^{*}_{n}\land k)^{p-1}\cdot X_{n} \right]\\ 
			       &\stackrel{\text{H\"{o}lder}}{\leq} \frac{p}{p-1}\cdot \norm{X_{n}}_{p}\cdot \norm{X^{*}_{n}\land k}^{p-1}_{p}. 
	\end{array}
	\]
	So we proved $ \norm{X^{*}_{n}\land k}^{p}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}\cdot\norm{X^{*}_{n}\land k}^{p-1}_{p}$, which implies $ \norm{X^{*}_{n}\land k}_{p}\leq \frac{p}{p-1}\cdot \norm{X_{n}}_{p}$. Now take $ k\to \infty$ and use monotone convergence to conclude the argument.
\end{proof}



\begin{theorem}[$  \mathcal{L}^{p}$-convergence theorem]\label{thm: Lp convergence theorem discrete}
	Let $ X$ be a martingale and $ 1<p<\infty$, then the following are equivalent: 

	\begin{enumerate}
		\item $ X$ is bounded in $ \mathcal{L^{p}}$, i.e. $ \displaystyle\sup_{n\geq 0}\norm{X_{n}}_{p}<\infty $.
		\item $ X$ converges 'underline{almost surely} and in $ \mathcal{L}^{p}$ to a limit $ X_{\infty}\in \mathcal{L}^{p}$.
		\item There exists $ Z\in \mathcal{L}^{p} $ s.t. $ X_{n} = \mathbb{E}\left[Z | \F_{n}  \right]$ a.s.
	\end{enumerate}
	
\end{theorem}


\begin{proof}
\underline{$1) \implies 2)$:} $ X$ bounded in $ \mathcal{L}^{p}$ implies $ X$ is bounded in $ \mathcal{L}^{1} $. So there exists $ X_{\infty}$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ \underline{a.s.} \\ 
	Also, $ \mathbb{E}\left[ |X_{\infty}|^{p} \right] = \mathbb{E}\left[  \displaystyle\liminf_{n}|X_{n}|^{p} \right]\stackrel{\text{Fatou}}{\leq} \displaystyle\liminf_{ \mathbb{E}\left[ |X_{n}|^{p} \right]}<\infty$. Thus, $ X_{\infty}\in \mathcal{L}^{p} $.\\ 

	Now, let $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n}|X_{k}|$, $ X^{*}_{\infty} = \displaystyle\sup_{k\in \N}|X_{k}|$. Thus, 
	\[
		|X_{n}-X_{\infty}|\leq 2X^{*}_{\infty}
	\]
for all $ n\in \N$. Thus, it is enough to show by DCT that $ X^{*}_{\infty}\in \mathcal{L}^{p} $. By Doob's $ \mathcal{L}^{p}- $inequality,
	$\norm{X^{*}_{n}}_{p} &= \frac{p}{p-1}\cdot \displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty $
By MCT ($ X^{*}_{n}\uparrow X^{*}_{\infty}$):
$\norm{X^{*}_{\infty}}_{p}\leq \frac{p}{p-1}\displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty$
Thus, $ X^{*}_{\infty}\in \mathcal{L}^{p} $.

\underline{$2)\implies 3)$:} $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s. and in $ \mathcal{L}^{p} $. Set $ Z = X_{\infty}$. Need to show: $ X_{n} = \mathbb{E}\left[ X_{\infty} | \F_{n}\right]$ for all $ n\in \N$. 
\[
\begin{array}{ll}
	\norm{X_{n}- \mathbb{E}\left[ X_{oo}|\F_{n} \right]}_{p} &\stackrel{m\geq n}{=} \norm{\mathbb{E}\left[ X_{m}-X_{\infty} |\F_{n}\right]}_{p} \\ 
								 &\stackrel{\text{contraction (Jensen)}}{\leq} \norm{X_{m}-X_{\infty}}_{p}\to 0, \quad m\to \infty.
\end{array}
\]
\underline{$3)\implies  1)$:} By conditional Jensen, we can conclude.
\end{proof}

\begin{boxdef}\label{def: lp closed mg disc}
	A martingale of the form $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$, $ Z\in \mathcal{L}^{p} $ is called a martingale \underline{closed in $ \mathcal{L}^{p} $}.
\end{boxdef}

\begin{boxcor}\label{cor: lp closed a.s. conv}
	Let $ Z\in \mathcal{L}^{p} $, $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ Z|\F_{\infty} \right]$ a.s. and in $ \mathcal{L}^{p} $ where $ F_{\infty} = \sigma(X_{n}, n\geq 0)$.
\end{boxcor}

\begin{proof}
By theorem \ref{thm: Lp convergence theorem discrete}, we have $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s. And in $ \mathcal{L}^{p} $. Now, we need to show:
	\[
	X_{\infty} = \mathbb{E}\left[ Z|\F_{\infty} \right]\quad \text{a.s.}
	\]
Now, we have that $ X_{\infty}$ is $ \F_{\infty}-$measurable (being the point wise limit of $ X_{n}, n\geq 0$) and for all $ A\in \F_{\infty}$, $ \mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right]$. Fix $ A\in\displaystyle\bigcup_{n\geq 0}\F_{n} $, a $ \pi-$system generating $ \F_{\infty}$. There exists $ N\in \N$ such that $ A\in \F_{N}$. Let $ n\geq N$, now 
\[
	\mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right]\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right].
\]
\end{proof}

\begin{boxdef}[Uniform integrability]\label{def: UI}
A collection of variables $ (X_{i})_{i\in I}$ is called uniformly integrable (UI) if 

\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(|X_{i}|> M) \right]\stackrel{M\to \infty}{\longrightarrow}0.
\]
\end{boxdef}

Equivalently, $ (X_{i})_{i\in I}$ is UI if $ (X_{i})$ is bounded in $ \mathcal{L}^{1} $ and for all $ \epsilon>0$, there exists $ \delta >0$ such that for all $ A\in \F$ with $ \PP(A)<\delta$, 
\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(A_{i}) \right]<\epsilon.
\]
\begin{itemize}
	\item A UI family is bounded in $ \mathcal{L}^{1} $. 
	\item If a family $ (X_{i})$ is bounded in $ \mathcal{L}^{p} $, $ p>1$, then it is also UI.
\end{itemize}

\begin{boxlemma}\label{lemma: UI a.s. conv}
	Let $ (X_{n})_{n\in \N}$, $ X$ be in $ \mathcal{L}^{1} $ and $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow}$ in $ \mathcal{L}^{1} $ if and only if $ (X_{n})_{n\in \N}$ is UI.
\end{boxlemma}

\begin{theorem}\label{thm: cond exp UI family}
	Let $ X\in \mathcal{L}^{1}$. The family $ \{ \mathbb{E}\left[ X| \mathcal{G} : \mathcal{G} \subset \F  \right]\}$ is uniformly integrable (UI).

\end{theorem}
\begin{proof}
    Need to show for all $ \epsilon>0$, there exists $ 
    \lambda$ large enough such that for all $ \mathcal{G}\subset \F$
    \[
    \begin{array}{ll}
	    &\mathbb{E}\left[ || \mathbb{E}\left[ X \mathcal{G} \right] \cdot \mathbf{1}(|\mathbb{E}\left[ X \mathcal{ G} \right] |>\lambda)\right] <\epsilon\\
	    &\leq \mathbb{E}\left[  \mathbb{E}\left[ |X| | \mathcal{G} \right]\cdot \mathbf{1}(|\smash{\underbrace{\mathbb{E}\left[ X| \mathcal{G}|\right]}_{ \mathcal{G}-\text{measurable}}}|>\lambda) \right].
    \end{array}
    \]

    Since $ X\in \mathcal{L}^{1} $, for all $ \epsilon>0$, there exists $ m\delta >0$ such that if $ A\in \F$, $ \PP(A)<\delta$, then $ \mathbb{E}\left[ |X| \cdot \mathbf{1}(A)\right]<\epsilon$. Now, 
    \[
    \begin{array}{ll}
        \PP(|\mathbb{E}\left[ X \mathcal{G} \right]|>
	\lambda) &\stackrel{\text{Markov}}{\leq} \frac{\mathbb{E}\left[ |\mathbb{E}\left[ X  \mathcal{G} \right]| \right]}{\lambda}\\ 
		 &\leq \frac{\mathbb{E}\left[  \mathbb{E}\left[ |X|  \mathcal{G}\right] \right]
}{\lambda} = \frac{\mathbb{E}\left[ |X| \right]}{\lambda}.
    \end{array}
    \]
    Take $\lambda =  \frac{\mathbb{E}\left[ |X| \right]}{\lambda}$, then we are done.
\end{proof}

\begin{boxdef}\label{def: UI mg}
$ X = (X_{n})_{n\in \N}$ is called UI (super/sub) martingale if it is a (super/sub) martingale and $ (X_{n})_{n\geq 0}$ is UI.
\end{boxdef}


\begin{examplesblock}{Examples:}\label{examples: 4}
	Let $ X_{1}, X_{2}, \cdots$ be an iid sequence with $ \PP(X_{1}=0) = \PP(X_{1}=2)=1/2$. Set $ Y_{n} = X_{1}\cdotX_{2}\cdot \cdots \cdot X_{n}$, which can be seen to be a martingale. Also have $ \mathbb{E}\left[ Y_{n} \right]=1$ for all $ n\in \N$ and $ Y_{n}\stackrel{n\in \N}{\longrightarrow}Y_{\infty}=0$ a.s. by the martingale convergence theorem, not \underline{not} in $ \mathcal{L}^{1} $ (because it is not UI).
\end{examplesblock}


\begin{theorem}\label{thm: UI mg L1 conv them}
Let $ X$ be a martingale. Then the following are equivalent: 
\begin{enumerate}
	\item $ X$ is UI.
	\item $ X$ converges a.s. and in $ \mathcal{L}^{1} $ to $ X_{\infty}$ as $ n\to \infty$.
	\item There exists $ Z\in \mathcal{L}^{1} $ such that $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$ for all $ n\geq 0$.
\end{enumerate}


\end{theorem}

\begin{proof}
	\underline{$ 1)\implies 2)$:} $ X$ is bounded in $ \mathcal{L}^{1} $ implies (by the martingale convergence theorem), $ X_{n}\to $ a.s. Since $ X_{n}$ is UI, then $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $.\\ 

	\underline{$ 2)\implies 3)$:} Set $ Z= X_{\infty}$. Need to show: $ X_{n} = \mathbb{E}\left[ X_{\infty}|\F_{n} \right]$ a.s. Indeed, 
	\[
	\begin{array}{ll}
		\norm{X_{n}-\mathbb{E}\left[ X_{\infty}|\F_{n} \right]}_{1} &\stackrel{m\geq n}{=} \norm{ \mathbb{E}\left[ X_{m}-X_{\infty} |\F_{n}\right]}_{1} \\ 
									    &\leq \norm{X_{m}-X_{\infty}}_{1}\stackrel{m\to \infty}{\longrightarrow} 0. 
	\end{array}
	\]

	\underline{$ 3)\implies 1)$:} The tower property implies $ (X_{n})_{n\in \N}$ is a martingale and the previous theorem implies that $ (X_{n}_{n\in \N})$ is UI.
\end{proof}


\begin{remark}
    \begin{enumerate}
	    \item We get as before, $ X_{\infty} = \mathbb{E}\left[ Z|\F_{\inft} \right]$ a.s., where $ \F_{\infty} = \sigma(X_{n}:n\geq 0)$. 
	    \item It $ X$ were a UI super/sub martingale, then we would get $ \mathbb{E}\left[ X_{\infty}|\F_{n} \right]\stackrel{\geq \text{sub}}{\leq} X_{n}$ (check!).
    \end{enumerate}
    
\end{remark}


$ X$ is UI implies $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $ and a.s. Now let $ T$ be a stopping time. We can then define 
\[
X_{T} =\displaystyle\sum^{\infty}_{n=0}X_{n}\cdot \mathbf{1}(T=n) + X_{\infty}\cdot \mathbf{1}(T=\infty).
\]

\begin{theorem}[Optional stopping theorem for UI martingales]\label{thm: OST for UI mg}
Let $ X$ be a UI martingale and let $ S, T$ be stopping times with $ S\leq T$. Then 
\[\mathbb{E}\left[ X_{T}|\F_{S} \right] = X_{S} \quad \text{a.s.}
\]

\end{theorem}

\begin{proof}
    We know that $ X_{n} = \mathbb{E}\left[ X_{\infty}\F_{n} \right]$ a.s. since $ X$ is UI. It suffices to prove that for any stopping times $ T$, $ \mathbb{E}\left[ X_{\infty}|\F_{T} \right] = X_{T}$ a.s. Indeed, $ \mathbb{E}\left[ X_{T}|\F_{S} \right] = \mathbb{E}\left[ \mathbb{E}\left[ X_{\infty}|\F_{T} \right]|\F_{S} \right]$ and since $ S\leq T$we have $ \F_{S}\subseteq \F_{T}$ and hence the tower property would give: 
    \[
    \mathbb{E}\left[ X_{T}|\F_{S} \right] = \mathbb{E}\left[ X_{\infty}|\F_{S} \right] = X_{S}
    \]
    a.s. 
    Thus, we need to show: for all $ T$ stopping times, $ \mathbb{E}\left[ X_{\infty}|\F_{T} \right] = X_{T}$ a.s. 

    \begin{enumerate}
	    \item \underline{NTS:} $ X_{T}\in \mathcal{L}^{1} $:
		    \[
		    \begin{array}{ll}
		        \mathbb{E}\left[ |X_{T}| \right] &=\displaystyle\sum^{\infty}_{n=0}\mathbb{E}\left[ |X_{n}\cdot \mathbf{1}(T=n)| \right]+ \mathbb{E}\left[ |X_{\infty}|\cdot \mathbf{1}(T=\infty) \right]  \\
							 &\stackrel{\text{have $ X_{n}=\mathbb{E}\left[ X_{\infty}|\F_{n} \right]$}}{\leq}\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[  \mathbb{E}\left[ |X_{\infty}\F_{n}| \right]\cdot \overbrace{\in \F_{n}}^{\mathbf{1}(T=n)} \right]\\ 
							 &+ \mathbb{E}\left[ |X_{\infty}\cdot \mathbf{1}(T=\infty)| \right]\\ 
							 &=\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[ |X_{\infty}|\cdot \mathbf{1}(T=n) \right] + \mathbb{E}\left[ |X_{\infty}\cdot \mathbf{1}(T = \infty)| \right]\\ 
							 &= \mathbb{E}\left[ |X_{\infty}| \right]<\infty
		    \end{array}
		    \]
		    as $ X_{\infty}\in \mathcal{L}^{1} $. It is also not hard to check that $ X_{T}$ is $ \F_{T}-$measurable.
\item \underline{NTS:} for all $ B\in \F_{T}$: $ \mathbb{E}\left[  X_{\infty}\cdot \mathbf{1}(B) \right] = \mathbb{E}\left[ X_{T}\cdot \mathbf{1}(B) \right]$
	\[
	\begin{array}{ll}
		\mathbb{E}\left[ X_{T}\cdot \mathbf{1}(B) \right] &=\displaystyle\sum^{\infty}_{n=0} \mathbb{E}\left[ X_{n}\cdot \underbrace{\mathbf{1}(T=n)\cdot \mathbf{1}(B)}_{\in \F_{n}} \right]  \\
								  &+ \mathbb{E}\left[ X_{\infty} \\\cdot \mathbf{1}(T=\infty)\cdot \mathbf{1}(B)\right]\\ 
								  &=\displaystyle\sum^{\infty}_{n=0}\mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(T=n) \cdot \mathbf{1}(B)\right]\\ 
								  &= \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(B) \right]
	\end{array}
	\]
	
    \end{enumerate}

\end{proof}


\begin{boxdef}[Backwards martinagles]\label{def: backwards mg}
Let $ \cdots \subseteq \mathcal{G}_{-2}\subseteq \mathcal{G}_{-1}\subseteq \mathcal{G}_{0} $ be a decreasing family of sub sigma algebras of $ \F$. We call $ X = (X_{n})_{n\leq 0}$ a \underline{backwards martingale} if $ X_{o}\in \mathcal{L}^{1} $ and for all $ n\leq -1$ $ \mathbb{E}\left[ X_{n+1}| \mathcal{G}_{n} \right] = X_{n}$ a.s. By the tower property, $ \mathbb{E}\left[ X_{0} | \mathcal{G}_{n} \right] = X_{n}$ for all $ n\leq 0 $. Since $ X_{0}\in \mathcal{L}^{1} $, a backwards martingale is automatically UI. 
\end{boxdef}


\begin{theorem}[ $ \mathcal{L}^{p}$/a.s. backwards martingale convergence theorem]\label{thm: backwards mg conv theorem}
	Let $ X$ be a backwards martingale with $ X_{0}\in \mathcal{L}^{p} $, $ 1\leq p <\infty$. Then $ X_{n}\to X_{-\infty}$ as $ m\to -\infty$ a.s. and in $ \mathcal{L}^{p} $ and $ X_{-\infty} = \mathbb{E}\left[ X_{o}| \mathcal{G}_{-\infty} \right]$ a.s., where $ \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\leq 0} \mathcal{G}_{n} $.
\end{theorem}


\begin{proof}
	Set $ \F_{k} = \mathcal{G}_{-n+k}$, $ 0\leq k \leq n$. This is an increasing filtration and $ (X_{-n+k})_{0\leq k \leq n}$ is $ \F_{k}-$martingale. Let $ N_{-n}([a,b], X)$ be the number of upcrossings of the interval $ [a,b]$ between $ -n$ and $ 0$. Doob's upcrossing inequality gives:
	\[
		(b-a)\cdot \mathbb{E}\left[ N_{-n}([a,b], X) \right]\leq \mathbb{E}\left[ (X_{n}-a)^{-} \right].
	\]
As before, we get that $ X_{n}\to X_{-\infty}$ as $ n\to -\infty$ a.s. We also have $ X_{-\infty} $is $ \mathcal{G}_{-\infty}-$measurable. Also observe that n$ X_{o}\in \mathcal{L}^{p} $ implies $ X_{n}\in \mathcal{L}^{p} $ for all $ n\leq 0$.\\ 

\mymark{Lecture 9} $ X_{n} = \mathbb{E}\left[ X_{n}| \mathcal{G}_{n} \right]$ a.s. (backwards martingale). If $ X_{n}\in \mathcal{L}^{p} $, $ p\in [1,\infty)$ $ X_{n\to X_{-\infty}}$ a.s. $ n\to -\infty$ a.s. and $ X_{-\infty}$ is $ \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\leq 0} \mathcal{G}_{n}-$measurable.

\end{proof}

Observe we have that $ X_{n}\in \mathcal{L}^{p} $ by conditional Jensen and using Fatou, we obtain
$ X_{-\infty}\in \mathcal{L}^{p} $. Now we need to show that $ X_{n}\to X_{-\infty}$ in $ \mathcal{L}^{p} $. Indeed, 

\[
\begin{array}{ll}
    |X_{n}-X_{-\infty}|^{p} &= |\mathbb{E}\left[ X_{0}| \mathcal{G}_{n} \right]-\mathbb{E}\left[ X_{-\infty}| \mathcal{G_{n}} \right]|^{p} \\
			    & = |\mathbb{E}\left[ X_{0]-X_{-\infty}| \mathcal{G}_{n}} \right]|^{p} \\ 
			    &\stackrel{\text{Jensen}}{\leq}\displaystyle\underbrace{\mathbb{E}\left[ |X_{0}-X_{-\infty}|^{p} | \mathcal{G }_{n}\right]}_{\text{UI family}}. 
\end{array}
\]
Hence, $ (|X_{n}-X_{-\infty}|^{p})_{n\leq 0}$ is UI, hence giving $ \mathcal{L}^{1} $ convergence.\\

\underline{NTS:} $ X_{-\infty} = \mathbb{E}\left[ X_{o}| \mathcal{G_{-\infty}} \right]$ a.s.\\ 

Let $ A\in \mathcal{G}_{-\infty} =\displaystyle\bigcap_{n\geq 0} \mathcal{G}_{n} $ implies that $A \in \mathcal{G}_{n}$ for all $ n\leq 0$. Hence, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{0}\cdot \mathbf{1}(A) \right]$, for all $ n\leq 0$. Take $ n\to -\infty$ and use $ \mathcal{L}^{1} $ convergence to get $ \mathbb{E}\left[ X_{-\infty}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{0}\cdot \mathbf{1}(A) \right]$ to conclude.


\section{Applications of martingales}\label[{sec: applications of mgs}

\begin{theorem}[Kolmogorov's $ 0-1$ law]\label{thm: Kolmogorov's 0-1}
	Let $ (X_{i})$ be iid and for all $ n\in \N$, $ \F_{n} = \sigma(X_{k}:k\geq n)$, $ \F_{\infty} =\displaystyle\bigcap_{n\geq 0}\F_{n} $. Then, $ \F_{\infty}$ is trivial, i.e. for all $ A'in 'F_{\infty}$, $ \PP(A)\in \{0,1\}$.
\end{theorem}


\begin{proof}
    Let $ A\in \F_{\infty}$. Define $ \mathcal{G_{n } = \sigma(X_{n }: k\leq n)}$ and $ \mathcal{G}_{\infty} = \sigma (\mathcal{G}_{n}, n\geq )$. Now, we have that $ \mathbb{E}\left[ \mathbf{1}(A)| \mathcal{G}_{n} \right]$ is a martingale and 
    \[
	    \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G}_{n}\right] \stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ \mathbf{1}(A)| \mathcal{G}_{\infty} \right] \quad \text{ a.s.}
    \]
    Now, $ A\in \F_{\infty}$ implies that $ A\in \F_{n+1}$ and also have $ \mathcal{G}_{n}\perp \F_{n+1}$ and $ \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G}_{n}\right] = \PP(A)$ a.s., $ \mathbb{E}\left[ \mathbf{1}(A) | \mathcal{G_{\infty}} \right] = \mathbf{1}(A)$ a.s. since $ \F_{\infty}\subseteq \mathcal{G}_{\infty}$ implies that $ A\in \mathcal{G}_{\infty}$. So $ \PP(A) = \mathbf{1}(A)$ a.s. finally giving $ \PP(A) \in \{0,1\}$.   
\end{proof}


\begin{theorem}[Strong law of large numbers]\label{thm: slln}
Let $ (X_{i})_{i\in I}$ be an iid sequence in $ \mathcal{L}^{1} $ with $ \mathbb{E}\left[ X_{1} \right]$. Define $ S_{n} = X_{1}+\cdots X_{n}$. Then $ \frac{S_{n}}{n}$ converges a.s. and in $ \mathcal{L}^{1} $ to $ \mu$ as $ n\to \infty$ a.s.
\end{theorem}

\begin{proof}
    Define $\mathcal{G} = \sigma(S_{n}, S_{n+1}\cdots)  = \sigma(S_{n}, X_{n+1}, \cdots)$. For $ n\leq -1$, $ M^{n} = \frac{S_{-n}}{-n}$. We will show that $ (M_{n})_{n\leq -1}$ is a backwards martingale with respect to $ (\mathcal{G}_{-n}_{n\leq -1} )$. Indeed, 
    \[
    \begin{array}{ll}
	    \mathbb{E}\left[ M_{m+1}|\mathcal{G}_{-m}  \right] &= M_{-m}  \quad \text{a.s. for } m\leq -1\\         &= \mathbb{E}\left[ \frac{S_{-m-1}}{-m-1} |\mathcal{G}_{-m}  \right] \stackrel{\tet{set }n = -m}{=} \mathbb{E}\left[ \frac{S_{n-1}}{n-1} | \mathcal{G}_{n}  \right]\\ 
							       & = \mathbb{E}\left[ \frac{S_{n-1}}{n-1}| S_{n-1}, X_{n+1}\cdots \right]\\ 
							       &= \mathbb{E}\left[ \frac{S_{n}-X_{n}}{n-1}S_{n} \right] \\ 
							       & = \frac{S_{n}}{n-1}-\mathbb{E}\left[ \frac{X_{n}}{n-1}|S_{n} \right].
    \end{array}
    \]
    Now since $ S_{n} = X_{1} +\stackrel{\text{iid}}{\cdots}+X_{n}$, we have that $ \mathbb{E}\left[ X_{k}|S_{n} \right]= \mathbb{E}\left[ X_{1}| \right]S_{n}$ and so $ \frac{S_{n}}{n-1}-\frac{1}{n-1} \left( \frac{S_{n}}{n} \right) = \frac{S_{n}}{n-1} \left( \frac{n-1}{n} \right) = \frac{S_{n}}{n}$. Hence $ \frac{S_{n}}{n} \stackrel{n\to \infty}{\longrightarrow} Y$ a.s. and in $ \mathcal{L}^{1} $measurable for all $ k\geq 0$. Thus $ Y$ is $\displaystyle\underbrace{\bigcap_{k} \sigma(X_{k+1}, \cdots )}_{\text{Kolmogorov 0-1 law}\implies \text{trivial}}-$measurable. So there exists $ c\in \R$ such that $ \PP(Y = c)  = 1$. So $ \frac{S_{n}}{n}\stackrel{n\to \infty}{\longrightarrow}$ in $ \mathcal{L}^{1} $ and hence $ c = \mathbb{E}\left[ Y \right] = \lim_{i \to \infty} \mathbb{E}\left[ \frac{S_{n}}{n} \right] = \mu$ and so finally $ c = \mu$. 
\end{proof}

\begin{theorem}[Radon-Nikodym Theorem]\label{thm: radon nokodym thm}
Let $ P$ and $ Q$ be two probability measures on the space $ (\Omega, \F, \PP)$. Suppose that $\F $ is countable generated, i.e. there exists a sequence $ (F_{n})_{n\in \N}$ such that $ \F = \sigma(F_{n}: n\in \N)$. Then the following are equivalent: 

\begin{enumerate}
	\item For all $ A\in \F$, $\PP(A) = 0$ implies $ Q(A) = 0$. $ (Q<<P)$.
	\item For all $ \epsilon >0$, there exists $ \delta>0$ such that if $ A\in \F$ with $ \PP(A)<\delta$, then $ Q(A)<\epsilon$. 
	\item There exists a non-negative random variable $ X$ such that $ Q(A) = \mathbb{E}\left[ X\cdot \mathbf{1}(A) \right]$, for all $ A\in \F$.
\end{enumerate}

\end{theorem}

\begin{remark}
$ X$ is called a version of the \underline{Radon-Nikodym derivative} of $ Q$ with respect to $ P$, or $ X = \frac{\diff  Q}{\diff  P}$ on $ \F $ a.s.
\end{remark}


\begin{proof}
	\underline{$ 1)\implies 2):$} Suppose $ 2)$ does not hold, then there exists an $ \epsilon>0$ such that for all $n\in \N$, there exist $ A_{n}$ with $ P(A_{n})\leq \frac{1}{n^{2}}$ and $ Q(A_{n})\geq \epsilon$. Now, since $\displaystyle\sum^{\infty}_{n=1}P(A_{n})<\infty$ Borel-Cantelli implies $ P(A_{n}\text{ i.o})=0 $ and so $ Q(A_{n}) = 0$. However, 
	\[
	\begin{array}{ll}
	\{A_{n} \text{i.o.} \} &= \bigcap_{n} \bigcup_{k\geq n} A_{k} \implies Q(A_{n}\text{ i.o})  \\
	     &= \lim_{n \to \infty} Q \left( \bigcup_{k\geq n } A_{n}\right)\\ 
	     &\geq  \lim_{n \to \infty} Q(A_{n})\geq \epsilon \,
	\end{array}
	\]
a contradiction.	

\underline{$ 3) \implies 1):$} trivial. \\ 

\underline{$ 2) \implies 3):$} Let $ \mathcal{A}_{n} = \{H_{1}\cap \cdots\cap H_{n}:H_{i} = F_{i} \text{ or } F_{i}^{c} \text{ for all } i \}$. In other words $ \mathcal{A}_{n} = \left\{F_{1}, F_{2}, \cdots, F_{n}, \bigcup_{k\geq n} F_{k}\right\}$. Let $ \F_{N} = \sigma (\mathcal{A}_{n})$, so $ \F_{n}$ is a filtration.\\ 

Now defined
\[
X_{n}(\omega) =\displaystyle\sum_{A\in \mathcal{A_{n}}}\frac{Q(A)}{P(A)} \cdot \mathbf{1}(\omega \in A). 
\]
Thus, for all $ A|in \F_{n}$, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right] = Q(A) = \stackrel{F_{n}\subseteq\F_{n+1}}{=} \mathbb{E}\left[ X_{n+1}\cdot \mathbf{1}(A) \right]
$. So $ (X_{n})_{n\in \N}$ is indeed a martingale. Furthermore $ \mathbb{E}\left[ X_{n} \right] = Q(\Omega)=1$ (and since $ X_{n}\geq 0$ for all $ n\geq 0$), we have that $ X_{n}$ is an $ \mathcal{L}^{1 }$ bounded martingale. Thus, $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s.\\ 

Now we show that $ (X_{n})_{n\in \N}$ is UI: 
\[
\begin{array}{ll}
	\PP(X_{n}\geq \lambda)&\leq 1/\lambda<\infty \\
     &\leq \delta
\end{array}
\]
using Markov's inequality and taking $ \lambda = 1/\delta$. Thus, $ \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(X_{n}\geq \lambda) \right] = Q(X_{n}\geq \lambda)<
epsilon$. Thus $ (X_{n})_{n\in \N}$ is UI and so $ X_{n}\to X_{\infty}$ in $ \mathcal{L}^{1} $.\\ 

Now define $ \tilde{Q}(A) = \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right]
$. Want to show: $ \tilde(Q)(A) = Q(A)$ for all $ A\in \F$. Indeed, we have $ X_{n} = X_{\infty}|\F_{n}$. Now if we let for a moment $ A \in \bigcup_{n\geq 0}\F_{n} $, there exists some $ N\in \N$ such that $ A\in \F_{N}$. Thus, 
\[
	\displaystyle\underbrace{\mathbb{E}\left[ X_{N}\cdot \mathbf{1}(A) \right] }_{= Q(A)} =\displaystyle\underbrace{\mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right] }_{=\tilde{Q}(A)}.
\]

Hence, $ Q = \tilde{Q}$ on a $\pi-$system, $ \left( \bigcup_{n} \F_{n} \right)$, that generates $ \F$, and by the extension theorem we have that $ Q \equiv \tilde{Q}$ everywhere.
\end{proof}

\mymark{Lecture 10}\section{Continuous Time processes}\label{sec: cont time processes}
Let $X  =(X_{n})_{n\in \N} $ be a process, that is for all $ n\in \N$ $ X_{n}$ is a random variable on some underlying probability space $ (\Omega, \F ,\PP))$. $ X$ can also be viewed as the map
\[
X:(\omega,n)\mapsto X_{n}(\omega).
\]
and observe that this map is $ \F\otimes \mathcal{P}(\N) = \sigma(\{A\times \{k\}: A\in \F, k\in \N\})$ as long as $ X_{n}$ is $ \F-$measurable for all $ n\in \N$. Now we consider random variables taking values in the spaces $ \R^{d}$, $ d\geq 1$. \\ 

\begin{boxdef}[Stochastic process]\label{def: stoch proces}
	The family $ (X_{t})_{t\in \R_{+}}$ is called a \underline{stochastic process} if for all $ t$ positive $ X_{t}$ is a random variable.
\end{boxdef}


\begin{remark}
    The map $ X:(\omega, t)\mapsto X_{t}(\omega)$ need not be $ \F\otimes  \mathcal{B}(\R_{+})-$measurable. 
\end{remark}

\underline{Claim:} If for all $ \omega \in \Omega$, $ \mapsto X_{t}(\omega)$ is a continuous function for $ t\in(0,1]$, then the map $ X:(\omega, t)\mapsto X_{t}(\omega)$ is $ \F\otimes  \mathcal{B}(\R_{+})-$measurable. \\ 

Indeed, by continuity we can write 

\[
	X_{t}(\omega) = \lim_{n \to \infty}\displaystyle\overbrace{\displaystyle\sum^{2^{n}-1}_{k=0}  \mathbf{1}(t\in (k\cdot 2^{-n}, (k+1)\cdot 2^{-n}])X_{k\cdot 2^{-n}}(\omega) }^{\text{for all }n \text{this sum is} \F\otimes \mathcal{B}((0,1])-\text{meas.} }
\]
Thus $ X$ is measurable with as a limit of measurable functions.\\ 

From now onwards, we will always (unless otherwise stated) assume that $ X$ is right-continuous and admits left limits, almost everywhere. \underline{We call such processes cadlag}.


We now revisit some of the earlier definition we have made in the discrete setting and extend the to the continuous case. A \underline{filtration} is an increasing family of sigma algebras $ (\F_{t})_{t\in \R_{+}}$ whenever $ t\leq t'$. We say $ X$ is adapted to the filtration above if $ X_{t}$ if $ \F_{t}-$measurable for all $t\in \R_{+}$. A random variable $ T:\Omega \to [0,\infty]$ is called a \underline{stopping time} if for all $ t$, $ \{T\leq t\}\in \F_{t}
$. Define $ \F_{T} = \{A|in \F: A\cap \{T\leq t\}\in \F_{t} \text{ for all } t\}$ and $ A|in \mathcal{B}(\R)$. Furthermore, $ T_{A} = \displaystyle\inf_{t\geq 0 : X_{t}\in A}$ is \underline{not} always a stopping time.\\ 
\[
	\{T_{A}\leq t\} = \bigcup_{s\leq t} \{X_{s}\in A\}\,
\]
an uncountable union so not immediately clear whether it in $ \F_{t}$.\\ 

\begin{examplesblock}{Examples: }\label{examples: 5}

		\begin{wrapfigure}{r}{0.3\textwidth}
		    \centering
		    \includesvg[width = 0.8\linewidth]{images/stopping time counterexample.svg}
		    \caption{Illustration of $ X$.}
		    \label{fig: counterexample stopping time}
		\end{wrapfigure}

	Let $ J= \left\lbrace
	\begin{array}{@{}l@{}}
		1, \text{ with probability} \frac{1}{2} \\
		-1, \text{ with probability} \frac{1}{2}
	\end{array}\right.$
	and 
	\[	X_{t}(\omega) = \left\lbrace
	\begin{array}{@{}l@{}}
		t, \quad t\in [0,1] \\
	1+ J(t-1), \quad t>1.
	\end{array}\right.
	\]
	Let $(\F_{t})_{t\geq 0} = (\F^{X}_{t})_{t\geq 0}$ and fix $ A\in (1,2)$. Then $ \{T_{A}\leq 1\}\not in \F_{1} = \{\emptyset, \Omega\}$, since $\{T_{A}\leq 1\} = \{J = 1\}$.
\end{examplesblock}

Again, we say $ X^{T}_{t} = X_{T\land t}$, $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ whenever $ T(\omega)<\infty$.

\begin{boxprop}\label{prop: stopping times continuous}
	Let $ S,T$ be stopping times and $ X$ a cadlag adapted process. Then
\begin{enumerate}
	\item If $ S\leq T$, then $ \F_{S}\subseteq \F_{T}$.
	\item $ S\land T$ is a stopping time.
	\item $ X_{T}\cdot \mathbf{1}(T<\infty)$ is $ \F_{T}-$measurable.
	\item $ X^{T}$ is adapted.
\end{enumerate}

\end{boxprop}


\begin{proof}
    $ 1), 2)$ are clear (check!) and $ 4)$ is immediate from $ 3)$, since $ X_{T\land t}$ if $ \F_{T\land t}-$measurable and $\F_{T\land t} \subseteq \F_{t}$.\\ 

\underline{proof of $ 3)$:} \underline{Claim:} a random variable $ Z$is $ \F_{T\land t}-$measurable if and only if $ Z\cdot \mathbf{1}(T\leq t)$ is $ \F_{t}-$measurable for all $ t\geq 0$. Indeed, \\ 

\underline{$ \impliedby$):} is true by definition. \\ 

\underline{$ \implies):$} if $ Z = c\cdot \mathbf{1}(A)$, $ A\in \F$, then $ A\in \F_{T}$ which means that $ Z$ is $ \F_{T}-$measurable. Now, if $ Z =\displaystyle\sum_{i}c_{i}\cdot \mathbf{1}(A_{i}) $, a finite sum with $ c_{i}>0$, $ A_{i}\in \F$, then $ Z$ is $ \F_{T}-$measurable.\\ 

\underline{$ Z$ general ($ \geq 0$)}: let $ Z_{n}\uparrow Z$, where 
\[
	Z_{n} = 2^{-n}\lfloor 2^n Z\rfloor \land n, \quad \text{ for all } n \in \N.
\]
Observe that $ Z_{n}$ are simple for all $ n$ and so by the previous steps $ Z_{n}$ is $ \F_{T}-$measurable and hence so is $ Z$, being an a.s. pointwise limit of measurable functions.\\ 

The case for completely general $ Z$ follows by decomposing $ Z = Z^{+}-Z^{-}$, $ Z^{+} = Z \lor, Z^{-} = (-Z)\lor 0$ and apply the previous case to $ Z^{+}, Z^{-}$.\\ 

Now, by the above claim, it suffice to show: $ X_{T}\cdot \mathbf{1}(T\leq t)$ if $ F_{t}$ measurable for all $ t$. We have $ X_{T} \mathbf{1}(T\leq t) = X_{T}\cdot \mathbf{1}(T<t)+X_{t}\cdot \mathbf{1}(T=t)$. Hence, it suffices to show  that  $ X_{T}\cdot \mathbf{1}(T< t)$ if $ F_{t}$ measurable for all $ t$.\\ 

Define $ T_{n} = 2^{-n}\lceil 2^n T \rceil $, stopping times since 
\[
\begin{array}{ll}
	\{T_{n}\leq t\} &= \{\lceil 2^n T\rceil \leq 2^n t\} \\
			&= \{2^n T\leq \lfloor 2^n t\rfloor\} = \{T\leq 2^{-n}\lfloor 2^n T\rfloor\}\\ 
			&\in \F_{2^{-n}\lfloor 2^n T\rfloor}\subseteq \F_{t}.
\end{array}
\]
Also, $ T_{n}\downarrow T$, as $ n\to \infty$. Now by the cadlag property of $ X$,\\ 
$ X_{T}\cdot \mathbf{1}(T<t) = \lim_{n \to \infty} X_{T_{n}\land t}\cdot \mathbf{1}(T<t)$.\\ 

Furthermore, $ T_{n}$ takes values in $ \mathcal{D}_{n}= \{k\cdot 2^{-n}, k\in \N\}$. Now, 
\[
\begin{array}{ll}
	X_{T_{n}\land t}\cdot \mathbf{1}(T<t) &=\displaystyle\sum_{d\in \mathcal{D}_{n}, d\leq t} \overbrace{X_{d}\cdot \mathbf{1}(T_{n} = d)\cdot \mathbf{1}(T<t)}^{\F_{t}-\text{meas.}} \\
     &+ X_{t}\cdot \mathbf{1}(T_{n}=t)\cdot \mathbf{1}(T<t).
\end{array}
\]
Hence, $ X_{T}\cdot \mathbf{1}(T<\infty)$ is $ \F_{t}-$measurable as a limit of $ \F_{t}-$measurable functions.


\end{proof}


\begin{boxprop}\label{prop: stopping time cont process}
	Let $ X$ be a continuous and adapted process and let $ A$ be a closed set. Then $ T_{A} = \{t\geq 0: X_{t}\in A\}$ is a stopping time.
\end{boxprop}


\begin{proof}
	\underline{Need to show:} $ \{T_{A}\leq t\} = \left\{ \displaystyle\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0 \right\}$.

	\underline{$ (\subseteq ):$} $ d(x,A)=$distance of $ x$ from $ A$. Let $ T_{A} = s\leq t$, then there exists a sequence $ s_{n}\downarrow s$, such that $ X_{S_{n}}\in A$. Since $ A$ is closed, we have $ d(X_{s}, A) = 0$ and $ X_{s_{n}}\to X_{s}$, as $ n \to \infty$. Again $ A$ being closed implies that $ d(X_{s}, A) = 0$. The continuity of $ X$ and $ d(\cdot, A)$ means that there exists another sequence $ (q_{n})_{n\in \N}\subseteq \Q$ such that $ q_{n}\uparrow s$ such that $ d(X_{q_{n}}, A)\to 0$ hence $\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0$. \\ 

	\underline{($\supseteq  $):} If $\inf_{s\in \Q, s\leq t} d(X_{s}, A) = 0$, then there exists a sequence $ (s_{n})_{n\in \N}$ such that $ s_{n}\leq t$ for all $ n$ and $ d(X_{s_{n}, A}\to 0)$ as $ n\to \infty$. Then by compactness, there exists a convergent subsequence of $ s_{n}\to s$ (without relabelling), such that $ s\leq t$ and $ d(X_{s_{n}, A})\to 0$ as $ n\to \infty$ and by continuity we obtain $ d(X_{s}, A) =0$, hence $ X_{s}\in A$ and so $ T_{A}\leq t$.

\end{proof}

\begin{boxdef}\label{def: future sigma algebra}
	Given a filtration $ (\F_{t})_{t\geq 0}$, we define $ \F_{t^{+}}= \bigcap_{s>t} \F_{s}$, for all $ t\geq 0$. Observe that $ (\F_{t^{+}})_{t\geq 0}$ is a filtration. If for all $ t\geq 0$, $ \F_{t^{+}}$, we say $ (\F_{t})_{t\geq 0}$ is right-continuous. 
\end{boxdef}


\mymark{Lecture 11}\begin{boxprop}\label{prop: open set stopping time wrt future filtration}
	Let $ X$ be a continuous process, and $ A$ be an \underline{open} set. Then 
	\[
		T_{A} = \displaystyle\inf\{t\geq 0: X_{t}\in A\}
	\]
	is a stopping time with respect to the filtration $ (\F_{t^{+}})_{t\geq 0}$.
\end{boxprop}


\begin{proof}
	\underline{Need to show: } for all $ t\geq 0$, $ \{T_{A}\leq t\}\in \F_{t^{+}}$. Have, 
\[
\begin{array}{ll}
	\{T_{A}<s\} &= \displaystyle\bigcup_{q\in \Q, q<s}\displaystyle\underbrace{X_{q}\in A}_{\in \F_{s}}\in \F_{s}  \\
	\{T_{A}\leq t\} &= \bigcap_{n}\displaystyle\underbrace{\{T_{A}<t+\frac{1}{n}\}}_{\in \F_{t+\frac{1}{n}}} \in \F_{t^{+}}.
\end{array}
\]
\end{proof}
Let $ (X_{t})_{t\geq 0}
$ be a stochastic process. It can be viewed, as a random element in the space of functions $ \{f:\R_{+}\to E\}$ endowed with the product sigma-algebra making all projections measurable. Further, let $ \mathcal{C}(\R_{+}, E)$ be the space of all continuous functions and $ \mathcal{D}(\R_{+}, E)$ the space of all cad lag functions. Endow the spaces $ \mathcal{C}, \mathcal{D}$ with the sigma algebra that makes all projections $ \pi_{t}: f\mapsto f_{t}$ measurable for all $ t\geq 0$. This sigma algebra is generated by the cylinder sets 
\[
	\left\{ \displaystyle\bigcap_{s\in J} \{f_{s}\in A_{s}: \text{for all } T\subseteq \R_{+}, \text{ finite}, A_{s}\in \mathcal{B}(E)\} \right\}.
\]
For $ A$ in the product sigma algebra, we write $ \mu(A) = \PP(X\in A)$ and we call $ \mu$ the law of $ X$. (``$ X_{*}\PP = \mu$``). For every $ J$ finite subset of $ \R_{+}$, write $ \mu_{J}$ for the law of $ (X_{t})_{t\in J}$. The measures $ (\mu_{J})$ are called the finite dimensional marginals of $ X$. The $ \mu_{J}$ completely characterise the law of $ \mu$. This follows because the sets above form a $\pi-$system that generates the sigma fields previously mentioned.


\begin{examplesblock}{Examples: }\label{examples: 6}
	Let $ X = 0$ for all $ t\in [0,1]$ and $ U\sim [0,1]$ (uniform) and $ X_{t'} = \mathbf{1}(U = t)$ for $ t\in [0,1]$. Both of them have the same finite dimensional distributions which are Dirac masses at zero, but the processes are not \underline{equal}. 
	\[
	\begin{array}{ll}
		\PP(X_{t} = 0 \text{ for all } t\in [0,1])) &= 1 \\
							   \PP(X'_{t} = 0 \text{ for all } t\leq 1) &= 0. \quad \text{ But, }\\ 
 \PP(X_{t } = X'_{t}) &= 1 \quad \text{ for all } t\in [0,1]. 
	\end{array}
	\]
\end{examplesblock}

\begin{boxdef}\label{def: version processes}
Let $ X$ and $ X'$ be two processes on $ (\Omega, \F, \PP)$, we say $ X'$ is a version of $ X$ if ($X_{t} = X'_{t}$ a.s.) for all $ t$. That is 
\[
\text{For all} t\geq 0: \PP(X_{t} = X'_{t})=1.
\]
\end{boxdef}


\begin{boxdef}\label{def: sigma null sets}
Fix a filtered probability space $ (\Omega, \F, (\F_{t})_{t\geq 0}, \PP)$. Set $ \mathcal{N}$ to be the collection of sets of measure zero. Furthermore, set 
\[
	\tilde{\F}_{t} = \sigma(\F_{t}, \mathcal{N})
\]
for all $ t\geq 0$. If for all $ t$, $ \F_{t} = \tilde{\F}_{t}$, we say that $ (\F_{t})_{t\geq 0}$ satisfies the usual conditions. 
\end{boxdef}


\begin{theorem}[Martingale regularisation theorem]\label{thm: mg reg thm}
	Let $ (X_{t})_{t\geq 0 }$ be a martingale wrt $ (\F_{t})_{t\geq 0}$. Then, there exists a cadlag process $ (\tilde{X}_{t})_{t\geq 0}$ satisfying for all $ t\geq 0$:
	\[
		X_{t} = \mathbb{E}\left[ \tilde{X}_{t}| \F_{t} \right] \quad \text{a.s.}
	\]
	and $ X$ is a martingale with respect to the augmented filtration $ (\tilde{\F}_{t})_{t\geq 0}$. If $ (\F_{t)_{t\geq 0}}$ satisfies the usual conditions, then $ \tilde{X}$ is a version of $ X$. 
\end{theorem}

We start with a Lemma
\begin{boxlemma}\label{lemma: mg reg lemma}
Let $ f: \Q_{+}\to \R$ such that for all $ I\subseteq \Q_{+}$ bounded, $ f$ is bounded on $ I$ and for any $ a<b, a,b,\in \Q_{+}$, for all $ I$ bounded and suppose 
\[
\begin{array}{ll}
	\mathcal{N}([a,b], I, f) = \displaystyle\sup\left\{ n\geq 0 : \text{ there exists } 0<s_{1}<t_{1}<\cdots <s_{n}<t_{n},\right. \\ 
	\left.s_{i}, t_{i}\in I \text{ s.t. } f(s_{i})<a, f(t_{i}>b)\right\}<\infty.
\end{array}
\]
Then, for all $ t\geq 0$, the limits 
\[
\lim_{s\uparrow t, s\in \Q_{+}}f(s), \lim_{s\downarrow t, s\in \Q_{+}}f(s)  
\]
exist and are finite.
\end{boxlemma}

\begin{proof}
	Let $ s_{n}\downarrow t$, the sequence $ (f(s_{n}))$ will converge by the finite upcrossing property (see lemma \ref{lemma: upcrossing lemma}). Now suppose $ t_{n}\downarrow t$ is another such sequence, then combining them (by selecting elements from each sequence in an alternating fashion exploiting convergence) we get a decreasing sequence converging to $ t$ to conclude $ \lim_{n \to \infty} f(s_{n}) = \lim_{n \to \infty}f(t_{n})  $. Finally, $ f$ being bounded gives that both limits are equal and finite.
\end{proof}

\underline{Goal:} To define $ \tilde{X}_{t} = \lim_{s\downarrow t, s\in \Q_{+}} X_{s}$ on a set of measure $ 1$, and zero otherwise. We now outline below the main steps in the proof of Theorem \ref{thm: mg reg thm}. 

\underline{Steps:}
\begin{enumerate}
	\item Show that the limit exists and is finite on a set of measure one. 
	\item Show that $ \tilde{X}$ is $ \tilde{\F}_{t}-$measurable and satisfies $ \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ a.s. for all $ t\geq 0$.
	\item $ \tilde{X}$ is a $ (\tilde{\F}_{t})_{t\geq 0}$ martingale. 
	\item $ \tilde{X}$ is cadlag.
\end{enumerate}

\begin{proof}{(Theorem \ref{thm: mg reg thm})}
   \begin{enumerate}
	   \item Let $ I$ be a bounded subset of $ \Q_{+}$. Need to show that $ \PP\left(\displaystyle\sup_{t\in I}|X_{t}|<\infty\right)=1$. Observe that 
		   \[
		   \displaystyle\sup_{t\in I}|X_{t}| = \displaystyle\sup_{J \subseteq I, J \text{ finite}}\displaystyle\sup_{t\in J}|X_{t}|.
		   \]
		   Now, let $ J = \{j_{1}, \cdots, j_{n}\}\subseteq I$ with $ j_{1}<\cdots j_{n}$ and $ k>\displaystyle\sup I$. Then $ (X_{t})_{t\in J}$ is a discrete time martingale. Hence the maximal inequality in \ref{thm: doob maximal ineq discrete} gives
		   \[
		   \lambda \cdot \PP(\displaystyle\sup_{t\in J}|X_{t}|\geq \lambda)\leq \mathbb{E}\left[ |X_{j_{n}}| \right]\leq \mathbb{E}\left[ |X_{k}| \right]
		   \]	
		    by the martingale property and Jensen. Now taking the limit as $ J\uparrow I$, 
	\[
	 \lambda \cdot \PP\left(\displaystyle\sup_{t\in I}|X_{t}|\geq \lambda\right)\leq \mathbb{E}\left[ |X_{j_{n}}| \right]\leq \mathbb{E}\left[ |X_{k}| \right]
	\]
	So, $ \PP \left(\displaystyle\sup_{t\in I}|X_{t}|\geq \lambda \right) = 1$. Now for $ M\in \N$ define $ I_{M} = \Q_{+}\cap [0,M]$, then by the above, 
	\[
	\PP \left( \displaystyle\bigcap_{M\in \N} \left\{ \displaystyle\sup_{t\in I_{M}}|X_{t}|<\infty \right\} \right) = 1
	\]
	and on the above event, $ X_{t}$  is bounded on bounded intervals of $ \Q_{+}$.

	\mymark{Lecture 12} Let $ a<b$, $ a,b \in \Q_{+}$, $ I\subseteq \Q_{+}$, bounded. Observe that
	\[
		\mathcal{N}([a,b], I, X) = \displaystyle\sup_{I \subseteq I, J \text{ finite}} \mathcal{N}([a,b], J, X).
	\]
	Now, let $ J = \{j_{1}, \cdots, j_{n}\}\subseteq I$ with $ j_{1}<\cdots j_{n}$ and $ k>\displaystyle\sup I$. Then $ (X_{t})_{t\in J}$ is a discrete time martingale. Now, Doob's upcrossing inequality from \ref{lemma: doob upcrossing} gives
	\[
	\begin{array}{ll}
		(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b], J, X) \right] &\leq \mathbb{E}\left[ (X_{j_{n}}-a)^{-} \right] \\
	     &\leq \mathbb{E}\left[ (X_{k}-a)^{-} \right].
	\end{array}
	\]
	By monotone convergence, we get 
\[
(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b], I, X) \right]<\infty.
	\]
	Let $ M\in \N$, $ I_{M} = \Q_{+}\cap [0,M]$ and 
 \[
	 \Omega_{0} = \displaystyle\bigcap_{m\in\N} \left( \displaystyle\bigcap_{a<b, a,b\in\Q} \{ \mathcal{N}([a,b], I_{M}, X)<\infty\} \bigcup \left\{ \displaystyle\sup_{t\in I_{m}} |X_{t}|<\infty \right\}\right).
 \]
 On $ \Omega_{0}$, from lemma \ref{lemma: mg reg lemma}, $ \lim_{s \downarrow tX_{s}} $ exists and we have $\PP(\Omega_{0})=1$. Now, define 
 \[
	 \tilde{X}_{t} = \left\lbrace
\begin{array}{@{}l@{}}
	\lim_{s\downarrow t, s\in \Q_{+}} X_{s}, \quad \text{ on }\Omega_{0}    \\
	0 , \quad\quad\quad\quad\quad \text{ otherwise}. 
\end{array}\right. 
 \]
 Recall $ \tilde{\F}_{t} = \sigma(\F_{t}, \mathcal{N})$ for all $ t\geq 0$. From the definition definition, we see that $ \tilde{X}$ is $ \tilde{\F}-$adapted.\\ 

 It remains to show that $X_{t} =  \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ a.s. and $ \tilde{X}$ is cadlag and a martingale. 

\item Let $ t_{n\downarrow} t$, $ t_{n\in \Q_{+}}$, then 
	\[
		\tilde{X}_{t} = \lim_{n\to \infty}X_{t_{n}} 
	\]
	a.s. Observe that $ (X_{t_{n}})$ is a backwards martingale with respect to the filtration $ (\F_{t_{n}})_{n\in \N}$. SO $ (X_{t_{n}})$ converges a.s. and in $ \mathcal{L}^{1} $. In other words, $ X_{t } = in $ $\mathcal{L}^{1} $. So $ X_{t} = \mathbb{E}\left[ \tilde{X}_{t}|\F_{t}\right]$ a.s.

\item We now prove that $ \tilde{X}$ is a martingale. Let $ s<t$, we need to show that $ \mathbb{E}\left[ \tilde{X}_{t}|\tilde{\F}_{s} \right] = 'tilde{X}_{s}$ a.s.\\ 

	\underline{Claim:} $ \mathbb{E}\left[ X_{t}|\F_{t^{+}} \right] = \tilde{X}_{s}$ a.s. Indeed, first observe that for $ Y$ any random variable and $ \mathcal{G}$ a sigma algebra it follows that
	\[
	\mathbb{E}\left[ Y| \sigma \mathcal{G}, \mathcal{N}) \right] = \mathbb{E}\left[ X | \mathcal{G} \right]
	\]
	which is clear because the conditional expectation is defined almost surely and $ \mathcal{N}$ only contains sets of measure zero.\\ 

Now, fix $ s<t$ and let  $ s_{n}\downarrow s$, $ s_{n}\in \Q_{+}$, $ s_{0}<t$. We have by the  tower property that $ (\mathbb{E}\left[ X_{t}|\F_{s_{n}} \right])_{n\in \N}$ is a backwards martingale and so it converges a.s. and in $ \mathcal{L}^{1} $ to $ \mathbb{E}\left[ X_{t}|\F_{t^{+}} \right]$. But $ \mathbb{E}\left[ X_{t}|\F_{s_{n}} \right] = X_{s_{n}}$ a.s. and $ X_{s_{s}}\to \tilde{X}_{s}$ a.s. as $ n'to \infty$. So $ \tilde{X}_{s} = \mathbb{E}\left[ X_{t}|\F_{s^{+}} \right]$.
	
\item Finally, we show that $\tilde{X}$ is a cadlag. First we show that $ \tilde{X}
	$ is right continuous. Suppose \underline{not}. Then, there exists $ \omega \in \Omega_{0}$ and some $ t\geq 0$ such that $ \tilde{X}(\omega)$ is not right continuous at $ t$. That is there exists a sequence $ s_{n}\downarrow t$ such that $ |\tilde{X}_{s_{n}}-\tilde{X}_{t}|\geq \epsilon >0$ (for some positive $ \epsilon$). By the definition of $ \tilde{X}$, there exists another sequence $ s'_{n}>s_{n}$, for all $ n\in \N$ and $ s^{'}_{n}\downarrow t$, $ s'_{n}\in \Q_{+}$ such that $ |\tilde{X}_{s_{n}}-X_{s_{n}'}|\leq \frac{\epsilon}{2}$. So $ |X_{s^{'}_{n}}-\tilde{X}_{t}|\geq \frac{\epsilon}{2}$, a contradiction since $ s'_{n}\downarrow t$, $ s'_{n}\in \Q_{+}$. The argument for left continuity is entirely analogous. 
	
   \end{enumerate}
    
\end{proof}

\begin{examplesblock}{Examples: }\label{examples: 7}
Let $ \xi , \eta$ be independent iid  symmetric Bernoulli with success probability $ 1/2$. Define
\[
X_{t = }\left\lbrace
\begin{array}{@{}l@{}}
    0, \quad t<1 \\
    \xi, \quad t =1 \\ 
    \xi+\eta, \quad t>1.
\end{array}\right.
\]
and let $ \F_{t} = \sigma(X_{s}, s<\leq)$ for all $ t\geq 0$. Observe that $ X$ is an $ (\F_{t})_{t\geq 0}$ martingale. Also, $ \tilde{X}$ satisfies $ X_{t} = \mathbb{E}\left[ \tilde{X}_{t}|\F_{t} \right]$ where 
\[
	\tilde{X}_{t} = \left\lbrace
	\begin{array}{@{}l@{}}
	    0, \quad t<1 \\
	    \xi+\eta, \quad t\geq 1.
	\end{array}\right.
\]
Furthermore, $ \F_{1} = \sigma(\xi)$ and $ \F_{t} = \sigma(\xi, \eta)$ for all $ t>1$, $ \tilde{X}$ is cadlag with respect to $ \tilde{F}$. Observe finally that $ \F_{1^{+}} = \sigma(\xi, \eta
)$ and so the filtration $ \F$ is not right continuous and $ \tilde{X}$ is not a version of $ X$. We thus see that the right-continuity of $ (\F_{t})_{t\geq 0}$ is necessary in Theorem \ref{thm: mg reg thm}. 
\end{examplesblock}

\begin{theorem}[Almost sure martingale convergence theorem]\label{thm: a.s. mg conv thm cont time}
Let $ X$ be a cadlag martingale bounded in $ \mathcal{L}^{1} $. Then $ X_{t}\to X_{\infty}$ a.s. with $ X_{\infty}\in \mathcal{L}^{1}(\F_{\infty}) $. 
\end{theorem}

\begin{proof}
	Let $ I_{M} = \Q_{+}\cap[0,M]$. Then Doob's upcrossing inequality \ref{lemma: doob upcrossing} from the discrete setting and a monotone convergence argument give for $ a<b, a,b \in \Q_{+}$
	\[
		(b-a)\cdot \mathbb{E}\left[ \mathcal{N}([a,b] , I_{M}, X) \right]\leq a + \displaystyle\sup_{t\geq 0}\mathbb{E}\left[ |X_{t}| \right].
	\]
	Taking $ M\to \infty$ gives $ \mathcal{N}([a,b], \Q_{+}, X)<\infty$ a.s. Hence, for the event
	\[
		\Omega_{0} = \displaystyle\bigcap_{a<b, a,b\in \Q+} \left\{ \mathcal{N}([a,b], \Q_{+}, X)<\infty \right\}
	\]
we have $ \PP(\Omega_{0})=1$ and on $ \Omega_{0}$, $ \lim_{q\to \infty, q\in \Q_{+}} X_{q}$ exists and is finite. We thus have $ X_{\infty} = \lim_{q\to \infty, q\in \Q_{+}} X_{q}$ on $ \Omega_{0}$.  Now for all $ \epsilon>0$, there exists $ q_{0}$ such that $ |X_{q_{0}}-X_{\infty}|\leq \frac{\epsilon}{2}$ for all $ q>q_{0}$, $ q\in \Q_{+}$. Now let $ t>q_{0}$. Then there exists some $ q>t$, $ q\in \Q_{+}$ such that $ |X_{t}-X_{q}|\leq \frac{\epsilon}{2}$ by right continuity of $ X$. So $ |X_{t}-X_{\infty}|\leq\epsilon$.

\end{proof}


\begin{theorem}[Doob's maximal inequality]\label{thm: doob maximal ineq cont time}
Let $ X$ be a cadlag martingale, $ X^{*}_{t} = \displaystyle\sup_{s\leq t}|X_{s}|$. Then for all $ \lambda>0$, 
\[
\lambda\cdot \PP(X^{*}_{t}\geq \lambda)\leq \mathbb{E}\left[ |X_{t}|\cdot \mathbf{1}(X^{*}_{t}\geq \lambda) \right] \leq \mathbb{E}\left[ |X_{t}| \right].
\]

\end{theorem}

\begin{proof}
    Have 
    \[
	    \displaystyle\sup_{s\leq t}|X_{s}| = \displaystyle\sup_{s\in \{t\}\cup(\Q_{+}\cap[0,t])}|X_{s}|
    \]
    and use the beginning of the proof of theorem \ref{thm: mg reg thm}.
\end{proof}


\begin{theorem}[Optional stopping theorem for cadlag UI martingales]\label{thm: ost for UI }
Let $ X$ be a cadlag UI martingale, then for all $ S\leq T$ stopping times 
\[
	\mathbb{E}\left[  X_{T}|\F_{S} \right] = X_{S} \quad \text{ a.s.}
\]
\end{theorem}


\begin{proof}
	Let $ T_{n} = 2^{-n}\rceil 2^nT\rceil$ and $ S_{n} = 2^{-n}\lceil 2^n S\rceil$. Both are stopping times and $ T_{n}'downarrow T$, $S_{n}\downarrow S $ as $ n\to \infty$. \underline{need to show:} for $ A'in \F_{S}$, then $ \mathbb{E}\left[ X_{T}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{S}\cdot \mathbf{1}(A) \right]$. Indeed, $ X_{T_{n}}\to X_{T}$ and $ X_{S_{n}}\to X_{S}$ a.s. as $ n\to \infty$ ($ X$ is right continuous).\\ 

	Now, by the discrete optional stopping theorem applied to the martingale $ (X_{k\cdot 2^{-n}})_{k\in \N}$ with respect to the filtration $ (\F_{K\cdot 2^{-n}})_{k\in \N}$, $ X_{T_{n}}= \mathbb{E}\left[ X_{\infty}|\F_{T_{n}} \right]$, so $ X_{T_{n}}$ is UI  (since $ T_{n}$ take values in $ 2^{-n}\cdot \N$). Thus, $ X_{T_{n}}\to X_{T}$ in $ \mathcal{L}^{1} $, and the same holds for $ X_{S_{n}}\to X_{S}$ using the exact same argument. By the discrete optional stopping theorem, we have that $ \mathbb{E}\left[ X_{T_{n}}|\F_{S_{n}} \right] = X_{S_{n}}$ a.s. Now for $ A\in \F_{S}$, we have that $ A\in \F_{S_{n}}$ for all $ n\in \N$ since $ S_{n}\geq S$. So $ \mathbb{E}\left[ X_{T_{n}}\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{S_{n}}\cdot \mathbf{1}(A) \right]$.
\end{proof}


\mymark{Lecture 13}\begin{theorem}[Kolmogorov's continuity criterion]\label{thm: kolmogorov continuity criterion}
	Let $ \mathcal{D}_{n} = \{K\cdot 2^{-n}: 0\leq k \leq 2^{n}\}$ and $ \mathcal{D} = \displaystyle\bigcup_{n\geq 0} \mathcal{D}_{n}$. Let $ (X_{t})_{t\in \mathcal{D}}$ be a stochastic process taking real values. Suppose there exists some $ \epsilon>0$ $ p>0$, such that
	\[
	\mathbb{E}\left[ |X_{t}-X_{s}|^{p} \right]\leq c\cdot |t-s|^{1+\epsilon}, \quad \text{ for all }s,t\in \mathcal{D}
	\]
	where $ c$ is a positive constant. Then for all $ \alpha \in (0, \epsilon/p)$, the process is $ \alpha-$H\"{o}lder continuous, that is there exists a random variable $ K_{\alpha}<\infty$ such that
	\[
	|X_{t}-X_{s}|\leq K_{\alpha}\cdot |t-s|^{\alpha}, \quad \text{ for all } s,t\in \mathcal{D}.
	\]
\end{theorem}



\begin{proof}
\[
	\PP \left( |X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \geq 2^{-n\alpha}\right)\stackrel{\text{Markov + assumption}}{\leq} c\cdot 2^{-n\alpha}p\cdot2^{-n(1+\epsilon)}. 
\]
Thus, 
\[
	\PP \left( \max_{0\leq k \leq 2^n}|X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \geq 2^{-n\alpha} \right)\stackrel{\text{union bound}}{\leq} c\cdot 2^{n\alpha p n\epsilon}, \quad (\alpha \in (0, \frac{\epsilon}{p}).   
\]
By Borel-Cantelli, 
\[
\max_{0\leq k \leq 2^n}|X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}| \leq 2^{-n\alpha}
\]
for all $ n\in \N$ sufficiently large. Thus, 
\[
 \displaystyle\sup_{ n\geq 0}\max_{0\leq k \leq 2^n} \frac{ |X_{k\cdot 2^{-n}}-X_{(K+1)\cdot 2^{-n}}|}{2^{-n\alpha}} \leq 2^{-n\alpha}\leq M(\omega)<\infty
\]
a.s. For some random variable $ M$. \\ 

\underline{Need to show: } there exists some $ M'$ such that $ |X_{t}-X_{s}|\leq M'\cdot |t-s|^{\alpha}$ for all $ s,t \in \mathcal{D}$.\\ 

Let $ s<t$, $ s,t\in \mathcal{D}$ and let $ r$ be the unique integer such that $ 2^{-(r+1)}<t-s\leq 2^{-r}$. Then there exists some $ k\in \N$ such that $ s<k\cdot 2^{-(r+1)}<t$. Now, observe that $ t-\alpha\leq 2^{-r}$ so 
\[
	t-\alpha = \displaystyle\sum^{\infty}_{j=r+1} \frac{x_{j}}{2^j}, \quad x_{j}\in \{0,1\}
\]
and 
\[
	\alpha-s = \displaystyle\sum^{\infty}_{j=r+1} \frac{y_{j}}{2^j}, \quad y_{j}\in \{0,1\}.

\]
Observe that $ [s, t)$ is a disjoint union of dyadic intervals each of them having length $ 2^{-n}$ with $ n\geq r+1$ and each interval of length will appear at most twice. Thus, we get the bound
\[
\begin{array}{ll}
|X_{t}-X_{s}| &\leq\displaystyle\overbrace{\sum_{d,n}\displaystyle\underbrace{|X_d-X_{d+2^{-n}}|}_{\leq 2^{-n\alpha\cdot M}}}^{d,n \text{ is the endpoint of a dyadic interval in the decomposition of }[s,t) \text{ of length } 2^{-n}}  \\
	      &\leq 2\cdot M \cdot\displaystyle\sum^{\infty}_{n =  r+1} 2^{-n\alpha} = \frac{2M\cdot 2^{-(r+1)\alpha}}{1-2^{-\alpha}} < \frac{2M}{1-2^{-\alpha}}|t-s|^{\alpha}.
\end{array}
\]
\end{proof}

\section{Weak Convergence}\label{sec: weak convergence}

We fix $  (\mathcal{M}, d)$ a metric space endowed with its Borel sigma algebra.

\begin{boxdef}\label{def: weak convergence of measures}
Let $ (\mu_{n})_{n\in \N}$ be a sequence of probability measures on $ \mathcal{M}$. We say $ (\mu_{n})_{n\in \N}$ converges weakly to $ \mu$ and write $\mu_{n}\implies \mu$ as $ n\to \infty$ if 
\[
	\mu_{n}(f) \coloneqq \int_{\mathcal{M}} f(x) \mu_{n}(\diff  x) \stackrel{n\to \infty}{\longrightarrow} \int_{ \mathcal{M}} f(x) \mu(\diff  x) \coloneqq \mu(f)
\]
for any $ f$ continuous and bounded.

\end{boxdef}


\begin{examplesblock}{Examples: }\label{examples: 8}
\begin{enumerate}
	\item Let $ x_{n}\to x$ as $ n\to \infty$ in $ ( \mathcal{M}, d)$ then $ \delta_{x_{n}}\stackrel{n\to \infty}{\longrightarrow}\delta_{x}$ ,since $ \delta_{x_{n}}(f)= f(x_{n})\stackrel{n\to \infty}{\longrightarrow}f(x) = \delta_{x}(f)$. 
	\item Let $  \mathcal{M} = [0,1]$, with the Euclidean metric and its Borel sigma algebra. Let $ \mu_{n} = \frac{1}{n}\displaystyle\sum_{0\leq k\leq n}\delta_{k/n} $. Then $ \mu_{n}$ converges weakly to the Lebesgue measure. Indeed, $ \mu_{n}(f) = \frac{1}{n}f(k/n)\stackrel{n\to \infty}{\longrightarrow}\int f(x) \diff x$, being Riemann sums.
	\item $ \mu_{n} = \delta_{\frac{1}{n}}\implies \delta_{0}$, as $ n\to \infty$. Notice however that for $ A = (0,1)$, $ \mu_{n}(A) = $ for all $ n\geq 0$ and so $ \nu_{n}(A) \cancel{\rightarrow} \delta_{0}(A) = 0$.
\end{enumerate}

\end{examplesblock}


\begin{theorem}\label{thm: weak convergence equivalence}
Let $ (\mu_{n})_{n\in \N}$ be a sequence of probability measures on $ ( \mathcal{M}, d)$. Then the following are equivalent: 
\begin{enumerate}
	\item $ \mu_{n}\implies \mu$.
    \item For all $ G$ open, $ \displaystyle\liminf_{n}\mu_{n}(G)\geq \mu(G)$.
    \item For all $ A$ closed, $ \displaystyle\limsup_{ n} \mu_{n}(A)\leq \mu(A)$.
    \item For all $ A$ with $ \mu(\partial A) = 0$, then $ \mu_{n}(A)\to \mu(A)$. 
\end{enumerate}

\end{theorem}


\begin{proof}
	\underline{$ 1\implies 2$:} Let $ G$ be open with $ G^{c}\neq \emptyset$. Let $ M>0$ and set $ f_{M}(x) = \mathbf{1}(M d(x, G^{c}))\leq \mathbf{1}(x\in G)$. Observe that 	$ f_{M}(x)\uparrow \mathbf{1}(x\in G)$ as $ M\to \infty$, $ f_{M}$ is bounded and continuous for all $ M$. So $ \mu_{n}(f_{M})\to \mu(f_{M})$ as $ n\to \infty$ for all $ M$. Thus, 
	\[
		\displaystyle\liminf_{n}\mu_{n}(G)\geq \displaystyle\liminf_{n}\mu_{n}(f_{M}) = \mu(f_{M})\stackrel{\text{monotone convergence}}{\to}\mu(G).
	\]
	\underline{$2\implies 3$:} follows from the previous case by taking complements. 
	\underline{$ 2,3\implies 4$:} $ 0 = \mu(\partial A) = \mu(A\setminus \intr{A})$, hence $ \mu(\overline{A}) = \mu(A) = \mu(\intr{A})$.\ \ 
	\underline{$ 2:$} $ \displaystyle\liminf_{n}\mu(\int{A})\geq \mu(\intr{A})=\mu(A)$.
	\underline{$ 3:$} $ \displaystyle\limsup_{n} \mu_{n}(\overline{A})\leq \mu(\overline{A}) = \mu(A)$.

	\underline{$ 4\implies 1$:} Need to show for any $ f$ continuous and bounded, $ \mu_{n}(f)\to \mu(f)$. We can assume further that $ f\geq 0$. Fix $ K> \displaystyle\sup f$. Have, 
	\[
	\begin{array}{ll}
		\displaystyle\int_{ \mathcal{M}}f(x) \mu_{n}(\diff x)    &= \displaystyle\int_{ \mathcal{M}} \left( \int^{K}_{0} \mathbf{1}(t\leq f(x)) \diff t \right) \mu_{n}(\diff  x)  \\
									 &\stackrel{\text{Fubini}}{=} \int^{K}_{0}\mu_{n}(f\geq t) \diff t.  
	\end{array}
	\]
	It suffices to show $ \mu_{n}(f\geq t)\to \mu(f \geq t)$ as $ n\to \infty$. Since then we can conclude using dominated convergence. Thus it suffices to show that $ \mu(\partial \{f\geq t\}) = 0$. Indeed, 
	\[
		\partial\{f\geq t \} \subset \{f = t\}.
	\]
	since $ f$ is continuous and $ \{f>t\}$ is open and $ \subset \int\{f\geq t\}$. Also observe that there exists an at most countable number of $ t$ such that $ \mu(f = t)>0$. Thus, 
	\[
	\{t: \mu(f=t)>0\} = \displaystyle\bigcup_{n}\displaystyle\underbrace{\{t: \mu(\{f = t\})\geq \frac{1}{n}\}}_{\#\leq n} .
	\]
	Thus, $ \partial \{f\geq t\}$ is countable and has Lebesgue measure zero.
\end{proof}

Now, let $ \mathcal{M} = \R$. Let $ \mu$ be a probability measure on $ \R$. We define the distribution function of $ \mu$ to be the function $ F_{\mu}:x\mapsto \mu((-\infty,x] )$, $ F_{\mu}\R\to [0,1]$. 


\begin{boxprop}\label{prop: weak convergence distribution function}
	Let $ (\mu_{n})_{n\in \N}$. be a sequence of probability measures on $ \R$. Then the following are equivalent: 
	\begin{enumerate}
	    $ \mu_{n}\implies \mu$, as $ n\to \infty$.
	    \item $F_{\mu_{n}}(x)\stackrel{n\to \infty}{\longrightarrow}F_{\mu}(x)$ for all $ x\in \R$ continuity points of $ F_{\mu}$.
	\end{enumerate}
\end{boxprop}

\begin{proof}
	\underline{$ 1\implies 2$:}  Let $ x$ be a continuity point of $ F_{\mu}$. Have $ F_{\mu_{n}}(x) = \mu_{n}((-\infty, x])$ and 
	\[
	\begin{array}{ll}
		\mu(\partial(-\infty, x]) &= \mu(\{x\}) \\
					  &= \mu((-\infty, x])- \lim_{n \to \infty} \mu((-\infty, x-\frac{1}{n}])\\ 
					  & = F_{\mu}(x)- \lim_{n \to \infty} F_{\mu}(d-\frac{1}{n}) = 0
	\end{array}
	\]
	since $ x$ is a continuity point of $ F_{\mu}$. 
\end{proof}

\underline{$ 2\implies 1$:} Let $ G$ be an open set in $ \R$. Then $ G = \displaystyle\bigcup_{n} (a_{k}, b_{k})$, a union of disjoint open intervals. Now, 

\[
\begin{array}{ll}
    \displaystyle\liminf_{n}\mu_{n}(G) &= \displaystyle\liminf_{n}\displaystyle\sum_{k}\mu_{n}(a_{k}, b_{k})  \\
				       &\stackrel{\text{Fat}}{\geq}\displaystyle\sum_{k} \displaystyle\liminf_{n} \mu_{n}(a_{k}, b_{k}). 
     &= +d
\end{array}
\]
So it suffices to show that $ \displaystyle\liminf_{n} \mu_{n}(a,b)\geq \mu(a,b)$ for all $ a<b\in \R$.\\ 

Indeed, We have $ \mu_{n}((a,b)) = F_{\mu_{n}}(b-)-F_{\mu_{n}}(a)$ and since $ F_{\mu}$ is non-decreasing and has at most countably many discontinuities, there exist $ a', b'$ continuity points of $ \F_{\mu}$. Hence, $ F_{\mu_{n}}(a') \stackrel{n\to \infty}{\longrightarrow} F_{\mu}(a')$ and $ F_{\mu_{n}}(b') \stackrel{n\to \infty}{\longrightarrow} F_{\mu}(b')$. This means that
\[
    \displaystyle\liminf_{n}\mu_{n}((a,b)) \geq F_{\mu}(b')-F_{\mu}(a').
\]
By the density of continuity points, there exist $(b'_{m})_{m\in \N}$, such that $ b'_{m}\uparrow b'$ and $ (a'_{m})_{m\in \N}$, $ a'_{m} \downarrow a'$ all continuity points. Thus, 
\[
\begin{array}{ll}
	\displaystyle\liminf_{n} \mu_{n}((a,b)) &\geq \displaystyle\sup_{n}F_{\mu_{n}}(b'_{m})-F_{\mu}(a'_{m})\\
     &= F_{\mu}(b-)-F_{\mu}(a) = \mu((a,b)).
\end{array}
\]
\begin{boxdef}\label{def: weak conv law}
Let $ (X_{n})_{n\in \N}$ be a sequence of random variables taking values in $ ( \mathcal{M}, d)$, defined on probability spaces $ (\Omega_{n}, \F_{n}, \PP_{n})$. We say that $ (X_{n})_{n\in \N}$ converges weakly (or in distribution) to a random  variable $ X$ defined on $ (\Omega, \F, \PP)$ if $  \mathcal{L}(X_{n})\implies \mathcal{L}(X)$ (i.e. the laws converge weakly). 
\end{boxdef}


\begin{remark}
	Equivalently, $ X_{n} \stackrel{w/d}{\implies}X$ if for all $ F$ continuous and bounded, $  \mathbb{E}_{\PP_{n}}\left[ f(X_{n}) \right]\to \mathbb{E}_{\PP}\left[ f(X) \right]$, as $ n\to \infty$.
\end{remark}

\begin{boxprop}\label{prop: conv in prob conv in dist}
\begin{enumerate}
	\item If $ X_{n}\stackrel{\PP}{\implies} X$ as $ n\to \infty$, then $ X_{n}\stackrel{d}{\implies} X$ as $ n\to \infty$. 
	\item If $  X_{n}\stackrel{d}{\implies} c$, $ c$ a constant, then $  X_{n}\stackrel{\PP}{\implies} c$
\end{enumerate}

\end{boxprop}


\begin{examplesblock}{Examples: (CLT)}\label{examples: 8}
	Let $(X_{n})_{n\in \N}$ be iid and $ \mathbb{E}\left[ X_{1} \right] = m$ and $ \sigma^{2} = \text{Var}(X_{1})$. Then with $ S_{n} =\displaystyle\sum^{n}_{i=1}X_{i} $
	\[
		\frac{S_{n}-n\cdot m}{\sqrt{n\sigma^{2}}} \stackrel{d}{\longrightarrow} \mathcal{N}(0,1)
	\]
	as $ n\to \infty$.
\end{examplesblock}

\begin{boxdef}[Tightness]\label{def: waek conv tightness}
	Let $ ( \mathcal{M}, d)$ be a metric space. A sequence of probability measures $ (\mu_{n})_{n\in \N}$ on $ \mathcal{M}$ is called \underline{tight} if for all $ \epsilon>0$, there exists a compact set $ K\subseteq \mathcal{M}$ such that 
\[
 \displaystyle\sup_{n\geq 0}\mu( \mathcal{M}\setminus K)\leq \epsilon.
\]
\end{boxdef}


\begin{remark}
    It $ \mathcal{M}$ is compact, then all sequences of probability measures are tight. 
\end{remark}

\begin{theorem}[Prohorov]\label{thm: prohorov}
Let $ (\mu_{n})_{n\in\N}$ be a tight sequence of probability measures, then there exists a subsequence $ (\mu_{n_{k}})_{k\in \N}$ and a probability measure $ \mu$ such that 
\[
	\mu_{n_{k}} \stackrel{d}{\implies} \mu, \quad \text{ as }k\to \infty.
\]
\end{theorem}

\begin{proof}   

We focus on the case $ \mathcal{M} = \R$. Let $ \Q = \{x_{1}, x_{2}, \cdots\}$ be an enumeration of $ \Q$ and $ F_{n} = F_{\mu_{n}}$. Then, the sequence $ (F_{n}(x_{1}))_{n\in \N}$ in $ [0,1]$ has a convergent subsequence $F_{n^{(1)}_{k}}(x_{1})\stackrel{k\to \infty}{\longrightarrow} F(x_{1})$ by compactness. So does $ (F_{n^{(1)}_{k}}(x_{2}))_{k\in \N}$. Thus, continuing so inductively, we obtain for all $ i\in \N$ that there exist sequences $ (n^{(i)}_{k})_{k\in \N}$ such that 
\[
F_{n^{(i)}_{k}}(x_{j}) \stackrel{k\to \infty}{\longrightarrow}F(x_{j}), \quad \text{ for all } 1\leq j\leq i.
\]
Thus, we can extract a diagonal sequence $ (m_{k})_{k\in \N}$, where $ m_{k} = n^{(k)}_{k}$ for all $ k\in \N$ and Have 
\[
F_{m_{k}}(x) \stackrel{k\to \infty}{\longrightarrow} F(x), \quad \text{ for all } x\in \Q.
\]
Observe now that the functions $ F_{m_{k}}$ are non-decreasing, and so $ F$ is non-decreasing, so for $ x\in \R$ define $ F(x) = \lim_{q\downarrow x, q\in \Q} F(q)$. Thus, $ F$ is right continuous, non-decreasing and so $ F$ has left-limits.\\ 

Let $ x\in \R$ be a continuity point of $ F$. We need to show that $ F_{m_{k}}(x)\stackrel{k\to \infty}{\longrightarrow}F(x)$. Indeed, for any $ \epsilon >0$, there exist $ s_{1}<x<s_{2}$, $ s_{i}\in \Q$ such that  $F(s_{i})-F(x)|<\epsilon/2$ (since $ F$ is continuous at $ x$). We now have the chain of inequalities
\[
	F(x)-\epsilon\leq F(s_{1})-\frac{\epsilon}{2}\leq F_{m_{k}}(s_{1})\leq F_{m_{k}}(x)\leq F_{m_{k}}(s_{2}\stackrel{\text{conv. in } \Q}{\leq} F(s_{2})+\frac{\epsilon}{2}\leq F(x)+\epsilon
\]
for all $ k\in \N$ sufficiently large.\\ 

Finally, it remains to show that  there exists some probability measure $ \mu$ such that $ F = F_{\mu}$. Indeed, by tightness, we have that for all $ \epsilon>0$, there exists $ N\in \R$ large enough so that (with $ \pm N$ being continuity points of $ F$)
\[
	\displaystyle\sup_{n\geq 0 }\mu_{n}([-N, N]^{c})\leq \epsilon.
\]
Thus, $ F(-N)\leq \epsilon$ and $ 1-F(N)\leq \epsilon$. This guarantees that 
\[
\lim_{x\to -\infty} F(x) = 0, \quad \lim_{x \to \infty} F(x) = 1.
\]
Finally, define define $ \mu((a,b]) = F(b)-F(a)$. Then, $\mu$ can be extended to the Borel sigma algebra by Calathea dory's extension theorem.
\end{proof}

\begin{boxdef}\label{def: charateristic function}
Let $ X$ be a random variables with values in $ R^{d}$. The characteristic function of $ X$ is defined as 
\[
	\phi_{X}(u) = \mathbb{E}\left[ e^{i\bracket{u}{X}} \right], \quad u\in \R^{d}.
\]
\end{boxdef}
\underline{Properties of $ \phi_{X}$:}
\begin{enumerate}
	\item $ \phi_{X}$ is continuous on $ \R^{d}$ and $ \phi_{X}(0)=1$.
	\item $ \phi_{X}$ completely determines the law of $ X$, that is if $ \phi_{X}(u) = \phi_{Y}(u)$ for all $ u\in \R^{d}$, then $  \mathcal{L}(X) = \mathcal{L}(Y)$.
\end{enumerate}

\mymark{Lecture 15} \begin{theorem}[L\'{e}vy's convergence theorem]\label{thm: levy conv thm}
Let $ (X_{n})_{n\in \N}$, $ X$ be random variables taking values in $ \R^{d}$. Then 
\begin{enumerate}
	\item $ \mathcal{L}(X_{n})\implies \mathcal{L}(X)$  as $ k\to \infty$, then $ \phi_{X_{n}}(u) \stackrel{n\to \infty}{\longrightarrow}\phi_{X}(u)$ for all $ u\in \R^{d}$. 
	\item Suppose there exists $ \psi:\R^{d}\to \C$ such that $ \psi(0) = 1$, $ \psi$ is continuous at zero and  $ \phi_{X_{n}}(u) \stackrel{n\to \infty}{\longrightarrow}\psi(u)$ for all $ u\in \R^{d}$. Then there exists a random variable $ X$ with characteristic function $ \psi = \phi_{X}$ and $ \mathcal{L}(X_{n})\implies \mathcal{L}(X)$. 
 
\end{enumerate}

\end{theorem}

Before we proceed with the proof of the theorem, we state a Lemma
\begin{boxlemma}\label{lemma: characteristic function bound}
Let $ X$ be a random variable in $ \R^{d}$. Then, for all $ K>0$, 
\[
	\PP(\norm{X}_{\infty})\leq C\cdot \left( \frac{K}{2} \right)^{d} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X}(u))  \diff  u, 
\]
where $ C = (1-sin(1))^{-1}$.
\end{boxlemma}

\begin{proof}
    Fix $ \lambda>0$ and let $ \mu = \mathcal{L}(X)$. Then,
    \[
    \begin{array}{ll}
	    \displaystyle\int_{[-\lambda, \lambda]^{d}}\phi_{X}(u) \diff u   &= 
 \displaystyle\int_{[-\lambda, \lambda]^{d}} \left( \displaystyle\int_{\R^{d}} \prod^d_{j=1}e^{iu_{j}\cdot x_{j}}\mu(\diff x)   \right) \diff   u \\
									     &\stackrel{\text{Fubini}}{=} \displaystyle\int_{\R^{d}} \mu(\diff  x) \displaystyle\prod^d_{j=1} \left( \displaystyle\int_{[-\lambda,\lambda]}e^{iu_{j}\cdot x_{j}} \diff u_{j} \right) \\ 
									     &= \displaystyle\int_{\R^{d}} \mu(\diff  x) \displaystyle\prod^d_{j=1} \left( \frac{e^{ix_{j}\lambda}-e^{-ix_{j}\lambda}}{ix_{j}} \right) \\ 
									     &= \displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \frac{2\cdot \sin(\lambda x_{j})}{x_{j}} \mu( \diff  x)\\ 
									     & = (2\lambda)^{d}\displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \left(\frac{2\cdot \sin(\lambda x_{j})}{\lambda x_{j}}\right) \mu( \diff  x).

    \end{array}
    \]
     Thus, 
     \[
     \displaystyle\int_{[-\lambda, \lambda]^{d}}(1-\phi_{X}(u)) \diff u  = (2\lambda)^{d}\displaystyle\int_{\R^{d}}\displaystyle\prod^d_{j=1} \left(1- \frac{2\cdot \sin(\lambda x_{j})}{\lambda x_{j}}\right) \mu( \diff  x)
\]
     Now, let $ f(u) = \displaystyle\prod^d_{j=1} \left(\frac{2\cdot \sin(u_{j})}{u_{j}}\right)$, $ f:\R^{d}\to \R$. \\ 
	     \underline{Claim: } not hard to see that if $ x\geq 1$, then $ \left| \frac{\sin(x)}{x} \right|\leq \sin(1)$. Hence, if $ \norm{u}_{\infty}\geq 1$, then $ |f(u)|\leq \sin(1)$. So $ \mathbf{1}(\norm{u}_{\infty}\geq 1)\leq C\cdot(1-f(u))$, where $ C = (1-\sin(1))^{-1}$. Hence, 
	     \[
		     \PP(\norm{X}_{\infty}\geq k)\leq C\cdot \mathbb{E}\left[ 1- f \left( \frac{X}{K} \right) \right]
	     \]
and by simple scaling, one can conclude for the general case.
\end{proof}

\begin{proof}{(Theorem \ref{thm: levy conv thm})}
   \begin{enumerate}
	   $ f(x) = e^{i\bracket{u}{x}}$ is continuous and bounded so by bounded convergence, have 
	   \[
	   \phi_{X_{n}}(u) = \mathbb{E}\left[ f(X_{n}) \right]\to \mathbb{E}\left[ f(X) \right]
	   \]
	   as $ n\to \infty$. 
	   \item First we prove that $  \mathcal{L}(X_{n}))_{n\in \N}$ is tight. By Lemma \ref{lemma: characteristic function bound}, have that 
		   \[
		   	\PP(\norm{X_{n}}_{\infty})\leq C\cdot \left( \frac{K}{2} \right)^{d} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X_{n}}(u))  \diff  u
		   \]
and $ |1-\phi_{X_{n}}(u)|\leq 2$ for all $ u\in \R^{d}, n\in \N$. Thus, by dominated convergence, 
\[
 \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\phi_{X_{n}}(u))  \diff  u \stackrel{n\to \infty}{\longrightarrow} \displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\psi(u))  \diff  u. 
\]
Since $ \psi$ is continuous at zero and $ \psi(0) = 1$, taking $ K$ large enough we get 
\[
\displaystyle\int_{[-\frac{1}{K}, \frac{1}{K}]^{d}} (1-\psi(u))  \diff  u <\frac{\epsilon}{2^d Cd}(2K^{-1})^{d}.
\]
Thus, $ \PP(\norm{X_{n}}_{\infty}\geq K)\leq \epsilon$ for all $ n\in \N$ sufficiently large. Taking $ K$ possibly even larger, we conclude that 
\[
	\displaystyle\sup_{n\geq 0}\PP(\norm{X}_{\infty}\geq K)\leq \epsilon,
\]
hence showing that $ ( \mathcal{L}_{n})_{n\in \N}$ is tight. By Pro horror, there exists a subsequence $ (n_{k})_{k\in \N}$ such that
\[
 \mathcal{L}(X_{n_{k}}) \stackrel{n\to \infty}{\implies} \mathcal{L}(X)
\]
and so $ \phi_{X_{n_{k}}}(u) \to \phi_{X}(u)$ for all $ u\in \R^{d}$. Thus, $ \psi \equiv \phi$.\\ 

Suppose for a contradiction that $ \mathcal{L}_{X_{n}
}$ does not converge. Then there exists $ f$ continuous and bounded and a subsequence $ m_{k}$ such that  
\[
 \left|\mathbb{E}_{m_{k}}\left[ f(X_{m_{k}}) \right] - \mathbb{E}\left[ f(X) \right]\right|\geq \epsilon
\]
for all $ k\ni \N$. Now, since $ (\mathcal{L}(X_{m_{k}}))_{k\in \N}
$ is tight, there exist a subsequence, without relabelling, such that $ ( \mathcal{L}(X_{m_{k}}))$ converges weakly, a contradiction. Thus, the limit must also be $ X$. 
   \end{enumerate}
    
\end{proof}

Now, we briefly embark on a discussion of the theory of \textit{large deviations}.

\section{Large deviations}\label{sec: large deviations}


Let $ X_{1}, X_{2}, \cdots$ be iid $\sim \mathcal{N}(0,1) $ random variables. Let $ \widehat{S}_{n} = \frac{1}{n}\displaystyle\sum^{n}_{i=1} X_{i} \sim \mathcal{N}(0, 1/n)$. Let $ \delta >0$, we by the weak law of large numbers that 
\begin{enumerate}
	\item 
\[
	\PP(|\hat{S}_{n}|\geq \delta)\stackrel{n\to \infty}{\longrightarrow} 0.
\]
\item \[ \PP(\sqrt{n}|\hat{S}_{n}|\in \stackrel{\text{interval}}{A}) \stackrel{\text{CLT}}{\longrightarrow} \displaystyle\int_{A} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\diff x.
	\]
	\item \[ \PP(|\hat{S}_{n}|\geq \delta) = 1- \displaystyle\int^{\delta\sqrt{n}}_{-\delta \sqrt{n}} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\diff x. 
	\]
In other words, 
\[
 \frac{\log \PP(|\hat{S}_{n}|\geq \delta)}{n} \stackrel{n\to \infty}{\longrightarrow} -\frac{\delta}{2}.
\]
\end{enumerate}

Observe that $ \hat{S}_{n}$, the ``typical `` value is of the order $ \frac{1}{\sqrt{n}}$ and it can take relatively large values $ (\geq \delta>0)$ with very small probability $ \tilde e^{-\frac{\delta^{2}n}{2}}$. Furthermore, $ 1, 2$ are universal but $ 3$ depends on the distribution. We shall focus on quantifying $ 3$ for an appropriate class of random variables.\\ 

Let $ X_{1}, X_{2}, \cdots$ be an iid family of random variables, such that $ \mathbb{E}\left[  X_{1}\right] = \overline{x}$, $S_{n} = X_{1}+X_{2}+\cdots+X_{n}$. Let $ a\in \R$. Now 
\[
	\PP(S_{n+m}\geq a(n+m))\stackrel{\text{independence}}{\geq}\PP(S_{n}\geq a_{n})\cdot \PP(S_{m\geq a_{m}}). 
\]
Now, with $ b_{n} = -\log \PP(S_{n}\geq an)$ for all $ n\in \N$, have that $ b_{n+m}\leq b_{n}+b_{m}$. This is called  sub-additive sequence. Actually, for such sequences one has 
\[
\lim_{n \to \infty} \frac{b_{n}}{n} = \displaystyle\inf_{n}\frac{b_{n}}{n}.
\]
\begin{examplesblock}{Sub-additive sequences}\label{aside: subbadtive sequences}
To quickly see this, suppose first that $ \displaystyle\inf_{n}\frac{b_{n}}{n}>-\infty$. Fix any $ \epsilon>0$, then there exists some $ m\in \N$ such that $ \frac{b_{m}}{m}< \displaystyle\inf_{n}\frac{b_{n}}{n}+\epsilon$. Hence, for any $ k \geq m$, we have by Euclidean division that there exists some $ q\in \Z_{+}$ and $ r\in [0,m)\cap \N$ such that $ k = qm + r$. Thus, the sub-additivity of $ (b_{n})_{n\in \N}$ implies that 
\[
\begin{array}{ll}
	\frac{b_{k}}{k} = \frac{b_{qm+r}}{qm+r}&\leq \frac{q \cdot b_{m} + b_{r}}{qm+r} \\
					       &\leq \cancelto{1}{\frac{qm}{qm+r}}\displaystyle\inf_{n}\frac{b_{n}}{n} + \cancelto{0}{\epsilon\cdot mq + \frac{b_{r}}{qm+r}}
\end{array}
\]
as $ k\to \infty$. The case where  $ \displaystyle\inf_{n}\frac{b_{n}}{n}=-\infty$ can be dealt with similarly.

\end{examplesblock}

So, we have that 
\[
-\frac{1}{n}\log\PP(S_{n}\geq a_{n}) \stackrel{n\to \infty}{\longrightarrow} I(a).
\]
Also, 
\[
\begin{array}{ll}
	\PP(S_{n}\geq an) &\stackrel{\lambda >0}{=} \PP(e^{\lambda S_{n}}\geq e^{\lambda an}) \\
			  &\stackrel{\text{Markov}}{\leq} \mathbb{E}\left[ e^{\lambda S_{n}} \right]\cdot e^{-n\lambda a} = \mathbb{E}\left[ e^{\lambda X_{1}} \right]\cdot e^{-\lambda an}.
\end{array}
\]
Define $ M(\lambda) = \mathbb{E}\left[ e^{\lambda \cdot X_{1}} \right]$, $ \psi(\lambda) = \log M(\lambda)$, $ \lambda \in \R$. In other words, we have 
\[
\PP(S_{n}\geq an)\leq \exp(-n(\lambda a-\psi(\lambda))).
\]
Furthermore, let $ \psi^{*}(a) = \displaystyle\sup_{ \lambda\geq 0}(\lambda a-\psi(\lambda))\geq 0$. So $ \PP(S_{n}\geq an)\leq \exp(-n\psi^{*}(a))$ and so have obtained 
\[
 \frac{-\log \PP(S_{n\geq an})}{n} \geq \psi^{*}(a).
\]

\mymark{Lecture 16} 
\begin{theorem}[Cramer's Theorem]\label{thm: cramer}
	Let $ X_{1}, X_{2}, \cdots$ be an iid sequence of random variables with $ \mathbb{E}\left[ X_{1} \right] = \overline{x}$. Let $ S_{n} =\displaystyle\sum^{n}_{i=1}X_{i} $. Then, 
	\[
	-\frac{1}{n}\log\PP(S_{n}\geq an) \stackrel{n\to \infty}{\longrightarrow} \psi*(a)
\]
for all $ a\geq \overline{x}$ where $ \psi^{*}(a) = \displaystyle\sup_{\lambda \geq 0}(\lambda alpha-psi(\lambda))$, $ \psi(\lambda) = \log \mathbb{E}\left[ e^{\lambda \cdot X_{1}} \right]$ ($ \psi^{*}$ is known as the \textit{Legendre transform}).
\end{theorem}

We collect some basic facts about the function $ M(\lambda) = \mathbb{E}\left[ e^{\lambda X_{1}} \right]$, $ \lambda \in \R$.

\begin{boxlemma}\label{lemma: cramer log mgf}
	The functions $ M$ and $ \psi$ are continuous on $ \mathcal{D} = \{\lamda: M(\lambda)<\infty\}$ and differentiable in $ \intr{\mathcal{D}}$ with $ M'(\lambda) = \mathbb{E}\left[ X_{1\cdot e^{\lambda X_{1}}} \right]$ and $ \psi'(\lambda) = \frac{M'(\lambda)}{\lambda}$, $ \lambda \in \mathcal{D}$. 
\end{boxlemma}

\begin{proof}
	\underline{Continuity:} Fix a sequence $ \lambda_{n} \stackrel{n\to \infty}{\longrightarrow}\lambda\in \mathcal{D}$. Then, pointwise, $ e^{\lambda_{n}X_{1}}\stackrel{n\to \infty}{\longrightarrow} e^{\lambda X_{1}}$ and take $ n\in \N$ such that for all $ n\geq N$, $ e^{\lambda_{n}X_{1}}\leq e^{\lambda_{N}X_{1}}+e^{\lambda X_{1}}\in \mathcal{L}^{1}$ (which holds by since $ \lambda_{N}\leq \lambda_{n}\leq \lambda$ for $ n$ possible larger). Thus, can conclude by dominated convergence that $ \psi(\lamnda_{n})\stackrel{n\to \infty}{\longrightarrow}\psi(\lambda)$.\\ 

	\underline{Differentiability:} Fix $ \eta \in \intr{ \mathcal{D}}$. We can now bound 
	\[
	\begin{array}{ll}
	    \left|\frac{M(\eta + \epsilon)-M(\eta)}{\epsilon}\right| &= \left| \mathbb{E}\left[\frac{ e^{(\eta+\epsilon)\cdot X_{1}}-e^{\eta\cdot X_{1}}}{\epsilon} \right] \right|\\
	     &\leq e^{\eta\cdot X_{1}} \left| \frac{e^{\epsilon\cdotX_{1}}-1}{\epsilon} \right|.
	\end{array}
	\]
	Now, let $ \delta>0$ sufficiently small such that $ (\eta-\delta, \eta+\delta)\subseteq \intr{ \mathcal{D}}$. Now, for all $ \epsilon \in (-\delta, \delta)$
	\[
		\left| \frac{e^{\epsilonX_{1}}-1}{\epsilon} \right| &\stackrel{\text{comparing power series}}{\leq} \frac{e^{\delta|X_{1}|}-1}{\delta}.
	\]
	So 
	\[
\left| \frac{e^{(\eta+\epsilon)X_{1}}-e^{\eta X_{1}}}{\epsilon} \right|	\leq e^{\eta X_{1}}\cdot \frac{e^{\delta|X_{1}|}-1}{\delta}. 
	\]
	Now, since $ e^{\eta X_{1}}\cdot e^{\delta |X_{1}|}\leq e^{\eta X_{1}}\cdot(e^{\delta X_{1}}+e^{-\delta X_{1}})\in \mathcal{L}^{1}$ since $ \eta \in \intr{ \mathcal{D}}$ and we can thus conclude by dominated convergence.
\end{proof}


\begin{proof}{(Theorem \ref{thm: cramer})}
    From the previously derived Chernoff bound, we have 
    \[
    \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)\geq \psi^{*}(a).
    \]
    It suffices to show now that 
    \[
	    \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)\leq \psi^{*}(a), \quad \text{ for all }a\geq \overline{x}.
    \]
    Observe that we can replace each $ X_{i}$ by $ \tilde{X}_{i} = X_{i}-a$ and define $ \tilde{S}_{n} =\displaystyle\sum^{n }_{i=1}\tilde{X}_{i}$ and \\ 
    $ \tilde{M}(\lambda) = \mathbb{E}\left[ e^{\lambda \tilde{X}} \right] = e^{-a\lambda}M(\lambda)$, where $ \tilde{\psi}(\lambda) = \psi(\lambda)-a\lambda$, $ \lambda \in \R$. 
\end{proof}

Thus we can restate the original inequality as follows
\[
\lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq an)= \lim_{n \to \infty} -\frac{1}{n}\log \PP(\tilde{S}_{n}\geq 0)\leq \tilde{\psi}^{*}(0)\,
\]
where $ \tilde{\psi}^{*}(\lambda) = \displaystyle\sup_{\lambda \geq 0}(-\tilde{\psi}(\lambda))$. Thus, without loss of generality, it suffices to show that 
\[
 \lim_{n \to \infty} -\frac{1}{n}\log \PP(S_{n}\geq 0)\geq \displaystyle\inf_{\lambda \geq 0}\psi(\lambda),
\]
when $ \overline{x}\leq 0$.\\ 


For the remainder of the proof, we let $ \mu = \mathcal{L}(X)$ and break the proof into several cases.\\ 

\underline{Case 1: } $ M(\lambda)<\infty$ for all $ \lambda \in \R$.\\ 

Define a new measure $ \mu_{\theta}$ for all $ \theta \geq 0$, absolutely continuous with respect to $ \mu$ and radon-Nikodym derivative 
\[
	\frac{\diff \mu_{\theta}}{\diff  \mu} = \frac{e^{\theta X_{1}}}{M(\theta)}.
\]
We compute
\[
 \mathbb{E}_{\theta}\left[ f(X_{1})\right] = \displaystyle\int_{\R} \frac{e^{\theta x}f(x)}{M(\theta)}\mu (\diff  x).    
\]
Now, if $X_{1}, \cdots, X_{n}$ are iid $ \sim \mu$. Then 
\[
	\mathbb{E}_{\theta}\left[ F(X_{1}, \cdots, X_{n}) \right] = \displaystyle\int F(X_{1}, \cdots, X_{n}) \displaystyle\prod^{n}_{i=1}\frac{e^{\theta x_{i}}}{M(\theta)}\mu(\diff  x_{i}).   
\]
Set $ g(\theta) = \mathbb{E}_{\theta}\left[ X_{1} \right] = \int x \frac{e^{\theta x}}{M(\theta)}\diff  \mu = \frac{M'(\theta)}{M(\theta)} = \psi'(\theta)$.\\ 

\underline{Seek:} $ \theta$ such that $ g(\theta) = \psi'(\theta) = 0$.\\ 

If $ \PP(X_{1}>0) = 0$, then $ \PP(S_{n}\geq 0) = (\PP(X_{1}=0))^{n}$ by independence. Thus, 
\[
\frac{1}{n}\log \PP(S_{n}\geq 0) = \PP(X_{1} = 0)
\]
and\[
\displaystyle\inf_{\lambda\geq 0} \leq \lim_{\lambda \to \infty}\psi(\lambda) = \lim_{\lambda \to \infty}\mathbb{E}\left[ e^{\lambda X_{1}} \right] \stackrel{\text{DCT}}{=} \lim_{\lambda \to \infty}\mathbb{E}\left[ e^{\lambda X_{1}} \mathbf{1}(X_{1}=0) \right] =  \PP(X_{1}=0).
\]

We can now focus on the case where $ \PP(X_{1}>0)>0$. Now, there exists an $ N\in \N$ such that $ \PP(X_{1}>\frac{1}{N})>0$. We deduce that 
\[
	\lim_{\theta\to \infty}\psi(\theta) = \lim_{\theta \to \infty}\mathbb{E}\left[ e^{\theta X_{1}} \right] \geq \lim_{\theta \to \infty} \mathbb{E}\left[ e^{ \frac{\theta}{N}} \mathbf{1}\left(X_{1}>\frac{1}{N}\right)\right] = \infty. 
\]
Thus, there exists some $ \eta \geq 0$ such that $ \displaystyle\inf_{\lambda \geq 0}\psi(\lambda) = \psi(\eta)$ and $ \psi'(\eta) = 0$. Now, 
\[
\begin{array}{ll}
	\PP(S_{n}\geq 0) &\geq \PP(S_{n}\in [0, \epsilon n])\geq \mathbb{E}\left[ e^{\eta S_{n}-\eta\epsilon n} \mathbf{1}(S_{n}\in [0,\epsilon n]) \right] \\
			 & = e^{-\eta \epsilon n}(M(\eta))^{n}\cdot\PP_{\eta}(S_{n}\in [0,\epsilon n])
\end{array}
\]
where $ \PP_{\eta}(X_{1}\in \cdot) = \mu_{\eta}(\cdot)$. Now, since $ \mathbb{E}_{\eta}\left[ X_{1} \right] = 0$, we claim that we can use the CLT on iid copies of $ X_{1}$ with law $ \mu_{\eta}$ to deduce 
\[
	\PP(S_{n}\in[0,\epsilon n])\stackrel{n\to \infty}{\longrightarrow}\frac{1}{2}.
\]
\begin{examplesblock}{Proof of claim}\label{aside: cramer clt}
This is a little messy, be warned! Fix any $ \epsilon'>0$. We have by the triangle inequality
\[
	\left| \PP_{\eta}(S_{n}\in [0,\epsilon n])-\frac{1}{2} \right|\leq \left| \PP_{\eta}(S_{n}\in [0,\epsilon n])- \PP_{\eta}(S_{n}\in [0,\infty)) \right| + \left| \PP_{\eta}(S_{n}\in [0,\infty))-\frac{1}{2} \right|.
\]
for all $ n\in \N$. Now, by the CLT and Theorem \ref{thm: weak convergence equivalence} we have that 
\[
\PP(S_{n}\in[0,\infty)) \stackrel{n\to \infty}{\longrightarrow} \frac{1}{2}. 
\]

Thus, for all n sufficiently large, we have that $ |\PP(S_{n}\in[0,\infty))-1/2|<\epsilon'/3$. Furthermore, there exists some $ N\in \N$ such that $ \PP( \mathcal{N}\in(\epsilon \sqrt{N}, \infty))<\epsilon'/3$ where $ \mathcal{N}$ denotes a standard normal random variable. Thus, for all $ n\in \N$ sufficiently large 
\[
\begin{array}{ll}
	\left| \PP_{\eta}(S_{n} \in [0,\epsilon n])-\frac{1}{2} \right|&\leq \frac{\epsilon'}{3}+ \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{n},\infty))\right| \leq \frac{\epsilon'}{3} + \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{n},\infty))\right| \\
								       &\leq \frac{\epsilon'}{3} + \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right| \leq  \frac{\epsilon'}{3} + \PP( \mathcal{N}\in(\epsilon \sqrt{N}, \infty))\\ 
								       &+ \left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))- \PP_{\eta}(\frac{ \mathcal{N}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right|\\  
								       &\leq \frac{\epsilon'}{3}+ \frac{\epsilon'}{3} + \cancelto{\text{(CLT) }\leq \frac{\epsilon'}{3}}{\left| \PP_{\eta}(\frac{S_{n}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))- \PP_{\eta}(\frac{ \mathcal{N}}{\sqrt{n}}\in (\epsilon \sqrt{N},\infty))\right|}\\
								       &\leq \epsilon'
\end{array}
\]
as required.
\end{examplesblock}

Thus, 
\[
	\frac{\log \PP(S_{n}\geq 0)}{n}\geq -\eta \epsilon +\log M(\eta) + \frac{\log\PP_{\eta}(S_{n}\in[0,\epsilon n])}{n}.
\]
Now, for all $ \epsilon> 0$, 
\[
 \displaystyle\liminf_{ n}\frac{1}{n}\log \PP(S_{n}\geq 0 )\geq \log M(\eta) -\eta \epsilon= \psi(\eta)\geq \displaystyle\inf_{\lambda \geq 0}\psi(\lambda).
\]
Sending $ \epsilon \to 0$ gives the desired inequality.\\ 

\underline{General Case:}\\ 

Without loss of generality, (arguing as in the previous case), let $ K>0$ sufficiently large so that $ \mu([0, K])>0$. Then define the conditional laws $ \nu = \mathcal{L}(X_{1}| |X_{1}|\leq K)$, $ \nu_{n} = \mathcal{L}\left(S_{n}\Big| \displaystyle\bigcap^n_{i = 1} \{|X_{i}|\leq K\}\right)$. Have 
\[
	\mu_{n([0,\infty)}\geq \nu_{n}([0,\infty))\cdot (\mu([-K,K]))^{n}
\]
and 
\[
\log \mu_{n}([0,\infty))\geq \frac{\log \nu_{n}([0,\infty)
}{n} +\mu([-K, K]).
\]
Let $ \psi_{K}(\lambda) = \log \displaystyle\int^{K}_{-K} e^{\lambda x}\diff\mu(x)$. Then, 
$ \log \displaystyle\int^{\infty}_{-\infty}e^{\lambda x} \diff \nu(x) = \psi_{K}(\lambda) -\log\mu([-K,K]) $. So, 
\[
	\overbrace{\lim_{n \to \infty}\frac{1}{n}\log\mu_{n}([0,\infty))]}^{ \text{exists again by sub-additivity}} \stackrel{\text{first step}}{\geq} \displaystyle\inf_{\lambda\geq 0} \left( \log \displaystyle\int^{\infty}_{-\infty}e^{\lambda x} \diff \nu(x)   \right)+ \log\mu([-K,K]) = \displaystyle\inf_{\lambda \geq 0}\psi_{K}(\lambda) \coloneqq J_{K}>-\infty.\\ 
\]
Now, as observe that $ \psi_{K}$ is a non-decreasing family of continuous functions. Hence, the $(J_{k})_{k\in \N}$ are non-decreasing and so one has $ J_{k}\uparrow J>-\infty$ $ K\to \infty$. Furthermore, the sets $ \{\lambda: \psi_{K}(\lambda)\leq J\}$ are compact by the continuity of the $ \psi_{K}$and the fact that $ \mu([0,K])>0$ implies $ \displaystyle\lim_{\lambda  \to \infty}\psi_{K}(\lambda) = \infty $, as well as nested. Thus, there exists some $ \lambda_{0}\in \displaystyle\bigcap_{k} \{\lambda: \psi_{K}(\lambda)\leq J\}$. hence, $ \psi(\lambda_{0}) = \displaystyle\lim_{k\to\infty}\psi_{K}(\lambda)\leq J $ by monotone convergence. So, 
	\[
\lim_{n \to \infty}\frac{1}{n}\log\mu_{n}([0,\infty)) \geq J\geq \psi(\lambda_{0})\geq \displaystyle\inf_{\lambda\geq 0 }\psi(\lambda)
	\] 
as required.\\

\newpage
\section{Brownian Motion}\label{Brownian motion}
\mymark{Lecture 17} 
\begin{boxdef}\label{def: Brownian motion}
	A process $ (B_{t})_{t\in \R_{+}}$ is called a \underline{Brownian motion} in $ \R^{d}$, $ d\geq 1$ starting from $ x\in \R^{d}$ if $ (B_{t})_{t\geq 0}$ is a continuous process and 
	\begin{enumerate}
		\item $ B_{0} = x$ a.s.
		\item For all $ s<t$, $ B_{t}-B_{s}\sim \mathcal{N}(0, (t-s)\cdot \Id_{d})$. 
		\item $ (B_{t})_{t\geq 0}$ has independent increments independent of $ B_{0}$. 
	\end{enumerate}
	If $ x = 0$ we call it a standard Brownian motion. Observe that $ i$ .determine uniquely its law.
\end{boxdef}

\begin{examplesblock}{Examples: }\label{examples: 9}
	Let $ (B_{t})_{t\geq 0}$ be a standard Brownian motion in $ \R$, $ U\sim [0,1]$ uniformly distributed and independent from $ (B_{t})_{t\geq 0}$ and define
	\[
		\tilde{B}_{t} = \left\lbrace
	\begin{array}{@{}l@{}}
	    B_{t}, \quad  t\neq U \\
	    0, \quad t = U
	\end{array}\right.
	\]
	Then $ \tilde{B}$ is a.s. discontinuous, so even though $ B, \tilde{B}$ have the same finite dimensional distributions, $ \tilde{B}$ is \underline{not} a Brownian motion.
\end{examplesblock}

\begin{theorem}[Wiener]\label{thm: wiener BM}
There exists a Brownian motion on some probability space.
\end{theorem}

\begin{proof}{(L\'{e}vy and Kolmogorov)}

    \begin{enumerate}
	    \item We shall proceed to construct a BM on $ [0,1]$ in $ d = 1$. Let $ \mathcal{D}_{0} = \{0,1\}$, $ \mathcal{D}_{n} = \{k\cdot 2^{-n}: 0\leq k \leq 2^n\}$ for $ n\in \N$ and $ \mathcal{D} = \displaystyle\bigcup_{n\geq 0} \mathcal{D_{n}}$.\\ 

		    We will now construct $ (B_{d}, d'in \mathcal{D})$ inductively. First for $ \mathcal{D}0$. Let $ (Z_{d}, d\in \mathcal{D})$ be an iid sequence $ \sim \mathcal{N}(0,1)$ on some probability space $ (\Omega, \F, \PP)$. Set $ b_{0} = 0$, $ B_{1} = Z_{1}$ (clearly satisfies properties in \ref{def: Brownian motion}). Suppose now we have constructed $ (B_{d}, d\in \mathcal{D}_{n-1})$ satisfying properties $ 2 \& 3$. We need to construct $ (B_{d}, d\in \mathcal{D}_{n})$.\\ 

		    For $ d\in \mathcal{D}_{n}\setminus \mathcal{D}_{n-1}$, have $d_{\pm} = d\pm 2^{-n}\in \mathcal{D}_{n-1}$. Now, set
		    \[
	 B_{d} =   \left\lbrace
		    \begin{array}{@{}l@{}}
		       \frac{B_{d-}+B_{d+}}{2}+\frac{Z_{d}}{2^{\frac{n+1}{2}}}, \quad d\in \mathcal{D}_{n}\setminus \mathcal{D}_{n-1} \\
		        B_{d}, \quad d\in \mathcal{D}_{n-1}.
		    \end{array}\right.
		    \]
		   We now show that our candidate process $ (B_{d})_{d\in \mathcal{D}_{n}}$ has independent increments. Indeed, we have that for $ d\in \mathcal{D}_{n}\setminus \mathcal{D}_{n-1}$,
		   \[
			   \begin{array}{cc}
		       &B_{d}-B_{d-} = \frac{B_{d+}-B_{d-}}{2}+\frac{Z_{d}}{2^{\frac{n+1}{2}}}\\ 
		       &B_{d+}-B_{d} = \frac{B_{d+}-B_{d-}}{2}-\frac{Z_{d}}{2^{\frac{n+1}{2}}}
		   \end{array}		   
		   \]
		   are independent. To see this, note that by induction we have that $\frac{B_{d+}-B_{d-}}{2}\sim \mathcal{N}(0, \frac{d+-d-}{4})$ and the same holds for $ \frac{Z_{d}}{2^{\frac{n+1}{2}}}$. Thus, $ _{d}-B_{d-}$, $B_{d+}-B_{d}$ are two mean-zero uncorrelated Gaussians, hence they are independent.\\ 

		   Now for any two disjoint intervals of length $ 2^{-n}$, the corresponding increments of the process $ (B_{d})_{d\in \mathcal{D}_{n}}$ are independent since one can express every increment as half the increment of the previous scale plus an independent Gaussian and apply the induction step.\\ 

		   Thus, we have been able to construct $ (B_{d}, d\in \mathcal{D})$ satisfying the conditions $ 2 \& 3$. Furthermore, by Gaussianity we have 
		   \[
		   \mathbb{E}\left[ |B_{d}-B_{q}|^{p} \right] = |d-q|^{\frac{p}{2}}\cdot \mathbb{E}\left[ |N|^{p} \right], 
		   \]
		   where $ N\sim \mathcal{N}(0,1)$. Since for all $p>0$ $ \mathbb{E}\left[ |N|^{p} \right]<\infty$. By Kolmogorov's continuity criterion, for all $ \alpha \in (0, \frac{\epsilon}{p})$ with $ \epsilon = \frac{p}{2}-1$ $(B_{d}, d\in \mathcal{D})$ is a.s. $ \alpha-$H\"{o}lder continuous for all $ \alpha <\frac{1}{2}$.\\

		   We now extend to the whole of $ [0,1]$ by setting $ B_{t} = \displaystyle\lim_{i \to \infty}B_{d_{i}} $, $ d\in \mathcal{D}$, $ d_{i}\to t$, $ i\to \infty$. It is immediate that $(B_{t, t\in [0,1])$ is a.s. $ \alpha-$ H\"{o}lder continuous for all $ \alpha<\frac{1}{2}$. Now it remains to check conditions $ 2\& 3$ are satisfied.\\

			   Let $ 0=t_{0\leq t_{1}\leq \cdots \leq t_{n}\leq 1$. Then, we claim 
				   the increments $ (B_{t_{i}}-B_{t_{i-1}})_{i=1,\cdots,k}$ are \underline{independent} Gaussian with $(B_{t_{i}}-B_{t_{i-1}})\sim \mathcal{N}(0, t_{i}-t_{i-1})$ for all $ 1\leq i \leq k$. Indeed, let
				   \[
				   \begin{array}{cc}
					      0\leq t^{n}_{0}\leq t^{n}_{1}\leq \cdots \leq t^{n}_{k}\leq 1 \\ 
 \downarrow \quad \quad  \downarrow \quad \cdots\quad\downarrow \\
 0\leq t_{0}\leq t_{1}\leq \cdots \leq t_{k}\leq 1
					   \end{array}		   
			   \]
			   be dyadic rationals. By continuity, we have a.s. $ B_{t^{n}_{j}}-B_{t^{n}_{j-1}}\stackrel{n\to \infty}{\longrightarrow}B_{t_{j}}-B_{t_{j-1}}$ for all $ j\leq k$. Thus, by bounded convergence, 
			   \[
			   \begin{array}{ll}
				   \mathbb{E}\left[ \exp \left( i\displaystyle\sum^{k}_{j=1}u_{j}(\smash{\overbrace{B_{t^{n}_{j}}-B_{t^{n}_{j-1}} )}^{\text{independent, normal}}} \right) \right] &= \displaystyle\prod^k_{j=1} \exp \left( \frac{-u^{2}_{j}(t^{n}_{j}-t^{n}_{j-1})}{2} \right) \\
			        &\stackrel{n\to \infty}{\longrightarrow} \displaystyle\prod^k_{j=1} \exp \left( \frac{-u^{2}_{j}(t_{j}-t_{j-1})}{2} \right)\equiv \phi(u). 
			   \end{array}
			   \]
			   By L\'{e}vy's convergence theorem, since $ \phi:\R^{k}\to \R$ is the characteristic function of independent Gaussians $ \sim \mathcal{N}(0, t_{j}-t_{j-1})$ and since the characteristic functions of the increments and the independent Gaussians agree, this forces the law of $ (B_{t_{j}}-B_{t_{j-1}})_{j\leq k}$ to be that of $ k$ independent $ \mathcal{N}(0, t_{j}-t_{j-1})$ gaussians. Hence, $ (B_{t}, t\in [0,1])$ satisfies all the properties.  

		   \item Extending the construction to all of $ \R$. Let $ (B^{i}_{t}, t\in[0,1])$ be independent brownian motions and define 
			   \[
			   B_{t} = B^{\lfloor t\rfloor}_{t-\lfloor t\rfloor} +\displaystyle\sum^{\lfloor t\rfloor-1}_{i=0} B^{i}_{t}, \quad t\geq 0.
			   \]
			   It is not hard to see that the conditions in \ref{def: Brownian motion} are satisfied. 

		   \item Now for $ d> 1$, let $ (B^{1}_{t})_{t\geq 0}, (B^{1}_{t})_{t\geq 0},\cdots, (B^{d}_{t})_{t\geq 0}$ be independent one dimensional Brownian motions. Set $ (B_{t})_{t\geq 0} = (B^{}_{t}, \cdots, B^{d_{t}})_{t\geq 0}$ and it is easy to check that the conditions are met. 

    \end{enumerate}
    
\end{proof}

\begin{theorem}\label{thm: scale invariance BM}
Let $ B$ be a standard Brownian motion in $ \R^{d}$. Then 
\begin{enumerate}
	\item If $ U$ is an orthogonal matrix, then $ UB = (UB_{t})_{t\geq 0}$ is also a standard Brownian motion. Hence so is $ -B$.
	\item (Scale invariance:) Let $ \lambda >0$ be given. Then $ \left( \frac{B_{\lambda t}}{\sqrt{\lambda}} \right)_{t\geq 0}$ is also a standard brownian motion. 
	\item (Simple Markov property:) For all $ c\geq 0$, $ (B_{t+s}-B_{s})_{t\geq 0}$ is also a standard Brownian motion and is independent of $ \F^{B}_{s}$, where $ \F^{B}_{s} = \sigma(B_{u}:u\leq s)$.
\end{enumerate}

\end{theorem}
\begin{proof}
    Easy to check that it follows from the definition of Brownian motion.
\end{proof}


\mymark{Lecture 18}
\subsection{Properties of Brownian Motion}\label{sec: bm properties}


\begin{theorem}[Time inversion]\label{thm: bm time inversion}
Let $ B$ be a standard Brownian motion in $ d=1$. Let 
\[
X_{t} = \left\lbrace
\begin{array}{@{}l@{}}
	tB_{\frac{1}{t}}, \quad t>0\\ 
	0, \quad t = 0.     
\end{array}\right.
\]
Then $ (X_{t})_{t\geq 0}$ is a standard brownian motion.
\end{theorem}

\begin{proof}
    Fix $ t_{t}, \cdots ,t_{k}>0$. Then $ (B_{t_{1}}, \cdots, B_{t_{k}})$ is  Gaussian random vector with zero mean and $ \text{Cov}(B_{s, B_{t}}= s\land t$. Need to check that $ (X_{t_{1}}, \cdots, X_{t_{k}})$ is Gaussian and with the same covariance as above. By inspection, we se thtah this vecto is clearly Gaussian with zero mean. Now for the covariance, we compute
    \[
    \text{Cov}(X_{t_{i}}, X_{t_{j}}) = \text{Cov}(t_{i}B_{t_{i}}, t_{j}B_{t_{j}}) = t_{i}t_{j}\text{Cov}(B_{t_{i}}, B_{t_{j}}) = t_{i}t_{j} \left( \frac{1}{t_{i}}\land \frac{1}{t_{j}} \right) = t_{i}\land t_{j}.
    \]
   Now it remains to show that $ X$ is continous. Indeed, for positive $ t$, $ X$ is clearly continuous. Now, we also claim that $ \lim_{t\downarrow 0} X_{t} = 0$ a.s. Observe that 
   \[
	   (X_{t}, t\in \Q_{+}) \stackrel{d}{=} (B_{t}, t\in \Q_{+})
   \]
   and so 
\[
\begin{array}{ll}
     
	\PP \left( \displaystyle\lim_{t\downarrow 0, t\in \Q_{+}}X_{t} = 0  \right) &=  \PP \left( \displaystyle\bigcap_{N\in \N} \displaystyle\bigcup_{r\in Q_{+}} \displaystyle\bigcap_{q\in \Q_{+}, q< r} \left\{|X_{q}|\leq \frac{1}{N}\right\}  \right)\\ 
								       &= \PP \left( \displaystyle\bigcap_{N\in \N} \displaystyle\bigcup_{r\in \Q_{+}} \displaystyle\bigcap_{q\in \Q_{+}, q< r} \left\{|B_{q}|\leq \frac{1}{N}\right\}  \right) = \PP \left( \displaystyle\lim_{t\downarrow 0, t\in \Q_{+}}B_{t} = 0  \right) = 1.
\end{array}
\]
Finally, since $ \Q_{+}$ is dense in $ \R_{+}
$ and $ X$ is continuous for $ t> 0$, we have that 
\[
\displaystyle\lim_{t\downarrow 0}X_{t} = \displaystyle\lim_{t\downarrow 0, t\in \Q_{+}}X_{t} = 0, \quad \text{ a.s.}
\] 
\end{proof}



\begin{boxcor}\label{cor: bm time inversion}
Let $ B$ be a standard brownian motion in $ d =1$. Then, 
\[
 \frac{B_{t}}{t}\stackrel{t\to \infty}{\longrightarrow}0,  \quad \text{ a.s. }
\]

\end{boxcor}

\begin{proof}

	By theorem \ref{thm: bm time inversion}, we have that with $ X$ defined therein,
	\[
	\displaystyle\lim_{t\to \infty}\frac{B_{t}}{t} = \displaystyle\lim_{t\to \infty}X \left( \frac{1}{t} \right) = 0
\]
by the continuity of $ X$ at zero.
\end{proof}


\begin{boxdef}\label{def: bm future sigma algebra}
For $ s\geq 0$, let $ \F^{+}_{s} = \displaystyle\bigcap_{t>s} \F^{B}_{t} = \sigma(B_{u}:u\leq t)$. Have $ \F^{B}_{s}\subseteq \F^{+}_{s}$.
\end{boxdef}

\begin{remark}
    From the simple Markov property, we have that 
    \[
	    (B_{t+s}-B_{s})_{t\geq 0}\perp \F^{B}_{s}.
    \]
    
\end{remark}

In fact we have more, that is 
\begin{theorem}\label{thm: bm Markov future sigma}
For all $s\geq 0$, 
\[
(B_{t+s}-B_{s})_{t\geq 0}\perp \F^{+}_{s}.
\]
\end{theorem}

\begin{proof}
It suffices to show that if $ t_{1}, \cdots, t_{k}$\in \R_{+}$ and $ F$ is a continuous and bounded, function on $ (\R^{d})^{k}$ and if $ A \subset \F^{+}_{s}$ then 
\[
  \mathbb{E}\left[ F(B_{t_{1}+s}-B_{s}, \cdots, B_{t_{k}+s}-B_{s})\cdot \mathbf{1}(A)
  \right] =  \mathbb{E}\left[ F(B_{t_{1}+s}-B_{s}, \cdots, B_{t_{k}+s}-B_{s})\right] \cdot \PP(A).
\]
Since, for any open set, $ U \subset (\R^{d})^{k}$, one can approximate $F_{m}\uparrow \mathbf{1}(U)$ from below by bounded continuous functions $ F_{m}(x) = f_{m}(\text{dist}(x, U^{c}))$, $ x\in (\R^{d})^{k}$ where $f: \R \to \R$ is the continuous, bounded function 
\[
f(r) = \left\lbrace
\begin{array}{@{}l@{}}
   1, \quad r \geq \epsilon  \\
   \frac{1}{\epsilon}r, \quad r<\epsilon. 
\end{array}\right. 
\]
for $ r\in \R$ and apply monotone convergence. Then one just has to observe that the collection of open sets generates the borel sigma algebra on $ (\R^{d})^{k}$ and apply the uniqueness of extension theorem.\\ 

Now, let $ s_{n}\downarrow s$ be a strictly decreasing sequence. Then, by continuity, have $ B_{t_{i}s_{n}}-B_{s_{n}}\stackrel{n\to \infty}{\longrightarrow}B_{t_{i}+s}-B_{s}$ a.s. for all $ i\leq k$.
 Thus, we have 
 \[
	 \mathbb{E}\left[ F(B_{t_{1}+s}-B_{s}, \cdots, B_{t_{k}+s}-B_{s})\cdot \mathbf{1}(A) \right]
\stackrel{\text{DCT}}{=}\mathbb{E}\left[ F(B_{t_{1}+s_{n}}-B_{s}, \cdots, B_{t_{k}+s_{n}}-B_{s_{n}})\cdot \mathbf{1}(A)
  \right]
  \]
 and observe that $ A\in \F^{+}_{s}$ implies $ A\in \F^{B}_{s_{n}}$
  for all $ n\in \N$. Thus, we can conclude by the simple Markov Property and another application of Dominated convergence.  
\end{proof}

\begin{boxcor}[Blumenthal's 0-1 Law]\label{cor: bm blumenthal}
	The sigma algebra $ \F^{+}_{0}$ is trivial, i.e. if $ A\in \F^{+}_{0}$, then $ \PP(A)\in \{0,1\}$. 
\end{boxcor}

\begin{proof}
    Take $ A\in \F^{+}_{0}\subseteq \sigma(B_{t}:t\geq 0 )$. But, bu the above, we have $ \sigma(B_{t}:t\geq 0 )\perp \F^{+}_{0}$ and so $ A \perp A$ which gives 
    \[
    \PP(A) = \PP(A\cap A) = \PP(A)\cdot\PP(A).
    \]
\end{proof}

\begin{theorem}\label{thm: bm zeros near zero}
Let $ B$ be a standard Brownian motion in $ d=1$. Define $ \tau = \displaystyle\inf\{t>0: B_{t} >0}$ and $ \sigma = \displaystyle\inf\{t>0 : B_{t} = 0\}$. Then $ \PP(\tau = 0) = \PP(\sigma=0) =1$.
\end{theorem}

\begin{proof}
	For all $ n\in \N$, have that $ \{\tau=0\} = \displaystyle\bigcap_{k\geq n} \underbrace{\{\therexists \epsilon\in(0, 1/k) \text{ s.t. } B_{\epsilon}>0\}}_{\F^{B}_{\frac{1}{n}}}$ and so have $ \{\tau = 0\}\in \F^{+}_{0}$ which means that $ \PP(\tau = 0) \in\{0,1\}$. Now, $ \PP(\tau \leq t)\geq \PP(B_{t}>0) = \frac{1}{2}$ for all $ t>$.  So, 
	\[
	\{{(\tau =0) = \displaystyle\limit_{t\downarrow 0}\PP(\tau \leq t)\geq \frac{1}{2}$
	\]
which gives that $ \PP(\tau = 0)=1$. By symmetry ($-B $ is a std BM) we also have that 
\[
	\displaystyle\inf\{t>0:B_{t}<0\} = 0, \quad \text{ a.s.}
\]
Since $ B$ is continuous, by the intermediate value theorem we get thath $ \sigma = 0$ a.s.
\end{proof}

\begin{boxprop}\label{prop: bm pos and neg values always attained}
Let $ B$ be a standard brownian motion in $ d=1$. For all $ t\geq0$,  set $ S_{t} = \displaystyle\sup_{s\leq t}B_{s}$ and $ I_{t} = \displaystyle\inf_{s\leq t}B_{s}$. Then, 

\begin{enumerate}
	\item For all $ \epsilon >0 $, have $ S_{\epsilon}>0$ and $ I_{\epsilon}<0$ a.s. In other words, in every interval $ (0,\epsilon)$ there exists a zero of BM.
	\item $ \displaystyle\sup_{t\geq 0}B_{t}  = +\infty$ and $ \displaystyle\inf_{t\geq 0}B_{t} = -\infty$ a.s. 
\end{enumerate}
\end{boxprop}

\begin{proof}
	\begin{enumerate}
		\item Let $ t_{n}\downarrow t$ as $ n\to \infty$. Then we have 
			\[
				\{B_{t_{n}} \text{ i.o }\}\subseteq \{S_{\epsilon}>0\}.
			\]
		It is not hard to see that $\{B_{t_{n}} \text{ i.o }\} \in \F^{+}_{0}$. Thus applyinf Fatou's lemma we deduce 
		\[
		\begin{array}{ll}
			\PP(B_{t_{n}} \text{ i.o. }) &= \PP(\displaystyle\limsup_{n} \{B_{t_{n}}\geq 0\}) \\
						     &\stackrel{\text{Fatou}}{\geq}\displaystyle\limsup_{n} \PP (\{B_{t_{n}}\geq 0\}) = \frac{1}{2}
		\end{array}
		\]
		Thus, $\PP(B_{t_{n}} \text{ i.o. })= 1 $ and so $ \PP(S_{\epsilon}>0) = 1$. By symmetry, ($-B$ is a std BM) we conclude that $ \PP(I_{\epsilon}<0) = 1$.
	\item Have for all $ \lambda >0$ that
		\[
			S_{\infty} = \displaystyle\sup_{t\geq 0}B_{t} = \displaystyle\sup_{t\geq 0}B_{\lambda t} \stackrel{d}{=} \sqrt{\lambda}\displaystyle\sup_{t\geq 0}\frac{B_{\lambda t}}{\sqrt{\lambda}}.
	\]
	So $ S_{\infty} \stackrel{d}{=} \alpha S_{\infty}$ for all $ \alpha >0$. We also know now thath $ S_{\infty}$. Hence it can only be the case that $ S_{\infty} = +\infty$ a.s. One can show that $ \displaystyle\inf_{t\geq 0}B_{t} = -\infty$ a.s.
	\end{enumerate}
\end{proof}

\begin{boxprop}\label{prop: bm cone}
Let $ B$ be a standard Brownian motion and let $ C$ be a cone with origin at zero and non-empty interior, that is $ \mathcal{C} = \{tu: t>0, u\in A\}$ with $ A\subseteq \mathbb{S}^{1} $(= unit sphere in $ \R^{d}$). Set $ H_{ \mathcal{C}} = \displaystyle\inf\{t>0: B_{t}\in \mathcal{C}\}$. Then, $ \PP(H_{ \mathcal{C}}=0)=1$.
\end{boxprop}

\begin{proof}
	Observe that $ \{H_{C}=0\}\in \F^{+}_{0}$ and $ \PP(B_{t}\in \mathcal{C}) = \PP(B_{1}\in \mathcal{C})$ by scale invariance of Brownian motion and $ \mathcal{C}$. Since $ \intr{ \mathcal{C}}\neq \emptyset$, $ \PP(B_{1}\in \mathcal{C})>0$. Thus, $ \PP(H_{ \mathcal{C}}\leq t)\geq \PP(B_{t}\in \mathcal{C})>0$.Taking $ t\downarrow 0$ and applying Blumenthal concludes the argument.
\end{proof}

\begin{figure}[H]
   \centering
   \includesvg[width=0.5\linewidth]{images/cone.svg}
   \caption{Illustration of cone in proposition \ref{prop: bm cone}}
\end{figure} 

\newpage
\mymark{Lecture 19} 
\begin{theorem}[Strong Markov Property]\label{thm: bm strong markov property}
Let $ B$ be a standard Brownian motion and let $ T$ be an a.s. finite stopping time. Then, 
$(B_{t+T}-B_{T})_{t\geq 0}$ is a standard Brownian motion and
\[
	(B_{t+T}-B_{T})_{t\geq 0}\perp \F^{+}_{T}.
\]
\end{theorem}

\begin{proof}
	Let $ T_{n} = 2^{-n}\lceil 2^n T\rceil$, $ T_{n}\downarrow T$, $ n\to \infty$. For $ k\in \N$, let $ B^{(k)}_{t} = B_{t+k\cdot 2^{-n}}-B_{k\cdot 2^{-n}}$ and $ B^{(n)}_{*}(t) = B_{t+T_{n}}-B_{T_{n}}$. Will show thath $ B_{*}$ is a Brownian motion independent of $ \F^{+}_{T_{n}}$.\\ 

	Clearly, $ B^{(n)}_{*}$ is continuous. Now, let $ A$ be any event and fix $ E\in \F^{+}_{T_{n}}$. Then, we compute 
	\[
	\begin{array}{ll}
		\PP(B_{*}\in A, E) &=\displaystyle\sum^{\infty}_{k=0}\PP(\displaystyle\overbrace{T_{n} = k\cdot 2^{-n}}^{\in \F^{+}_{k\cdot 2^{-n}}} ,\displaystyle\overbrace{B^{(k)}}^{\perp \F^{+}_{k\cdot 2^{-n}}} \in A, E)  \\
	     &= \displaystyle\sum^{\infty}_{k=0}\PP(T_{n} = k\cdot 2^{-n} ,E)\cdot \PP(B^{(k)}\in A) \\ & = \PP(B\in A)\cdot \PP(E).
	\end{array}
	\]
	He have thus shown that $ B_{*} \stackrel{d}{=} B$ and $ \perp \F^{+}_{T_{n}}$. Now, observe that 
	\[
\displaystyle\underbrace{B_{s+t+T}-Bs+T}_{ \mathcal{N}(0,t)} = \displaystyle\lim_{n\to \infty}(\displaystyle\underbrace{B_{s+t+T_{n}}-B_{s+T_{n}}}_{ \mathcal{N}(0,t)} ). 
	\]
	So, $ (B_{t+T}-B_{T})_{t\geq 0}$ is a standard BM.\\ 


It remains to show that $ (B_{t+T}-B_{T})_{t\geq 0} \perp \F^{+}_{T}$. Indeed, fix $ t_{1}, \cdots, t_{k}>0$ and let $ F:(\R^{d})^{k}:\to \R$ be a continuous and bounded function. Fix $ A\in\F^{+}_{T}$ and compute 
\[
	\mathbb{E}\left[ F(B_{t_{t}+T}-B_{T}, \cdots, B_{t_{k}+T}-B_{T})\cdot \mathbf{1}(A) \right] \stackrel{\text{DCT}}{=} \displaystyle\lim_{n\to \infty}\mathbb{E}\left[ F(B_{t_{t}+T_{n}}-B_{T_{n}}, \cdots, B_{t_{k}+T_{n}}-B_{T_{n}})\cdot \mathbf{1}(A) \right].
\]
Since $ A\in \F^{+}_{T}$, $ A\in \F^{+}_{T_{n}}$ for all $ n\in \N$. Finally, using that $ B^{(n)}_{*}\perp \F^{+}_{T_{n}}$ concludes the proof.
\end{proof}

\begin{theorem}[Reflection principle]\label{thm: bm reflection principle}
Let $ B$ be a standard Brownian motion in $ d=1$ and $ T$ an a.s. finite stopping time. Define 
\[
	\tilde{B}_{t} = \left\lbrace
	\begin{array}{@{}l@{}}
	    B_{t}, \quad 0\leq t\leq T \\
	    2B_{T}-B_{t}, \quad t>T
	\end{array}\right.
\]
Then $ \tilde{B}$ is a standard Brownian motion.
\end{theorem}

\begin{figure}[H]
    \centering
    \includesvg[width=0.5\linewidth]{images/reflection principle.svg}
    \caption{Illustration of reflection of $ B$ at time $ T$. }
    \label{fig: reflection}
\end{figure}

\begin{proof}
    We have by the Strong Markov Property that $B^{(T)} = (B_{t+T}-B_{T})_{t\geq 0}$ is a standard Brownian Motion independent of $ \F^{+}_{T}$. Let $ \mathcal{C_{0}} = \mathcal{C}_{0}([0,\infty):\R)$ denote the space of continuous functions on the positive reals that vanish at zero, endowed with the topology of local uniform convergence and $  \mathcal{A}$ the induced Borel sigma algebra. 

   \begin{examplesblock}{Metrisability of topology of local uniform convergence}\label{aside: metrisability of local uniform topology}
 Recall from Topology that this topology is induced by the metric 
\[
	\begin{array}{cc}
    d:  \mathcal{C}_{0}([0,\infty):\R)\times \mathcal{C}_{0}([0,\infty):\R)\to \R_{+}\\ 
    (f,g)\mapsto d(f,g)\coloneqq \displaystyle\sum_{n=1}^{\infty} \frac{1}{2^n}\frac{ \displaystyle\sup_{x\in [0,n]}|f(x)-g(x)|}{1+ \displaystyle\sup_{x\in [0,n]}|f(x)-g(x)|}
\end{array}
\]
   \end{examplesblock} 

   We also have the useful fact that

   \begin{examplesblock}{Characterisation of $ \mathcal{A}$}\label{aside: sigma alg space of cont funcs }
   We have, see Kallenberg's book on the `Foundations of Modern Probability` for instance, that
   \[
	   \mathcal{A} = \sigma(\{\pi_{t}: t\geq 0\})
   \]
   where for $ t\geq 0$, $ \pi_{t}: \mathcal{C}_{0}\to \R$ denotes the projection onto the $ t$ coordinate.
   \end{examplesblock}

    Now define the function 
    \[
	    \begin{array}{cc}

\psi: (\mathcal{C}_{0}\times [0,\infty) \times \mathcal{C}_{0},  \mathcal{A}\otimes \mathcal{B}([0,\infty))\otimes \mathcal{A})\to (\mathcal{C}_{0}, \mathcal{A})\\ 
(X,T,Y)\mapsto \psi_{T}(X,Y)(t)\\ 
\coloneqq X(t)\cdot \mathbf{1}([0,T])(t)+(X(t)+Y(t-T)) \mathbf{1}(T, \infty).
    
\end{array}

        \]
    is a continuous map in the product topology, therefore measurable. To see that $ \psi$ is continuous, 
    \begin{examplesblock}{Continuity of $ \psi$}\label{aside: proof of continuity reflection map}
	    Fix $ (X,T,Y)\in \mathcal{C}_{0}\times [0,\infty) \times \mathcal{C}_{0}$. Due to the metrisability of the underlying topologies, it suffices to show that for any sequence $(X_{n}, T_{n}, Y_{n})_{n\in \N}\subseteq  \mathcal{C}_{0}\times [0,\infty)\times \mathcal{C}_{0}$ such that $ X_{n}\stackrel{d}{\longrightarrow}X$, $Y_{n}\stackrel{d}{\longrightarrow}Y $ and $ T_{n}\to T$ as $ n\to \infty$, $\psi(X_{n}, T_{n}, Y_{n} ) \stackrel{d}{\longrightarrow} \psi(X,T,Y) $.\\ 

	    Now, fix $\epsilon>0 $, an arbitrary compact set $ K\subseteq  \R_{+}$ and let $ t\in K$ be arbitrary. We estimate
	    \[
	    \begin{array}{ll}
	    &|\psi(X_{n}, T_{n}, Y_{n})(t)-\psi(X, T, Y)(t)|\\ 
	    &\leq |(X(t)-X(T))\cdot \mathbf{1}(t\leq T)- (X_{n}(t)-X_{n}(T_{n}))\cdot \mathbf{1}(t\leq T_{n})| + \cancelto{0}{|X(T)-X_{n}(T_{n})|}  \\[10pt]
	    &+ |Y(t-T)\cdot \mathbf{1}((T_{n}\land T, T_{n}\lor T])|+ |Y(t-T)-Y_{n}(t-T_{n})|\\[10pt]
	    &\leq|(X(t)-X(T))\cdot \mathbf{1}((T_{n}\land T, T_{n}\lor T])|+ \cancelto{0}{\norm{X-X_{n}}_{\infty, K}}+ \cancelto{0}{|X(T)-X_{n}(T_{n})|} \\[10pt]
	    &+ |Y(t-T)\cdot \mathbf{1}((T_{n}\land T, T_{n}\lor T])|+ \cancelto{0}{|Y(t-T)-Y_{n}(t-T_{n})|}\\[10pt]
	    &\leq |(X(t)-X(T))\cdot \mathbf{1}((T_{n}\land T, T_{n}\lor T])| + |Y(t-T)\cdot \mathbf{1}((T_{n}\land T, T_{n}\lor T])|+\epsilon
	    \end{array}
	    \]
	    (where we make the set harmlessly $Y(t-T)=0$ for $ t\leq T$) for $ n\in \N$ large enough independent of $ t\in K$, since the crossed-out terms converge to zero uniformly in $ t\in K$ due to local uniform convergence and uniform contuinity on compact sets. The fact that $ Y(t-T)$ and $ X(t)-X(T)$ vanish at $ T$ and that $ T_{n}\to T$, $ n\to \infty$ enables us to bound for $ n$ sufficiently large independent of $ t$:
\[
	|\psi(X_{n}, T_{n}, Y_{n})(t)-\psi(X, T, Y)(t)|&\leq \displaystyle\sup_{t\in (T_{n}\land T, T_{n}\lor T]}\big(|(X(t)-X(T)|+|Y(t-T)|\big)+ \epsilon \leq 2\epsilon
\]
and conclude the argument. 
	    
	    
    \end{examplesblock}



Also observe that 
    \[
	    \begin{array}{cc}
		    \psi((B_{t\land T})_{t\geq 0}, T, B^{(T)}) = B\\ 
	\psi((B_{t\land T})_{t\geq 0}, T, -B^{(T)}) = \tilde{B}\\ 
    \end{array}
    \]
    By observing that $ B_{(T)}$ is independent of the stopped process $(B_{t\land T})_{t\geq 0}$,  we have that 
    \[
	    ((B_{t})_{t\geq 0}), T, (B^{T}_{t})_{t\geq 0}) \stackrel{d}{=} ((B_{t})_{t\geq 0}), T, -(B^{T}_{t})_{t\geq 0})
    \]
    and so it follows that $ B \stackrel{d}{=} \tilde{B}$. 
\end{proof}

\begin{boxcor}\label{cor: bm reflection principle}
For $ t\geq 0$, let $ S_{t} = \displaystyle\sup_{s\leq t}B_{s}$ and fix $ b>0$ and $ a\leq b$. Then 
\[
\PP(S_{t}\geq b, B_{t}\leq B_{t}\leq a) = \PP(B_{t}\geq 2b-a).
\]
\end{boxcor}

\begin{proof}
	Fix $ x>0$ and define $ T_{x} = \displaystyle\inf\{t\geq 0: B_{t} = x\}$. Since $ S_{\infty}<\infty$ a.s., it follows that $ T_{x}<\infty$ a.s. and $ B_{T_{x}} = x$. Observe that $ \{S_{t}\geq b\} = \{T_{b}\leq t\}$. Now we compute, 
	\[
	\begin{array}{ll}
	    \\
	    \PP(S_{t}\geq b, B_{t}, \leq a) &= \PP(\overbrace{T_{b}\leq t, B_{t}\leq a}^{\text{ on } T_{b}\leq t, \tilde{B}_{t} = 2b-B_{t}})\\ 
					    &= \PP(\tilde{B}_{t}\geq 2b-a, T_{b}\leq t)=\PP(\overbrace{\tilde{B}_{t}\geq 2b-a}^{ \tilde{B}_{t}\geq 2b-a \implies T_{b}\leq t})\\ 
				& = \PP(B_{t}\geq 2b-a).
	\end{array}
	\]
\end{proof}

\begin{boxcor}\label{cor: bm dist of running max}
	S_{t} \stackrel{d}{=} |B_{t}|.
\end{boxcor}

\begin{proof}
	\[
	\begin{array}{ll}
		\PP(S_{t}\geq a) &= \PP(\overbrace{S_{t}\geq a, B_{t}>a}^{=\PP(B_{t}\geq a)})+\overbrace{\PP(S_{t}\geq a, B_{t}\leq a)}^{=\PP(B_{t}\geq 2b-a) \text{ by the reflection principle}} \\
	     &= 2\PP(B_{t}\geq a)\\ 
	     &= \PP(|B_{t}|\geq a).
	\end{array}
	\]
\end{proof}

\begin{boxcor}\label{cor: bm hitting time distribution}
	Fix $ x>0$ and let $ T_{x} = \displaystyle\inf\{t\geq 0 : B_{t}=x\}$. Then 
	\[
		T_{x} \stackrel{d}{=} \left( \frac{x}{B_{1}} \right)^{2}.
	\]
\end{boxcor}

\subsection{Martingales for Brownian motion}

\begin{theorem}\label{thm: bm martingales}
Let $ (B_{t})_{t\geq 0}$ be a standard Brownian motion in $ d=1$. Then 
\begin{enumerate}
	\item $ (B_{t})_{t\geq 0}$ is a martingale with respect to the filtration $ (\F^{+}_{t})_{t\geq 0}$ 
	\item  $(B_{t}^{2}-t)_{t\geq 0}$ is a martingale with respect to the filtration $ (\F^{+}_{t})_{t\geq 0}$. 
\end{enumerate}
\end{theorem}

\begin{proof}
	Fix $ s\leq t$. Compute 
	\[
		\mathbb{E}\left[ B_{t}| \F^{+}_{s} \right] = \mathbb{E}\left[ \smash{\overbrace{B_{t}-B_{s}}^{\perp \F^{+}_{s}}} + B_{s} |\F^{+}_{s} \right] = B_{s}, \quad \text{ a.s.}
	\]
	and 
	\[
		\mathbb{E}\left[ (B_{t}^{2}-t)| \F^{+}_{s} \right] = \mathbb{E}\left[ (B_{t}-B_{s})^2 |\F^+_{s} \right] + 2 \cancelto{0}{\mathbb{E}\left[ (B_{t}-B_{s})B_{s} |\F^{+}_{s} \right]}\\ 
	+ \mathbb{E}\left[ B^{2}_{s}|\F^{+}_{s} \right]-t = B^{2}_{s}-s, \quad \text{ a.s.}

	\]	
\end{proof}

\begin{boxcor}\label{cor: bm gambler's ruin}
Let $ B$ be a standard Brownian motion in $ d=1$ and suppose $ x,y>0$. Then 
\[
	\PP(T_{-x}<T_{y}) = \frac{y}{x+y}
\]
and 
\[
 \mathbb{E}\left[ T_{-x}\land T_{y} \right] = x\cdot y
\]
with $ T_{\cdot}$ defined as in corollary \ref{cor: bm hitting time distribution}.
\end{boxcor}

\begin{boxprop}\label{prop: bm exp martingale}
Let $ B$ be a standard Brownian motion in $ \R^{d}$. Set 
\[
	M_{t} = \exp \left( \bracket{u}{B_{t}}-\frac{|u|^{2}t}{2} \right)
\]
is an $ \F^{+}_{t}$ martingale for all $ u\in \R^{d}$.
\end{boxprop}

\begin{proof}
    Fix $ u\in \R^{d}$. Integrability and adaptedness are clear. Now, for the martingale property, we have 
    
    \[
    \begin{array}{ll}
	    \mathbb{E}\left[ M_{t}|\F^{+}_{s} \right] &= \mathbb{E}\left[ \exp(\bracket{u}{B_{t}-B_{s}}-\bracket{u}{B_{s}}) |\F^{+}_{s}\right]\cdot e^{-\frac{|u|^{2}t}{2}}\\  
						      &= \exp(\bracket{u}{B_{s}})\cdot \exp \left( \frac{|u|^{2}(t-s)}{2} \right)\cdot e^{-\frac{|u|^{2}t}{2}} = M_{s}
    \end{array}
    \]
\end{proof}

\mymark{Lecture 20} 
\begin{theorem}\label{thm: bm ito formula}
Let $ f(t, x):\R_{+}\times \R^{d}\to \R$ be continuously differentiable in $ t$ and twice continuously differentiable in $ x$. Assume $ f$ and all its derivatives are bounded. Then the process 
\[
M_{t} = f(t, B_{t})-f(0, B_{0})- \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r, B_{r})\diff r   
\]
is an $\F^{+}_{t}-$martingale. 
\end{theorem}

\begin{proof}
    By the boundedness assumption, $ M$ is integrable and is clearly adapted. Now it remains to show the martingale property, that is for all $ t, z\geq 0$ $ \mathbb{E}\left[ M_{t+s}-M_{s} |\F^{+}_{s}\right] = 0$. We have 
    \[
    \begin{array}{ll}
        M_{t+s}-M_{s} &= f(t+s, B_{t+s})-f(s, B_{s}) - \displaystyle\int^{t+s}_{s} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r, B_{r})\diff r    \\
         &= f(t+s, B_{t+s})-f(s, B_{s}) - \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, B_{r+s})\diff r.
    \end{array}
    \]
    Now, taking conditional expectations, we have 
    \[
    \begin{array}{ll}
        \mathbb{E}\left[ M_{t+s}-M_{s}  |\F^{+}_{s}\right] &= -f(s, B_{s}) + \mathbb{E}\left[ f(t+s, B_{t+s}-B_{s}+B_{s}) |\F^{+}_{s} \right] \\
         &- \mathbb{E}\left[ \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, B_{r+s}-B_{s}+B_{s})\diff r |\F^{+}_{s}\right]\\ 
	 &\stackrel{(B_{r+s}-B_{s})_{r\geq 0}\perp \F^{+}_{s}}{=} -f(s, B_{s}) - \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, x+B_{s})\diff r \right) p_{r}(0,x)\diff x\\ 
	 &+ \displaystyle\int_{\R^{d}}f(t+s, x+B_{s}) p_{t}(0,x)\diff x
    \end{array}
    \]
    where $ p_{t}(0,x) = \frac{1}{\sqrt{2\pi t}^{d}}\exp \left( \frac{-|x|^{2}}{2t} \right)$ for $ x\in \R^{d}$, $ t>0$. Note that $ p_{t}$ satisfies the heat equation:
    \[
    \frac{\partial p_{t}}{\partial{t}} = \frac{1}{2}\Delta p_{t}.
    \]
    Using dominated convergence, we have that 
    \[
    \begin{array}{ll}
          \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, x+B_{s})\diff r \right) p_{r}(0,x)\diff x \\ 
	  = \displaystyle\lim_{\epsilon \downarrow 0}  \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{\epsilon} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, x+B_{s})\diff r \right) p_{r}(0,x)\diff x.
    \end{array}
    \]
   On $ (\epsilon, t)$, we have enough regularity to integrate by parts and s theorem to obtain 
   \[
   \begin{array}{ll}
        \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{0} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, x+B_{s})\diff r \right) p_{r}(0,x)\diff x \\
        = \displaystyle\lim_{\epsilon \downarrow 0}  \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{\epsilon} \left( \frac{\partial}{\partial r}+\frac{1}{2}\Delta \right) f(r+s, x+B_{s})\diff r \right) p_{r}(0,x)\diff x.\\ 
	= \displaystyle\int_{\R^{d}}f(t+s, x+B_{s}) p_{t}(0,x)\diff x - \displaystyle\lim_{\epsilon\downarrow 0}\displaystyle\int_{\R^{d}}f(\epsilon+s, x+B_{s}) p_{\epsilon}(0,x)\diff x \\ 
	+  \displaystyle\lim_{\epsilon \downarrow 0}  \displaystyle\int_{\R^{d}}\left( \displaystyle\int^{t}_{\epsilon} \left(\cancelto{0 (\text{PDE})}{-\frac{\partial p_{r}(0,x)}{\partial r}+\frac{1}{2}\Delta p_{r(0,x)} }\right) f(r+s, x+B_{s})\diff r \right) \diff x\\ 
	= \mathbb{E}\left[ f(t+s, B_{t+s}) \right]
- \displaystyle\lim_{\epsilon\downarrow 0}\mathbb{E}\left[ f(\epsilon+s, B_{\epsilon+s})|\F^{+}_{s} \right]\\ 
\stackrel{\text{DCT}}{=}\mathbb{E}\displaystyle\int_{\R^{d}}f(t+s, x+B_{s}) p_{t}(0,x)\diff x
-  f(s, B_{s}).
   \end{array}
   \]
   Combining all of the above together yields the desired equality $ \mathbb{E}\left[ M_{T+s}-M_{s} | \F^{+}_{s} \right]=0$ a.s.
\end{proof}

\subsection{Transience and recurrence}\label{sec: transience and recurrence}
Recall that if $ B$ is  Brownian motion $ B_{0}=0$ then it is called a standard Brownian motion. More generally, if $ B_{0} =x$ then call its law $ \PP_{x}$ and note that $ (B_{t}-x, t\geq 0)$ is a standard Brownian motion.\\ 

\begin{theorem}\label{thm: bm recurrence and transience}
Let $ B$ be a standard Brownian motion in $ \R^{d}$.\\ 

\begin{enumerate}
		\item If $ d=1$, then $ B$ is \underline{point-recurrent}, i.e. for all $ x,z$ $ \{t\geq 0: B_{t} = x\}$ is unbounded $ \PP_{z}-$a.s. \\ 
		\item If $ d = 2$, then $ B$ is \underline{neighbourhood recurrent}, that is for all $ \epsilon >0$ and $ x, z\in R^{d}$ the set of times $ \{t\geq 0 : |B_{t}-z|\leq \epsilon\}$ is unbounded $ \PP_{x}-$a.s. However, it does not hit points that is $ \PP_{x}(\exists t\geq 0: B_{t} = z)=0$. 
		\item If $ d=3$, $ B$ is \underline{transient}, that is $ |B_{t}|\to \infty$, as $ t\to \infty$ $ \PP_{x}-a.s.$
\end{enumerate}

\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item \underline{$ d=1$:} we have almost surely that $ \displaystyle\limsup_{t\to \infty} B_{t} = \infty$, $ \displaystyle\liminf_{t\to \infty}B_{t} = -\infty$ which gives the result. 
    \item \underline{$ d=2$:} by translation, it suffices to consider $ z = 0$. Fix radii $ \epsilon<|x|<R$. Let $ T_{r} = \displaystyle\inf\{t\geq 0: |B_{t}|=r\}$ for $ r>0$. We want to compute $ \PP_{x}(T_{\epsilon}<T_{R})$. Let $ H = T_{\epsilon}\land T_{R}$, an a.s. finite stopping time. Let $ \phi: \R^{2}\to \R$ be given by $ \phi(y) = \log|y|$ on the annulus $ \epsilon<|y|<R$ and extended outside that region in a fashion so that $ \phi\in \mathcal{C}^{2}_{b}(\R^{2})$. Then, $ \Delta\phi = 0$ in the annulus. 
	    \parbox[b]{0.55\textwidth}{ % The text part occupies 55% of the width  
	    By theorem \ref{thm: bm ito formula}, the process 
		    \[
		    M_{t} = \phi(B_{t})-\phi(B_{0})- \displaystyle\int^{t}_{0} \frac{1}{2}\Delta \phi(B_{s}) \diff s  
		    \]
		    is a continuous  $(\F^{+}_{t})_{t\geq 0}-$martingale. An argument similar to that in Theorem \ref{thm: OST for UI mg}\footnote{Approximate $ n\land H$ from above by the sequence $(T_{m})_{m\in\N}=(2^{-m}\lceil 2^m n\land H \rceil)_{m\in \N}$, use the discrete OST on the UI martingale $ (M_{d\land T_{m}})_{d\in \mathcal{ D}_{m}}$ of bdd stopping times and pass to the limit as $ m\to \infty$ using DCT.} gives $\mathbb{E}\left[ M_{n\land H} \right] = 0$ for all $ n\in \N$, in other words, $\mathbb{E}\left[ \log(|B_{n\land H}|) \right]=\log|x| $. Taking $ n\uparrow \infty$ and applying DCT gives $\mathbb{E}\left[ \log(|B_{H}|) \right]=\log|x| $. In other words, expressed in terms of the stopping times $ T_{\epsilon}, T_{R}$, this leads to
}
    \hfill  % Add space between the text and the figure
    \parbox[b]{0.4\textwidth}{ % The figure part occupies 40% of the width
        \centering
        \begin{tikzpicture}[scale=0.4]  % Adjust scale to fit nicely in the minipage

            % Define radii
            \def\innerRadius{2}
            \def\outerRadius{4}

            % Draw annulus (semi-complete)
            \draw[thick] (0,0) circle (\innerRadius); % Inner circle
            \draw[thick] (0,0) circle (\outerRadius); % Outer circle

            % Rays from origin to outer and inner circles
            \draw[thick] (0,0) -- (15:\innerRadius);   % First ray stopping at inner disk
            \draw[thick] (0,0) -- (30:\outerRadius);   % Second ray extending to outer disk

            % Draw portion of the annulus as incomplete
            \draw[thick] (45:\innerRadius) arc[start angle=45,end angle=135,radius=\innerRadius]; % Partial inner boundary
            \draw[thick] (45:\outerRadius) arc[start angle=45,end angle=135,radius=\outerRadius]; % Partial outer boundary

            % Add point x inside the annulus
            \filldraw[black] (10:3) circle (2pt); % Point in the annulus
            \node at (10:3.5) {$x$};  % Label for point x

            % Add label for inner and outer boundaries
            \node at (15:\innerRadius+0.3) {$\epsilon$}; % Move epsilon closer to the intersection with the inner disk
            \node at (30:\outerRadius+0.3) {$R$}; % Label for outer circle

            % Add label for origin
            \node at (0,0) [below left] {$0$};

        \end{tikzpicture}
    }	     		    \[
		    \PP_{x}(T_{\epsilon}<T_{R}) = \frac{\log R -\log |x|}{\log R - \log \epsilon} \tag{$\ast$}.
		    \]
		    Now, taking $ R\to \infty$, $ T_{R}\to \infty$ a.s. and so $ \PP_{x}(T_{\epsilon}<\infty)=1$. We now compute 
		    \[
		    \begin{array}{ll}
			    \PP_{x}(|B_{t}|\leq \epsilon \text{ for some }t>n) = \PP_{x}(|B_{t+n}-B_{n}+B_{n}|\leq \epsilon \text{ for some } t>0) \\
		         = \displaystyle\int_{\R} \PP_{0}(|B_{t}+y|\leq \epsilon \text{ for some }t>0) p_{n(x,y)}\diff y = 1.
		    \end{array}
		    \]
		    Hence, $ \{t\geq 0: |B_{t}|\leq \epsilon\}$ is unbounded $ \PP_{x}-$a.s. Now, in (*), letting $ \epsilon \to 0$, $ \PP_{x}(\text{ hit }0 \text{ before } R)=0$. Let $ R\to \infty$ we finally obtain $ \PP_{x}(\exists t>0: B_{t} = 0)=0$ for all $ x\neq 0$.\\ 

\mymark{Lecture 21} It remains to show now that $ \PP_{0}(B_{t}=0 \text{ for some }t>0)$=0. Indeed, let $ a>0$, and observe that 
\[
\begin{array}{ll}
	\PP_{0}(B_{t+a}=0 \text{ for some }t>0) &= \PP_{0}(\overbrace{B_{t+a}-B_{a}}^{\perp \F^{+}_{a}}+ B_{a}=0 \text{ for some }t> 0) \\
     &= \displaystyle\int_{R^{2}} \PP_{0}(\overbrace{B_{t+a}-B_{a}}^{\text{std BM}}+ y =0 \text{ for some }t> 0) p_{a}(y) \diff y  \\ 
     & = \displaystyle\int_{R^{2}} \PP_{y}(B_{t} = 0 \text{ for some }t> 0) p_{a}(y) \diff y = 0
\end{array}
\]
since $ \PP_{y}(\exists t>0: B_{t} = 0)=0$ for all $ y\neq 0$. So taking the limit as $ a\downarrow 0$, we get 
\[
\PP_{0}(B_{t}=0 \text{ for some }t>0) = \displaystyle\lim_{a\downarrow 0}\PP_{0}(B_{t}=0 \text{ for some }t>a) = 0.
\]
\item We now show that $ B$ is transient for $ d\geq 3$, that is $ |B_{t}|\to \infty$, as $t\to \infty$. To this end, it clearly suffices to prove transience for $ d=3$.\\ 
\parbox[b]{0.55\textwidth}{ 
As in the case $ d=2$, start by fixing radii $ \epsilon<|x|<R$. Let $ T_{r} = \displaystyle\inf\{t\geq 0: |B_{t}|=r\}$ for $ r>0$. Let $ H = T_{\epsilon}\land T_{R}$, an a.s. finite stopping time. Let $ \phi: \R^{2}\to \R$ be given by $ \phi(y) = \left(\frac{1}{|y|}\right)^{2-d}$ on the annulus $ \epsilon<|y|<R$ and extended outside that region in a fashion so that $ \phi\in \mathcal{C}^{2}_{b}(\R^{2})$. Then, $ \Delta\phi = 0$ in the annulus.}
    \hfill  % Add space between the text and the figure
    \parbox[b]{0.4\textwidth}{ % The figure part occupies 40% of the width
        \centering
        \begin{tikzpicture}[scale=0.4]  % Adjust scale to fit nicely in the minipage

            % Define radii
            \def\innerRadius{2}
            \def\outerRadius{4}

            % Draw annulus (semi-complete)
            \draw[thick] (0,0) circle (\innerRadius); % Inner circle
            \draw[thick] (0,0) circle (\outerRadius); % Outer circle

            % Rays from origin to outer and inner circles
            \draw[thick] (0,0) -- (15:\innerRadius);   % First ray stopping at inner disk
            \draw[thick] (0,0) -- (30:\outerRadius);   % Second ray extending to outer disk

            % Draw portion of the annulus as incomplete
            \draw[thick] (45:\innerRadius) arc[start angle=45,end angle=135,radius=\innerRadius]; % Partial inner boundary
            \draw[thick] (45:\outerRadius) arc[start angle=45,end angle=135,radius=\outerRadius]; % Partial outer boundary

            % Add point x inside the annulus
            \filldraw[black] (10:3) circle (2pt); % Point in the annulus
            \node at (10:3.5) {$x$};  % Label for point x

            % Add label for inner and outer boundaries
            \node at (15:\innerRadius+0.3) {$\epsilon$}; % Move epsilon closer to the intersection with the inner disk
            \node at (30:\outerRadius+0.4) {$R$}; % Label for outer circle

            % Add label for origin
            \node at (0,0) [below left] {$0$};

        \end{tikzpicture}
    }	     		
By theorem \ref{thm: bm ito formula}, the process 
		    \[
		    M_{t} = \phi(B_{t})-\phi(B_{0})- \displaystyle\int^{t}_{0} \frac{1}{2}\Delta \phi(B_{s}) \diff s  
		    \]
		    is a continuous  $(\F^{+}_{t})_{t\geq 0}-$martingale. Arguing in the same way as above, we obtain\\ $\mathbb{E}\left[ \left(\frac{1}{|B_{H}|}\right)^{2-d} \right]= \left(\frac{1}{|x|}\right)^{2-d} $. In other words, expressed in terms of the stopping times $ T_{\epsilon}, T_{R}$, this leads to

    \[
		    \PP_{x}(T_{\epsilon}<T_{R}) = \frac{\left(\frac{1}{R}\right)^{2-d}  - \left(\frac{1}{|x|}\right)^{2-d} }{\left(\frac{1}{\epsilon}\right)^{2-d}  - \left(\frac{1}{R}\right)^{2-d} } \tag{$\ast\ast$}.
		    \]
		    Now, taking $ R\to \infty$, $ T_{R}\to \infty$ a.s. and so $ \PP_{x}(T_{\epsilon}<\infty)= \left(\frac{\epsilon}{|x|}\right)^{2-d} $.\\ 
\parbox[b]{0.55\textwidth}{ % The text part occupies 55% of the width  
	    For $ n\in \N$, let $ A_{n} = \{|B_{t}|>n \text{ for all }t\geq T_{n^{3}}\}$, $T_{n^{3}}$ being almost surely finite for all $ n\in \N$. To prove $ |B_{t}|\to \infty$ a.s. as $ t\to \infty$, it suffices to show that the $ A_{n}$ happen eventually a.s. (recall $ d=3$). We now compute
		    \[
		    \begin{array}{ll}
		        \PP_{0}(A_{n}^{c}) &= \PP_{0}(|B_{t}|\leq n) \text{ for some }t\geq T_{n^{3}}) \\
					   &\stackrel{\text{SMP}}{=} \mathbb{E}_{0}\left[ \PP_{B_{T_{n}^{3}}}(|B_{t}|\leq n) \text{ for some }t\geq 0)  \right] = \frac{1}{n^{2}}
		    \end{array}
		    \]
		    so $\displaystyle\sum^{\infty}_{n=1}\PP(A_{n}^{c})<\infty $ and so we conclude that $ A_{n}$ occurs eventually in $ n\in \N$ a.s. thereby showing transience. }
    \hfill  % Add space between the text and the figure
    \parbox[b]{0.4\textwidth}{ % The figure part occupies 40% of the width
        \centering
        \begin{tikzpicture}[scale=0.4]  % Adjust scale to fit nicely in the minipage

            % Define radii
            \def\innerRadius{2}
            \def\outerRadius{4}

            % Draw annulus (semi-complete)
            \draw[thick] (0,0) circle (\innerRadius); % Inner circle
            \draw[thick] (0,0) circle (\outerRadius); % Outer circle

            % Rays from origin to outer and inner circles
            \draw[thick] (0,0) -- (15:\innerRadius);   % First ray stopping at inner disk
            \draw[thick] (0,0) -- (30:\outerRadius);   % Second ray extending to outer disk

            % Draw portion of the annulus as incomplete
            \draw[thick] (45:\innerRadius) arc[start angle=45,end angle=135,radius=\innerRadius]; % Partial inner boundary
            \draw[thick] (45:\outerRadius) arc[start angle=45,end angle=135,radius=\outerRadius]; % Partial outer boundary

            % Add label for inner and outer boundaries
            \node at (2:\innerRadius-0.5) {$n$}; % Move epsilon closer to the intersection with the inner disk
            \node at (35:\outerRadius-0.5) {$n^{3}$}; % Label for outer circle

            % Add label for origin
            \node at (0,0) [below left] {$0$};

        \end{tikzpicture}
    }	
    \end{enumerate}
    
\end{proof}

\subsection{Dirichlet Problem}\label{sec: Dirichlet problem}

\begin{boxdef}[Poincar\'{e} conce condition]\label{def: poincare cone condition}

$ D\subseteq \R^{d}$ is called a \underline{domain} if it is open, non-empty and connected. We say that $ D$ satisfies the Poincar\'{e} cone condition at $ x\in \partial D$ if there exists a non-empty open cone $ \mathcal{C}$ with origin at $ x$ and $ r>0$ such that $ \mathcal{C}\cap B(x,r)\subseteq D^{c}$.
\end{boxdef}

\begin{figure}[H]
    \centering
    \includesvg[width=0.4\linewidth]{images/poincare cone.svg}
    \caption{Illustration of Poincar\'{e} cone condition for a domain $ D\subseteq \R^{d}$.}
    \label{fig: poincare cone}
\end{figure}

\begin{theorem}[Dirichlet problem]\label{thm: dirichlet problem}

	let $ D$ be a bounded domain in $ \R^{d}$ such that every boundary point of $ D satisfies the  Poincar\'{e}$ cone condition, (see figure \ref{fig: poincare cone}). Let $ \phi$ be continuous on $ \partial D$ and let $ B$ be a Brownian motion, $ \tau_{\partial D} = \displaystyle\\displaystyle\inf_{\{t\geq 0 : B_{t}\in \partial D\}}$. Then the function 
	\[
		u(x)  = \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial D}}) \right], \quad x\in \overline{D}
	\]
	is the unique continuous function satisfying the boundary value problem 
	\[
	\left\lbrace
	\begin{array}{@{}l@{}}
	    \Delta u = 0, \quad \text{ in } D \\
	    u = \phi, \quad \text{ on }\partial D.
	\end{array}\right.
	\]
\end{theorem}

Before we proceed with the proof we recall some facts from the theory of PDEs. 

\begin{theorem}\label{thm: harmonic functions}
Let $ D\subseteq \R^{d}$ be a domain and $ u:D\to \R$ be measurable and locally bounded. Then the following are equivalent:
\begin{enumerate}
	\item $ u$ is twice continuously differentiable and $ \Delta u=0$.
	\item For all balls $B(x,r)\subseteq D $, 
		\[
		u(x) = \frac{1}{\mathcal{L}(B(x,r))} \displaystyle\int_{B(x,r)}u(y) \diff y .  
		\]
	\item For all balls $ B(x,r)\subseteq D$, 
		\[
		u(x) = \frac{1}{\sigma_{x,r}(B(x,r))} \displaystyle\int_{\partial B(x,r)}u(y) \diff\sigma_{x,r} y 
		\]
		where $ \sigma_{x,r}$ denotes the surface area measure of $ \partial B(x,r)$.
\end{enumerate}
\end{theorem}

\begin{boxdef}[Harmonic]\label{def: harmonic function}
	if $ u$ satisfies any of the above, we call u \underline{harmonic} in $ D$. 
\end{boxdef}

\begin{theorem}[Maximum principle]\label{thm: maximum principle}
Let $ u:\R^{d}:\to \R$ be harmonic in $ D$. Then 
\begin{enumerate}
\item If $ u$ attains its maximum in $ D$, then $ u$ is constant in $ D$. 
\item If $u$ is continuous in $ \overline{D}$ and $ D$ is bounded, then $ \displaystyle'max_{x\in \overline{D}}u(x) = \displaystyle\max_{x\in \partial D}u(x)$.
\end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item Let $ M$ be the maximum, let $ V = \{x\in D u(x) = M\}$, then by assumption such that the ball $ B(x,r\subseteq D$. Then, by the mean value property 
		    \[
		    M = u(x) = \frac{1}{\mathcal{L}(B(x,r))} \displaystyle\int_{B(x,r)}u(y) \diff y \leq M.
		    \]
		    Hence, $ u(y) = M$ for almost all $ y\in B(x,r)$. By the continuity of $ u$, we have equality everywhere in $ B(x,,r)$. Thus, $ B(x,r)\subseteq V$ and so $ V$ is now open, closed and also non-empty. Since, $ D$ is connected, we deduce that $ V = D$. 
	    \item $ u$ is continuous in $ \overline{D}$ and $ D$ is bounded implies that $ u$ attains a maximum in $ \overline{D}$. By $ 1$, $ \displaystyle \max_{\overline{D}}u = \displaystyle\max_{\partial D}u$.
    \end{enumerate}
\end{proof}

\begin{boxcor}\label{cor: max principle}
If $ u_{1}, u_{2}:\R^{d}$ are harmonic in $ D$, with $ D$ bounded and $ u_{1}$, $ u_{2}$ agree on $ \partial D$, then $u_{1} = u_{2}$ in $ D$. 
\end{boxcor}

\begin{proof}
    Have 
    \[
	    \displaystyle \max_{\overline{D}}(u_{1}-u_{2})=\displaystyle\max_{\partial D}(u_{1}-u_{2})=0
    \]
    by the maximum principle. Thus, $ u_{1}\leq  u_{2}$ for all $ x\in \overline{D}$ and  similarly we obtain $ u_{2}\leq u_{1}$ in $ \overline{D}$. Thus, we obtain $ u_{1}\equiv u_{2}$ in $ \overline{D}$.
\end{proof}


\mymark{Lecture 22} 
\begin{proof}{(Theorem \ref{thm: dirichlet problem})}
    With 
    \[
	    u(x) = \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial D}}) \right], \quad \text{ on } \overline{D}
    \]
    to show that $ u$ is twice continuously differentiable and harmonic, Theorem \ref{thm: harmonic functions} it suffices to show that it satisfies the mean value property.\\ 

    Now, we have that $ \tau = \displaystyle\\displaystyle\inf\{t\geq 0: B_{t}\in \partialb(x, \delta)\}<\infty$ a.s. and by the tower property 
    \[
    u(x) = \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial D}}) \right] = \mathbb{E}\left[ \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial d}}) | \F^{+}_{\tau}\right] \right]
    \]
   Now, define the function 
   \[
	   \begin{array}{cc}
       
F : \R_{+}\times \mathcal{C}_{0}([0,\infty))\to \R \\ 
(z, f)\mapsto F(z,f)\coloneqq \displaystyle\inf\{t\geq 0: z + f(t) \in D^{c}\}
   \end{array}
   \]
   which is measurable with respect to $ \mathcal{B}(\R_{+}) \otimes \mathcal{A}$, where $ \mathcal{A}$ is the Borel sigma algebra induced by the topology of local uniform convergence, as in the proof of Theorem \ref{thm: bm reflection principle}. Observe now that $ \tau_{B_{\partial D}} = \tau + F(B_{\tau}, (B_{\tau +t}-B_{\tau})_{t\geq 0})$. a.s.  By the strong Markov property, $ (B_{\tau +t}-B_{\tau})_{t\geq 0}\perp \F^{+}_{\tau}$ and so we can conclude 
   \[
   \begin{array}{ll}
        u(x) &= \mathbb{E}\left[ \mathbb{E}_{x}\left[ \phi(B^{(\tau)}_{\tau + F(B_{\tau}, B^{(\tau)}) }+B_{\tau}) | \F^{+}_{\tau}\right] \right]
 \\
	     &= \mathbb{E}\left[ \mathbb{E}_{x}\left[ \phi(B^{(\tau)}_{\tau + F(B_{\tau}, B^{(\tau)}) }+B_{\tau}) | \F^{+}_{\tau}\right] \right]\\ 
	     & = \mathbb{E}\left[ \mathbb{E}_{x}\left[ \phi(g(B_{\tau}, B^{(\tau)})+B_{\tau}) | \F^{+}_{\tau}\right] \right]

   \end{array}
   \]
   where $ g$ is the continuous, hence measurable function 
\[
	   \begin{array}{cc}
       
F : \R_{+}\times \mathcal{C}_{0}([0,\infty))\to \R \\ 
(z, f)\mapsto g(z,f)\coloneqq f(z)
\end{array}
   \]
   thus another application of the strong Markov property gives 
    \[
    \begin{array}{ll}
	    u(x) &\stackrel{\text{SMP \& indep.}}{=} \overbrace{\mathbb{E}_{x}\left[ \displaystyle \int_{ \mathcal{C}_{0}([0,\infty))}\left[ \phi(g(B_{\tau}, w)+B_{\tau}) \underbrace{\mu(\diff w)}_{\mu \text{ denotes the Wiener measure of BM started at }x} \right]}^{\text{the law of }B_{\tau} \text{ is invariant under rotations and by uniqueness is the spherical measure } \sigma(x,\delta)}  \\
	     &=  \frac{1}{\sigma_{x,r}(B(x,\delta))} \displaystyle\int_{\partial B(x,\delta)}\displaystyle \int_{ \mathcal{C}_{0}([0,\infty))} \phi(g(y, w)+y) \mu(\diff w)
 \diff\sigma_{x,\delta} y  \\
	     &=  \frac{1}{\sigma_{x,r}(B(x,\delta))} \displaystyle\int_{\partial B(x,\delta)}\mathbb{E}_{y}\left[\phi(g(y, B)+y)\right] 
 \diff\sigma_{x,\delta} y  \\ 
	     & = \frac{1}{\sigma_{x,r}(B(x,\delta))} \displaystyle\int_{\partial B(x,\delta)}u(y) \diff\sigma_{x,\delta} y  

    \end{array}
    \]
    thus, showing that $ u$ is indeed harmonic. Uniqueness follows from the result \ref{cor: max principle} established earlier. \\ 

    It remains to show that $ u$ is continuous up to the boundary, $ \partial D$. Let $ z\in \partial D$. Need to show that $ u$ is continuous at $ z$. Since $ \phi$ is continuous on $ \partial D$, we have that for all $ \epsilon>0$, there exists a $ \delta >0$ such that if $ |y-z|\leq \delta$, $ y\in \partial D$, $ |\phi(y)-\phi(z)|<\epsilon$. 
\begin{figure}[H]
	    \centering
	    \includesvg[width=0.6\linewidth]{images/nested discs delta.svg}
	    \caption{Illustration of situation near the boundary.}
	    \label{fig: nested discs delta}
	\end{figure}
    Let $ k\in \N$ to be determined and let $ x$ be such that $ |x-z|\leq 2^{-k}\cdot \delta$, then we estimates 
    \[
    \begin{array}{ll}
        |u(x)-u(z)| &\leq \left| \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial D}})-\phi(z) \right] \right|\\
         &\leq  \mathbb{E}_{x}\left[| \phi(B_{\tau_{\partial D}})-\phi(z)| \right] \\ 
	 &\leq \epsilon \cdot \PP_{x}(\tau_{\partial D}<\tau_{\partial B(z,\delta)}) + 2\norm{\phi}_{\infty}\cdot \PP_{x}(\tau_{\partial B(z,\delta}<\tau_{\partial D}).	 
    \end{array}
    \]
   Now, By the Poincar\'{e} cone condition, let $  \mathcal{C}_{z}$ be an open cone at $ z$ that lies in $ D^{c}$ sufficiently close to $ z$. Then 
	 \[
	\PP_{x}(\tau_{\partial B(z,\delta}<\tau_{\partial D})\leq \PP_{x}(\tau_{\partial B(z,\delta}<\tau_{\partial C_{z}}).
	 \]
We claim that 
\[
	\displaystyle\sup_{x\in B(0, \frac{1}{2})}\PP_{x}(\tau_{\partial B(0,1)}<\tau_{C})\leq \alpha <1 \tag{***}.
\]
where $ \mathcal{C}$ is a translate of the cone $ \mathcal{C}_{z}$ to the origin. 
\begin{examplesblock}{Proof of (***)}\label{aside: proof cone bound}
\parbox[b]{0.5\textwidth}{To establish $ (***)$ it suffices to show that Brownian motion stays arbitrarily close to straight bounded segment of lines (and in fact to any continuous function) with positive probability, i.e. 
\[
	p_{\epsilon, x, a} \coloneqq \PP_{x}(\norm{B-\ell_{x,a}}_{\infty, [0,1]}\leq \epsilon) >0 
\]
$ \epsilon >0, x\in \R^{d}$ and lines $ \ell_{x,a}$ connecting points $x,a\in \R^{d}$, i.e. $\ell_{x,a}(t) = tx + (1-t)a$, $ t\in [0,1]$ (see figure \ref{fig: cone bound}). To see this, the bound $ (***)$ essentially reduces to bounding uniformly from zero the probability that a Brownian path starting from a point $ x\in B(0, 1/2)$ stays within some uniform in $ x$ amount $ \epsilon > 0$ close to a line of length $ \leq 2$ (figure \ref{fig: cone bound}).\\ }\makebox[1cm]{}\parbox[b]{0.4\textwidth}{ 
	    \includesvg[width=\linewidth]{images/cone bound.svg}\label{fig: cone bound}
}
By the geometry of the situation, an $ \epsilon>0$ exists, so that no matter where the BM starts in $ B(0,1/2)$, there is a direction it can stay within epsilon to for times $t \in [0,1]$ that would guarantee it never touches the boundary $ \partial B(0,1)$ before the cone, in $ [0,1]$. \\  

We proceed with several reduction steps. By translation, we can let without loss of generality $ x=0$ and consider only $ p_{\epsilon,a}\coloneqq p_{\epsilon, 0, a}$ for $ \epsilon>0, a\in \R^{d}$. Moreover, by containment of events that $ p_{\epsilon, a}$ is decreasing in $ \epsilon>0$ and less than one away from zero with $ p_{\epsilon, 0} = 1$ for all $ \epsilon>0$. By independence of the components of $ B$ and rotational symmetry, we have 
\[
\begin{array}{ll}
	\PP_{0}(\norm{B_{t}-at}_{\infty, [0,1]}\leq \epsilon) &= \left(\PP_{0}(\norm{B}_{\infty, [0,1]}\leq \epsilon\right)^{d-1}\cdot \PP_{0}\left(\norm{B^{1}_{t}-\norm{a}\cdot t}_{\infty, [0,1]}\leq \epsilon\right)\\			      
\end{array}
\]
We now show that for fixed $ \epsilon,  >0$ and $|\lambda|$ bounded, one has a positive uniform lower bound on the probabilities 
\[
\PP_{0}(|W_{t}-\lambda t|\leq \epsilon). 
\]
where $ W$ is a standard Brownian motion in one dimension. To show this it suffices to note that from L\'{e}vy's construction of Brownian motion, one constructs $ (W_{t}, t\in [0,1])$ as an a.s. uniformly convergent power series (starting from the iid sequence $ (Z_{d})_{d\in \mathcal{D}}$) as 
\[
W_{t} =\displaystyle\sum^{\infty}_{n=1}F_{n}(t) 
\]
where the $ F_{n}$ are independent, piecewise linear functions given by 
\[
F_{n}(t) = \left\lbrace
\begin{array}{@{}l@{}}
	2^{-\frac{n+1}{2}}Z_{t}, \quad t\in \mathcal{D}_{n}\setminus \mathcal{D}_{n-1} \\
    0, \quad t\in \mathcal{D}_{n-1}\\ 
    \text{linear between consecutive points in } \mathcal{D}_{n}.
\end{array}\right.
\]
for $ n\geq 1$ and define $ F_{0}$ by interpolating linearly between $ Z_{1} $ and zero in $ [0,1]$. Now, a straightforward computation yields that for $c>0 $, $ n\in \N$ sufficiently large independent of $ c$, $\PP(|Z_{d}|\geq c\sqrt{n})\leq \exp \left( -\frac{c^{2}n}{2} \right) $. This in conjunction with the Borell-Cantelli lemma gives 
\[
	\PP(\{\exists d\in \mathcal{D}_{n}, \text{ s.t. } |Z_{d}|<c\sqrt{n}\}) = 0, \quad \text{ for }c>\sqrt{2\log 2}.
\]
Thus, using the continuity of $ \PP$, there exists some $ M\in \N$ such that 
\[
	\PP \left( \displaystyle\bigcap_{n\geq M} \{\norm{Z_{n}}_{\infty}< c\sqrt{n}2^{-\frac{n}{2}}\} \right) > 0.
\]
Additionally, observe that for the truncated series $ \sum_{n=1}^{N} F_{n}$ corresponds to the piecewise linear extension of the dyadic approximations to $ W$ in Theorem \ref{thm: wiener BM}. Now, for $ |\lambda|$ bounded, $ M$ possibly even larger,  by the independence of the Gaussians $ (Z_{d}, d\in \mathcal{D})$ it is not hard to approximate $ \lambda t$ on $ [0,1]$ by the truncated series up to stage $ M-1$ and then use the independence of the $ F_{n}$ and the above to obtain with a positive probability that the tail is also uniformly close and the approximation at stage $ M-1$, hence proving there is a uniform positive lower bound, thus proving $ (***)$. \\ 
\end{examplesblock}

In the final step, essentially we will iterate the bound $ (***)$ on nested balls surrounding the origin to get expo natal decay in the corresponding version of $ (***)$. More precisely, 

\begin{examplesblock}{Exponential decay step}\label{aside: poincare cone decay bound}
Let $ k\in \N$. We want to bound 
\[
\PP_{x}(\tau_{\partial B(0,1)}\subseteq \tau_{ \mathcal{C}}), \quad x\in B(0, 1/2^k).
\]
Now, we have 
\[
\begin{array}{ll}
	\PP_{x}(\tau_{\partial B(0,1)}\subseteq \tau_{ \mathcal{C}})&\\ 
	&= \mathbb{E}_{x}\left[ \mathbf{1}(\tau_{\partial B(0,1)}<\tau_{\mathcal{C}}) \right] \\
								&= \mathbb{E}_{x}\left[ \overbrace{\mathbf{1}(\tau_{\partial B(0,2^{-(k-1)})}<\tau_{\mathcal{C}})}^{\{\tau_{\partial B(0, 2^-k)}<\tau_{ \mathcal{C}}\}\supseteq \tau_{\partial B(0, 1)}<\tau_{ \mathcal{C}}\}}\cdot \mathbb{E}\left[ \mathbf{1}(\tau_{\partial B(0,1)}<\tau_{ \mathcal{C}})|\F^{+}_{\tau_{\partial B(0, 2^{-(k-1})}} \right] \right]\\ 
								&\stackrel{SMP}{=} \mathbb{E}_{x}\left[ \mathbf{1}(\tau_{\partial B(0,2^{-(k-1)})}<\tau_{\mathcal{C}}) \cdot \PP_{B_{\tau_{\partial B(0,2^{-(k-1)})}}}(\tau_{\partial B(0,1)}<\tau_{ \mathcal{C}})\right]\\ 
								&\leq \PP_{x}(\tau_{\partial B(0,1)}<\tau_{ \mathcal{C}})\cdot \displaystyle\sup_{y\in B(0, 2^{-(k-1))}}\PP_{y}(\tau_{\partial B(0,1)}<\tau_{ \mathcal{C}})\tag{A}.
\end{array}
\]
Now, for $ x\in B(0, 2^-k)$ consider $ \PP_{x}(\tau_{\partial B(0, 2^{-(k-1)})}<\tau_{ \mathcal{C}})$. By scale invariance of Brownian motion and the cone, $ \mathcal{C}$ we have with $ \lambda = 2^{(k-1)}$, 
\[
\begin{array}{ll}
&\PP_{x}(\tau_{\partial B(0, 2^{-(k-1)})}<\tau_{ \mathcal{C}})\\[10pt]
&= \PP_{0}(\displaystyle\inf\{t\geq 0: \overbrace{W_{t}}^{\text{std BM}} \in \overline{B(0, 2^{-(k-1)})}\}<(\displaystyle\inf\{t\geq 0: W_{t} \in \overline{ \mathcal{C}}\}) \\[10pt]
&\stackrel{\text{scale invariance}}{=} \PP_{0}(\displaystyle\inf\{t\geq 0: 1/\lambda W_{\lambda^{2}t} \in \overline{B(0, 2^{-(k-1)})}\}<(\displaystyle\inf\{t\geq 0: /\lambda W_{\lambda^{2}t} \in \overline{ \mathcal{C}}\}) \\[10pt]
& = \PP_{0}(\displaystyle\inf\{t\geq 0: W_{\lambda^{2}t}+\lambda x \in \overline{B(0, 1)}\}<(\displaystyle\inf\{t\geq 0: W_{\lambda^{2}t} + \lambda x \in \overline{ \mathcal{C}}\}) \\[10pt]
& = \PP_{0}(1/\lambda^{2}\displaystyle\inf\{\lambda^{2}t\geq 0: W_{\lambda^{2}t}+\lambda x \in \overline{B(0, 1)}\}<1/\lambda^{2}\displaystyle\inf\{\lambda^{2}t\geq 0: W_{\lambda^{2}t} + \lambda x \in \overline{ \mathcal{C}}\}) \\[10pt]
& = \PP_{\lambda x}(\displaystyle\inf\{t\geq 0: B_{t} \in \overline{B(0, 1)}\}<\displaystyle\inf\{t\geq 0: B_{t}\in \overline{ \mathcal{C}}\}) \\[10pt]
& = \PP_{\lambda x}(\tau_{\partial B(0, 1)}<\tau_{ \mathcal{C}}) \leq \displaystyle\sup_{|y|\leq 2^{-(k-1)}}\PP_{y}(\tau_{\partial B(0,1)}<\tau_{ \mathcal{C}}).
\end{array}
\]
\end{examplesblock}
Thus, inducting on $ k\in \N$ and using $ (***)$ as the base case and $ (A)$ as the induction step we deduce that 
\[
	\displaystyle\sup_{x\in B(0, 2^{-k})}\PP_{x}(\tau_{\partial B(z,\delta)}<\tau_{\mathcal{C}_{z}})\leq \alpha^k\to 0, \quad k\to \infty
\]
which allows us to conclude the proof.  

\end{proof}

\begin{examplesblock}{Example: }\label{examples: 10}
	Let $ d=2$ and let $ \phi:\partial B(0,1)\to \R$ continuous. Let $ v:D\to \R$ where $ D = B(0,1)\setminus \{0\}$ be the unique solution to the Dirichlet problem on $ B(0,1)$ with boundary data $ \phi$. Augment $ \phi $ to $ \partial B(0,1)\cap \{0\}$ and observe that 
	\[
	u(x) = \mathbb{E}_{x}\left[ \phi(B_{\tau_{\partial D}}) \right]
	\]
	is \underline{not} a solution if $ v(0)\neq \phi(0)$ since $ u(0)  = \phi(0) = v(0)$ because Brownian motion does not hit points, as we proved in Theorem \ref{thm: bm recurrence and transience}.
	\end{examplesblock}

	\section{Donsker's invariance principle}\label{sec: Donsker}
The main theorem of this section is Donsker's inavriance principle, which states 
\begin{theorem}[Donsker's invariance principle]\label{thm: donsker}
	Let $ X_{1}, X_{2}, \cdots$ be iid $ \R-$valued integrable random variables with law $ \mu$, such that $ \mathbb{E}\left[ X_{1} \right]=0$, and variance $ \sigma^{2}\in (0,\infty)$. Set $ S_{0}=0$, $ S_{n} X_{1}+\cdots +X_{n}$ for $ n\geq 1$ and $ S_{t} = (1-\{t\})S_{[t]}+\{t\}S_{[t]+1}$, where $ \{t\}= t-[t]$ and $ [t]$ is the integer part of $ t\geq 0$. Now, define 
	\[
		S^{[N]}_{t} = \frac{S_{tN}}{\sqrt{\sigma^{2}N}}
	\]
	for $ 0\leq t\leq 1$. Then, $ \left(  S^{[N]}_{t}, 0\leq t\leq 1\right)$ converges weakly to $ (B_{t}, 0\leq 1\leq 1)$, that is to a standard Brownian motion. More explicitly, we have for all continuous (in the local uniform topology) and bounded functionals $ F: \mathcal{C}([0,1], \R)\to \R$
	\[
	\mathbb{E}\left[ F(S^{[N]}) \right] \stackrel{N\to \infty}{\longrightarrow} \mathbb{E}\left[ F(B) \right].
	\]
\end{theorem}
	Before we prove it we need a supporting result, the so-called Skorokhod embedding.

	\begin{theorem}[Skorokhod embedding]\label{thm: skorokhod embedding}
	Let $ \mu$ be a probability measure with zero mean and variance $ \sigma^{2}\in (0,\infty)$. Then, there exists a probability space $ (\Omega, \F, \PP)$, a filtration $ (\F_{t})_{t\geq 0}$, a Brownian motion $ (B_{t})_{t\geq 0}$ and a sequence of stopping times $ 0=T_{0}\leq T_{1}\leq \cdots\leq$ such that
	\begin{enumerate}
		\item The sequence defined by $ (S_{n})_{n\in \N} = (B_{T_{n}})_{n\in \N}$ for $ n\in \N$ is a random walk with step distribution $ \mu$.
		\item The sequence $ (T_{n})_{n\in N}$ and steps of mean $ \sigma^{2}$.
 
	\end{enumerate}
\end{theorem}

\mymark{Lecture 23} 
\begin{proof}
Define the Borel measures on $ \mathcal{B}([0,\infty))$, for $ A\in \mathcal{B}([0,\infty))$
    \begin{eqnarray*}
	    &\mu_{+}(A) = \mu(A\cap [0, \infty))\\
	    &\mu_{-}(A) = \mu(-A\cap (-\infty,0)).
    \end{eqnarray*}
    Let $ (\Omega, \F, \PP)$ be a probability space on which we define a standard Brownian motion $ (B_{t})_{t\geq 0}$ and the iid sequence $ (X_{n}, Y_{n})_{n\in \N}$ with law $ \nu(\diff x,\diff y)=C\cdot \mu_{-}(\diff x)\mu_{+}(\diff  y)$ (independent from $ B$) and $ C>0$ a normalising constant. We have 
    \[
    \begin{array}{ll}
        \displaystyle\int^{\infty}_{0} \displaystyle\int^{\infty}_{0} \nu( \diff x,y\diff y) =1    &=  C\mu([0,\infty)) \displaystyle\int^{\infty}_{0}x\mu_{-}(\diff x)  \\
         &+ C\mu((-\infty, 0)) \displaystyle\int^{\infty}_{0}y\mu_{+}(\diff y). 
    \end{array}
    \]
    Since $ \mu$ has mean zero, we also have that $ \displaystyle\int^{\infty}_{0}x\mu_{-} (\diff x) =   \mu((-\infty, 0)) \displaystyle\int^{\infty}_{0}y\mu_{+}(\diff y)$ which gives 
    \[
    C  \displaystyle\int^{\infty}_{0}x\mu_{-} (\diff x)= C \mu((-\infty, 0)) \displaystyle\int^{\infty}_{0}y\mu_{+}(\diff y)=1.
    \]
    Now, define the random sequence $ T_{0}=0$, and for $ n\geq 1$
    \[
	    T_{n+1}= \displaystyle\inf\{t\geq T_{n}: B_{t}-B_{T_{n}}\in \{-X_{n+1}, Y_{n+1}\}\}.
    \]
    We claim that the $ (T_{n})_{n\in \N}$ are stopping times with respect to the filtration $ \F_{t}= \sigma(\F^{B}_{t}, \F_{0})$ for $ t\geq 0$ where $ \F_{0} = \sigma((X_{n}, Y_{n}):n\in \N)$.

    \begin{examplesblock}{ $(T_{n})_{n\in\N}$ are stopping times}\label{aside: skorokhod stopping time}
	    To see that the $(T_{n})_{n\in\N}$ are stopping times, we proceed by induction. Clearly $ T_{0}\equiv 0$ is a stopping time. Now, suppose for $ n\in N$ that $ T_{n}$ is a stopping time. Now, Fox $ s\geq 0$ and observe that since $ T_{n+1}=T_{n}+\eta$ a.s. where $ \eta = \displaystyle\inf\{t\geq 0 : B^{(T_{n})}\in \{-X_{n+1}, Y_{n+1}\}\}$
	    \[
		    \{T_{n+1}\leq s\} = \overbrace{\{T_{n}\leq r\}}^{\{T_{n+1}\leq t\}\subseteq \{T_{n}\leq t\}}\cap \{\eta\leq s\}.
	    \]
	    Observe, that $ \eta $ is $ \sigma(\F^{B}_{T_{n}}, \F_{0})-$measurable. Hence, we have that $ \{T_{n+1}\leq s\}\in \sigma(\F^{B}_{T_{n}}, \F_{0})\cap \F^{B}_{s}$. Noting that we can express $ sigma(\F^{B}_{T_{n}}, \F_{0})= \sigma(\{A\cap B: A\in \F^{B}_{T_{n}}, B\in \F_{0}\})\subseteq \F^{B}_{s} $, we conclude  that $ \{T_{n+1}\leq s\}\subseteq \F^{B}_{s}$ for any $ s\geq 0$, hence proving the statement. 
    \end{examplesblock}

    Now, define the measurable (wrt to the usual sigma algebras) function 
\[
\begin{array}{ll}
    \tau: \mathcal{C}([0,\infty))\times \R \to \R \\
    (f, x)\mapsto \displaystyle\inf\{t\geq 0 : f(t)=x\}
\end{array}
\]
and conditioning on $ X_{1}, Y_{1}$ we compute 
    \[
    \begin{array}{ll}
	    \PP(B_{T_{1}}=Y_{1}|X_{1, Y_{1}}) &= \mathbb{E}\left[ \mathbf{1}(\tau(B, Y)<\tau(B, -X))| X_{1}, Y_{1} \right] \\
         &= \frac{X_{1}}{X_{1}+Y_{1}}
    \end{array}
    \]
    using the well-known Gambler's ruin identity and the independence of $ B$ from $ (X_{n}, Y_{n})_{n\in \N}$. Similarly, we also obtain $ \mathbb{E}\left[ T_{1} |X_{1, Y_{1}}\right]= X_{1}\cdot Y_{1}$.\\ 

    Now we determine the law of $ B_{T_{1}}$. Fix $ A\in \mathcal{B}([0,\infty))$ and compute
    \[
    \begin{array}{ll}
	    \PP(B_{T_{1}}\in A)&= \displaystyle\int_{A} \displaystyle\int^{\infty}_{0} \frac{x}{x+y}C(x+y) \mu_{-}( \diff x) \mu_{+} (  \diff y)\\ 
			       &= \mu_{+}(A) = \mu(A). 
    \end{array}
    \]
    Similarly, we have for $ A\in \mathcal{B}((-\infty,0))$ $\PP(B_{T_{1}}\in A)=\mu(A) $ and 
    \[
    \mathbb{E}\left[ T_{1} \right]= \displaystyle\int^{\infty}_{0} \displaystyle\int^{\infty}_{0}x\cdot y\cdot C(x+y) \mu_{-}( \diff d) \mu_{+}(   \diff y)  = \displaystyle\int^{\infty}_{0}x^{2}\mu_{-}( \diff x) +    \displaystyle\int^{\infty}_{0}y^{2}\mu_{+}( \diff y) = \sigma^{2}. 
    \]
    This tease with the case $ n=1$, for the general case one proceeds inductively using the strong Markov property, that is $ (B_{t+T_{n}}-B_{T_{n}})_{t\geq 0}\perp \F^{B}_{T_{n}}$ and essentially reduce the argument to what we have already done.
\end{proof}

We now return to Theorem \ref{thm: donsker}.

\begin{proof}{(Theorem \ref{thm: donsker})}
    Without loss of generality, let $ \sigma^{2}=1$ (by scaling). Now, let $B$ be a Brownian motion and a sequence of stopping times $ (T_{n})_{n\in \N}$ as in Skorokhod's embedding theorem, on a possibly enlarged probability space such that 
    \[
	    (B_{T_{n}})_{n\in \N} \stackrel{d}{=} (S_{n})_{n\in \N}.
    \]
Now, define $ B^{(N)}_{t}= \sqrt{N}B_{\frac{t}{N}}$ a standard Brownian motion by scale invariance. Let $ (T^{(N)}_{n})_{n\N}$ be stopping times corresponding to $ B^{(N)}$ (again on a possibly enlarged probability space). Set $ S^{(N)}_{n} = B^{(N)}_{T^{(N)}_{n}}$ for all $ n\in \N$. Let $ S^{(N)}_{t}$ be the linear interpolation of $ (S^{(N)}_{n})_{n\in \N}$. Observe that we have 
    \[
	    \left( (S^{(N)}_{t})_{t\geq 0}, (T^{(N)}_{n})_{n\in \N} \right) \stackrel{d}{=} \left( (S_{t})_{t\geq 0}, (T_{n})_{n\in \N} \right).
    \]
Now, we need to show 
    \[
	    \mathbb{E}\left[ F \left( (S^{(N)}_{t})_{t\leq 1} \right) \right] \stackrel{N\to \infty}{\longrightarrow} \mathbb{E}\left[ F \left( (B_{t})_{t\leq 1} \right) \right] 
    \]
    for all continuous and bounded functionals $ F: \mathcal{C}([0,\infty)\to \R$. It suffices to show that $ S^{(N)}$ converges uniformly in probability to $ B$, that is 
    \[
	    \PP \left( \displaystyle\sup_{0\leq \leq 1} |\tilde{S}^{(N)}_{t}-B_{t}|>\epsilon \right) \stackrel{N\to \infty}{\longrightarrow} 0 
    \]
    for all $ \epsilon>0$, by Dominated convergence.\\ 

    Now, for $n\leq N $, 
    \[
	    \tilde{S}^{(N)}_{n/N} = \frac{S^{(N)}}{\sqrt{N}} = \frac{B^{(N)}_{T^{(N)}}}{\sqrt{N}} = \frac{\sqrt{N}B^{(N)}_{T_{n}/N}}{\sqrt{N}} = B_{\tilde{T}^{(N)}_{n}}\,
    \]
    where $ \tilde{T}^{(N)}_{n} = \frac{T^{(N)}_{n}}{N}$. By the Strong law of large numbers, we have that $ \frac{T_{n}}{n}\stackrel{n\to \infty}{\longrightarrow} 1$ a.s. (from Skorokhod embedding, the $ T_{n}$ are a random walk with independent and identically distributed steps). 
    \parbox[b]{0.4\textwidth}{Thus, 
    \[
    \frac{1}{N}\cdot \displaystyle\sup_{n\in \N}|T_{n}-n|\stackrel{n\to \infty}{\longrightarrow}0, \quad \text{ a.s.}
    \]
    Since $ \tilde{S}^{(N)}_{n/N}= B_{\tilde{T}^{(N)}_{n}}$ for all $ n\leq N-1$, we claim that for all $ \frac{n}{N}\leq t\leq \frac{n+1}{N}$, there exists $ \tilde{T}^{(N)}_{n}\leq u\leq \tilde{T}^{(N)}_{n+1}$ such that $ \tilde{S}^{(N)}_{t} = B_{u}$. This follows from an application of the Implicit Function Theorem and using the continuity of $ B$, $ \tilde{S}$ and that $ \tilde{S}$ is piecewise linear.}\makebox[2cm]{}\parbox[b]{0.4\textwidth}{
    \includesvg[width=\linewidth]{images/donsker.svg}
    }\\ 

    So now we have

\[
\begin{array}{ll}
	A&\coloneqq \left\{ |\tilde{S}^{(N)}_{t}-B_{t}|>\epsilon \text{ for some }t\in[0,1] \right\}   \\
	 &\subseteq \left\{ |\tilde{T}^{(N)}_{n/N}-\frac{n}{N}|\geq \delta \text{ for some } n\geq N \right\}\coloneqq A_{1}\\  &
	 \bigcup \left\{ |B_{t}-B_{n}|>\epsilon \text{ for some }t\in[0,1] \text{ and } |u-t|\leq \delta +\frac{1}{N} \right\}\coloneqq A_{2}.
\end{array}
\]
Hence we have the bound $ \PP(A)\leq \PP(A_{1})+\PP(A_{2})$. Take $ N \geq 1/\delta$ and $ \delta>0$ sufficiently small so that $ \PP(A_{2})<\epsilon/2$ since Brownian motion is uniformly continuous on $ [0,1]$.
\end{proof}

\mymark{Lecture 24} 
\section{Poisson random measures}\label{sec: poisson random measures}
Recall that $ X\sim Po(\lambda)$, $ \lambda >0$ if $ \PP(X=n) = e^{-\lambda}\frac{\lambda^{n}}{n!}$ for all $ n\in \N$. If $ \lambda =0$ set $ X\equiv 0$ and if $ \lambda =\infty$, set $ X\equiv \infty$. Also recall the following basic facts about Poisson random variables. 

\begin{boxprop}[Addition property]\label{prop: prm addition}
Let $ (N_{k})_{k\in \N}$ be independent Poisson $ N_{k}\sim Po(\lambda_{k})$, $ \lambda_{k}>0$ for all $ k\in \N$. Then 
\[
\displaystyle\sum^{\infty}_{k=0}N_{k}\sim Po \left( \displaystyle\sum^{\infty}_{k=0}\lambda_{k}  \right). 
\]
\end{boxprop}

\begin{boxprop}[Splitting property]\label{prop: prm splitting property}
Let $N\sim Po(\lambda)$, $ \lambda >0$ and let $ (Y_{n})_{n\in \N}$ be an iid sequence and independent of $ N$ with $ \PP(Y_{1}=j)=p_{j}$, $ j=1\cdots, k$. Set $ N_{j}=\displaystyle\sum^{N}_{n=1} \mathbf{1}(Y_{n}=j) $. Then $ N_{1}, \cdots, N_{k}$ are independent and $ N_{j}\sim Po(\lambda p_{j})$. 
\end{boxprop}

\begin{boxdef}\label{def: poisson random measure}
	Let $ (E, \mathcal{E}, \mu)$ be a $ \sigma-$finite measure space. A Poisson random measure \underline{with intensity $\mu$} $M$ is a random map $ M: \Omega \times \mathcal{E}\to \Z\cup\{\infty\}$ such that if $ (A_{k})_{k\in \N}$ is a disjoint collection in $ \mathcal{E}$, then 
	\begin{enumerate}
		\item $M \left( \displaystyle\bigcup_{k\in \N} A_{k} \right) =\displaystyle\sum^{\infty}_{k=0}M(A_{k})(\omega), \quad \text{ for all } \omega\in \Omega $
		\item $(M(A_{k}))_{k\in \N}$ are independent random variables.
		\item For all $ k\in \N$, $ M(A_{k})
			\sim Po(\mu(A_{k}))$.
	\end{enumerate}
\end{boxdef}

Let $ E^{*} = \{\Z_{+}\cup \{\infty\}-\text{valued measures on }(E, \mathcal{E})\}$. Now for $ A\in \mathcal{E}$ define the maps
\[
\begin{array}{ll}
	X: E^{*}\times \mathcal{E}\to \Z_{+}\cup\{\infty\} \\
	X_{A}:E^{*}\to \Z_{+}\{\infty\}\\ 
	(m,A)\mapsto X_{A}(m)\coloneqq m(A).
\end{array}
\]
Furthermore, set $ \mathcal{E}^{*} = \sigma(X_{A}:A\in \mathcal{E})$. We now can state the following existence (and uniqueness) theorem for Poisson random measures. 

\begin{theorem}\label{thm: prm existence and uniqueness}
	There exists a \underline{unique} probability measure $ \mu^{*}$ on $ (E^{*}, \mathcal{E}^{*})$ such that under $ \mu^{*}$, $ X$ is a Poisson random measure of intensity $ \mu$.
\end{theorem}

\begin{proof}
	\underline{Uniqueness:} Let $ A_{1}, \cdots, A_{k}$ be disjoint in $ \mathcal{E}$ and $ n_{1}, \cdots, n_{k}\in \Z_{+}$. Set 
	\[
		A^{*} = \{m\in E^{*}: m(A_{1}) = n_{1}, \cdots, m(A_{k}) = n_{k}\}.
	\]
	Let $ \mu^{*}$ be as in the statement. Then compute 
	\[
		\mu^{*}(A^{*}) = \displaystyle \prod^{k}_{j=1}e^{-\mu(A_{j})}\frac{(\mu(A_{j}))^{n_{j}}}{n_{j}!}.
	\]
	But, $ A^{*}$ of the above form is a $ \pi-$system that generates $ \mathcal{E}^{*}$, so $ \mu^{*}$ is uniquely determined. \\

\underline{Existence:} First assume $ \lambda = \mu(E)<\infty$. Let $ N\sim Po(\lambda)$ and $ (Y_{n})_{n\in \N}$ be an iid sequence independent of $ N$ with law $ \mu/\mu(E)$. Set 
\[
	M(A) =\displaystyle\sum^{N}_{n=1} \mathbf{1}(Y_{n}\in A) , \quad A\in \mathcal{E}^{*}.
\]

Let $ A_{1},\cdots,  A_{k}$ be disjoint in $ \mathcal{E}$. Need to show that $ M(A_{i})_{i\leq k}$ are independent $ \sim Po(\mu(A_{i}))$ random variables. Consider $ X_{n}=j$ whenever $ Y_{n}\in A_{j}$. The $ (X_{n})_{n\leq \N}$ are iid and $ M(A_{j}) =\displaystyle\sum^{N}_{n=1} \mathbf{1}(X_{n}=j) $. By the splitting property \ref{prop: prm splitting property}, we get that $ M(A_{1}, \cdots, M(A_{k})$ are independent and $ M(A_{j})\sim Po \left( \mu(E) \cdot \frac{\mu(A_{j})}{\mu(E)} \right)$.\\ 

If $\mu(E) = \infty$, let $ (E_{k})_{k\in \N}$ be a partition of $ E$ into sets with $ \mu(E_{k})<\infty$ for all $ k\in \N$. Then on some probability space we can construct independent Poisson random measures $ M_{k}$ with intensity $ \mu |_{E_{k}}$ (on some suitable product space). Then for $ A\in \mathcal{E}$, set 
\[
	M(A) =\displaystyle\sum^{\infty}_{k=0}\overbrace{M_{k}(A\cap E_{k})}^{\sim Po(\mu(A\cap E_{k})}. 
\]
By the addition property \ref{prop: prm addition}, $ M(A) \sim Po \left(\displaystyle\sum^{\infty}_{k=0} \mu(A\cap E_{k}) = \mu(A)  \right)$. Independence follows since the $(M_{k})_{k\in \N}$ are PRM. \\ 

We have now constructed Poisson random measures on some probability space $ (\Omega, \F, \PP)$. Now simply observe that $ \mu^{*}=\PP_{M}$ (the pushforward under of $ \PP$ under $ M$) is the probability measure on $ (E^{*}, \mathcal{E}^{*})$.

\end{proof}

\begin{boxprop}\label{prop: prm conditioning}
Let $ M$ be a Poisson random measure with intensity $ \mu$. Let $ A\in \mathcal{E}$ be such that $ \mu(A)<\infty$. Then $ M(A)\sim Po(\mu(A))$ and conditional on $ M(A) = k$, then we can express $ M  =\displaystyle\sum^{k}_{i=1} \delta_{X_{i}}$
, where $ (X_{1}, \cdots, X_{k})$ are independent and identically distributed, with law $ \frac{\mu(\cdot \cap A)}{\mu(A)}$. Moreover, is $ A\cap B = \emptyset$, $ \mu|_{A}$ is independent of $ \mu|_{B}$.
\end{boxprop}
	

We leave the following as an exercise: let $ E = \R_{+}$ ,$ \theta >0$, $ \mu = \theta\cdot \mathbf{1}(t\geq 0 ) \diff  t$. Let $ M$ be a PRM($ \mu$), let $ T_{0}=0$, $ (T_{n}-T_{n-1})_{n\geq 1}$ be iid $ \sim Exp(\theta)$. Set 
\[
N_{t} =\displaystyle\sum^{\infty}_{n=1} \mathbf{1}(T_{n}\leq t). 
\]
Then, $ (N_{t}, t\geq 0) =\stackrel{d}{=} (M([0,t]), t\geq 0)$.

\begin{theorem}\label{thm: integral wrt poisson random integral}
Let $ M$ be a Poisson random measure with intensity $ \mu$. Let $ f\in \mathcal{L}^{1}(\mu) $ and define $ M(f) = \displaystyle\int f(y) M(\diff y)  $. Then $ M(f)\in \mathcal{L}^{1}(\mu) $ and 
\[
 \mathbb{E}\left[ M(f) \right] = \displaystyle\int f(y) M (\diff y).   
\]
Fix $ f:E\to \R_{+}$ measurable. Then for all $ u>0$, 
\[
	\mathbb{E}\left[ e^{-uM(f)} \right] = \exp \left( \displaystyle\int_{E}(e^{-uf(y)}-1) \mu(\diff y)\tag{\text{Campbell's formula}}   \right)
\]

\end{theorem}

\begin{proof}
    The first part follows from a standard approximation by simple functions argument and Dominated Convergence. Let $ (E_{n})_{n\in \N}\subseteq  \mathcal{E}^{*}$ be such that $ \mu(E_{n})<\infty$. Have 
    \[
    \begin{array}{ll}
        \mathbb{E}\left[ e^{-yM(f\cdot \mathbf{1}(E_{n})} \right] &=\displaystyle\sum^{\infty}_{k=0} \mathbb{E}\left[  e^{-uM(f\cdot \mathbf{1}(E_{n})}| M(E_{n})=k \right]\\
         &\cdot e^{-\mu(E_{n})}\frac{(\mu(E_{n}))^{k}}{k!} .
    \end{array}
    \]
    Now, given $ M(E_{n}) = k$, $ M =\displaystyle\sum^{k}_{i=1}\delta_{X_{i}} $ with $ (X_{1}, \cdots, X_{k})$ independent 
    \[ \sim M|_{E_{n}} =\displaystyle\sum^{\infty}_{k=0} \left( \displaystyle\int_{E_{n}} e^{-uf(x)} \frac{\mu( \diff  x)}{\mu(E_{n}})    \right). \]
\end{proof}
Since $ M(f\cdot \mathbf{1}(E_{n}))$ are independent and conclude with monotone convergence.

\end{document}

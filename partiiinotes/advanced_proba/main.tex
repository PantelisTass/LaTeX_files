\documentclass{article}
\input{preamble}  % Include the preamble from an external file
%\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}\addtocounter{page}{-1}}
\fancyhead{}
\fancyhead[L]{\text{Advanced Probability}}
\fancyhead[R]{\text{Pantelis Tassopoulos}}

\title{\Huge Part III Advanced Probability \\ 
\huge Based on lectures by P. Sousi}
\author{\Large Notes taken by Pantelis Tassopoulos}
\date{\Large Michaelmas 2023}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage 

\section{Conditional Expectation}
\mymark{Lecture 1}\subsection{Basic definitions}
Let $ (\Omega, \F, \PP) $ be a probability space. Remember the following definitions 
\begin{boxdef}[Sigma algebra]\label{def: sigma algebra}
	$ \F $ is a sigma algebra if and only if: ($ \F\in \mathcal{P}{\Omega} $)
	\begin{enumerate}
		\item $ \Omega \in \F $
		\item $ A\in \F \implies A^c\in \F$
		\item $ (A_n)_{n\in \N}\subseteq \F \implies \displaystyle\bigcup_{n\in\N}A_n\in\F $
	\end{enumerate}
\end{boxdef}
\begin{boxdef}[Probability measure]\label{def: prob measure}
	$ \PP $ is a probability measure if
	\begin{enumerate}
		\item $ \PP:\F\to [0,1] $ (i.e. a set function)
		\item $ \PP(\Omega) = 1 $, and $ \PP(\emptyset) = 0 $
		\item $ (A_n)_{n\in \N} $ pairwise disjoint $ \implies \PP\left(\displaystyle\bigcup_{n\in\N}A_n\right) =\displaystyle\sum^{\infty}_{n=1}\PP(A_n)$.
	\end{enumerate}
	
\end{boxdef} 
\begin{boxdef}[Random Variable]\label{def: rv}
	$ X:\Omega\to \R $ is a \underline{random variable} if for all $ B $ open in $ \R $, $ X^{-1}(B)\in \F $.
\end{boxdef}
\begin{remark}
	Observe that the sigma algebra $ \mathcal{G}=\{B\subseteq\R: X(B)\in\F\}\supseteq \mathcal{O} \implies \mathcal{G}\supseteq \mathcal{B}(\R) $, the former being the collection of open sets in $ \R $ and the latter the Borel sigma algebra on $ \R $ with the usual topology, namely, $ \sigma(\mathcal{O})$ (see below for the notation).
\end{remark}

Let $ \mathcal{A} $ be a collection of subsets of $ \Omega $. We define 
\[\begin{array}{ll}
	\sigma(\mathcal{A}) &= \text{smallest sigma algebra containing $ \mathcal{A} $} \\
     &=\displaystyle \bigcap \{\mathcal{T}:\mathcal{T} \text{ sigma algebra containing }\mathcal{A}\}.
\end{array}
\]

\begin{boxdef}[Borel sigma algebra on $ \R $]\label{def: borel sigma alg}
	Let $ \mathcal{O} = \{\text{open sets} \R\} $. Then, the Borel sigma algebra $ \mathcal{B}(\R)( \coloneq \mathcal{B} ) $ is defined as above, namely, 
	\[\mathcal{B}(\R)\coloneq \sigma(\mathcal{O}).\]
\end{boxdef}

Let $ (X_i)_{i\in I} $ be a family of random variables, then $ \sigma(X_{i}:i\in I) =$ the smallest sigma algebra that makes them all measurable. We also have the characterisation 
$ \sigma(X_{i}: i\in I) = \sigma(\{\underbrace{\{\omega\in \Omega: X_{i}(\omega)\in B\}}_{X^{-1}_{i}(B)}, i\in I, B\in \mathcal{B}(\R)\})$.

\subsection{Expectation}

Note we use the following for the indicator function on some event $ A $
\[
    \mathbf{1}(A)(x) = \mathbf{1}(x\in A) 
     \coloneqq \left. \begin{array}{@{}l@{}}
    1, \quad x\in A \\
    0, \quad x\notin A
     \end{array}\right\rbrace, \quad A\in \F.
\]


We now begin the construction of the expectation of generic random variables.\\

\underline{Positive simple random variables:} $X = \displaystyle\sum^{
n}_{i=1}\mathbf{1}(A_{i}), c_{i}\geq 0, A_{i}\in\F.  $.
\[
	\mathbb{E}[X]\coloneqq\displaystyle\sum^{n}_{i=1}c_{i}\PP(A_{i}). 
\]


\underline{Non-negative random variables:} $ (X\geq 0). $
We proceed by approximation. Namely, let $ X_{n}(\omega)\coloneqq 2^{-n}\lfloor 2^{-n}\cdot X(\omega)\rfloor \land n \uparrow X(\omega) , n\to \infty$. Now, by monotone convergence, 
\[
	\mathbb{E}[X]\coloneqq \uparrow \displaystyle\lim_{n\to\infty}\mathbb{E}[X_{n}]=\displaystyle\sup\mathbb{E}[X].
\]

\underline{General random variables:} Have the decomposition $ X = X^{+}-X^{-} $, where $ X^{+} = X\lor 0$, $X^{-}=-X\land 0 $. If one of $ \mathbb{E}[X^{+}], \mathbb{E}[X^{-}]  <\infty $ then set 
\[
	\mathbb{E}[X]\coloneqq \mathbb{E}[X^{+}]-\mathbb{E}[X^{-}].  
\]

\begin{boxdef}\label{def: integrable rv}
	$ X $ is called \underline{integrable} if $ \mathbb{E}[|X|]<\infty $.
\end{boxdef}

\begin{boxdef}\label{def: cond prob event}
Let $ B\in \F $ with $ \PP(B)>0 $. Then for all $ A\in \F $, set 
\[
\PP(A|B)\coloneqq \frac{\PP(A\cap B)}{\PP(B)}
\] 

\end{boxdef}


Now for an integer-valued random variable $ X $, we set:
\[
	\mathbb{E}[X|B]\coloneqq \frac{\mathbb{E}[X\cdot \mathbf{1}_{B}]}{\PP(B)}
\]


\subsection{Conditional expectation with respect to countably generated sigma algebras}

\mymark{Lecture 2}We now extend the definition of the conditional expectation for a \underline{countably generated sigma algebra}. Let $ (\Omega, \F, \PP) $ be a probability space. We call the sigma algebra $\mathcal{G} $ coutnably generated if there exists a colection $ (B_{n})_{n\in \N} $ of pairwise disjoint events such that $\displaystyle\bigcup_{n\in I}B_{n} = \Omega$ with ($ I $ countable) and $\mathcal{ G} = \sigma(B_{i}:i\in I)$.\\ 

Let $X$ be an integrable random variable. We want to define $\mathbb{E}[X|\mathcal{G}]$.\\ 

Define $X'(\omega) = \mathbb{E}[X|B_{i}]$, whenever $w\in B_{i}$, i.e. 
\[
	X' =\displaystyle\sum_{i\in I}\mathbf{1}(B_{i})\cdot\mathbb{E}[X|B_{i}]. 
\]

We make the convention that $\mathbb{E}[X|B_{i}] = 0$ if $\PP(B_{i}) = 0$. It is easy to check that $X'$ is $\mathcal{G}-$measurable. We also have that 
\[
\mathcal{G}  = \left\{\displaystyle\bigcup_{j\in } B_{j}: J\subseteq I \right\}
\]
and $X'$ satisfies for all $ G\in\mathcal{G}$:$\mathbb{E}[X\cdot\mathbf{1}_{G}]=\mathbb{E}[X'\cdot\mathbf{1}_{G}] $ and 
\[\begin{array}{ll}
	\mathbb{E}[|X'|] &\leq\mathbb{E} \left[\displaystyle\sum_{i\in I}|\mathbb{E}[X|B_{i}]\mathbf{1}(B_{i})  \right] \\
			 &=\displaystyle\sum_{i \in I}\PP(B_{i})\cdot \left|\mathbb{E}[X|B_{i}] \right|\\ 
			 &\leq\displaystyle\sum_{i\in I}\PP(B_{i})\cdot \underbrace{\mathbb{E}[X\cdot\mathbf{1}(B_{i})]}_{\PP(B_{i})}\\ 
			 &=\mathbb{E}[|X|]<\infty.
\end{array}
\]

\subsection{General case}

We say $ A\in \F$ happens \underline{a.s.} if $ \PP(A) = 1$. \underline{Recall} (from measure theory and basic functional analysis): 
\begin{theorem}[Monotone Convergence Theorem (MCT)]\label{thm: MCT}
	Let $(X_{n})_{n\in \N}$ be such that $ X_{n}\geq 0, X$ be random variables such that $ X_{n}\uparrow X$ as $ n\to \infty$. Then, $\mathbb{E}[X_{n}]\uparrow\mathbb{E}[X]$ as $ n\to \infty$.
\end{theorem}
 
\begin{theorem}[Dominanted Convergenec Theorem (DCT)]\label{thm: DCT}
	Let $ (X_{n})_{n\in \N}$ be random variables such that $ X_{n}\to X$ a.s. as $ n\to \infty$ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$, where $ Y$ is integrable, then $\mathbb{E}[X_{n}]\to\mathbb{E}[X]$, as $ n\to \infty$.
\end{theorem}

Let $ 1\leq p <\infty$ and $ f $ a measurable function, then set $ \norm{f}_{p}\coloneqq \left(\mathbb{E}[\norm{f}^{p}]\right)^{\frac{1}{p}}$. If $ p =\infty$, then set $ \norm{f}_{\infty}\coloneqq \displaystyle \inf \{\lambda: |f|\leq \lambda \text{ a.s.}\}$. Recall for all $ p$, the Lebesgue spaces, $\mathcal{L}^{p}(\Omega, \F, \PP)=\{f: \norm{f}_{p}<\infty\}$.

\begin{theorem}\label{thm: orthog proj hilbert}
	$ \mathcal{L}^{2}(\Omega, \F, \PP) $ is a Hilbert space, with inner product $  \bracket{u}{v}_{2}=\mathbb{E}[u\cdot v]$. Furthermore, for any closed subspace $\mathcal{H}$, if $ f\in\mathcal{L}^{2}$, there exists a unique $ g\in\mathcal{H}$ s.t. $ \norm{f-g}_{\mathcal{L}^{2}}=\displaystyle\inf_{h\in\mathcal{H}}\norm{f-h}_{\mathcal{L}^{2}}$ and $ \bracket{f-g}{h}=0$, for all $ h\in\mathcal{H}$. We say that $ g$ is the \underline{orthogonal projection} of $ f$ in $\mathcal{H}$.
\end{theorem}


We now construct the conditional expectation in the general case, for any integrably random variable with respect to an arbitrary sigma algebras.

\begin{theorem}[Conditional Expectation]\label{thm: cond exp}
Let $ (\Omega, \F, \PP)$ be a probability space, $\mathcal{G}\subseteq \F$ a sub-sigma algebra, $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$. Then there exists an integrable random variable $ Y$ satisfying:
\begin{enumerate}
	\item $ Y$ is $\mathcal{G}-$measurable
	\item for all $ G\in\mathcal{G},\mathbb{E}[X\cdot\mathbf{1}(G)]= \mathbb{E}[Y\cdot\mathbf{1}(G)]$.
\end{enumerate}
Moreover, $ Y$ unique in the sense that if $ Y'$ also satisfies the above $ 1),2)$, then $ Y = Y'$ a.s.. We call $ Y$ a version of the conditional expectation of $ X$ given $ G$. We write $ Y =\mathbb{E}[X\mathcal{G}]$ a.s. If $\mathcal{G} = \sigma(Z)$, where $ Z$ is a random variable, then we write $\mathbb{E}[Z] =\mathbb{E}[X|\mathcal{G}]$.

\end{theorem}

\begin{remark}
	$ 2)$ could be replaced by $\mathbb{E}[X\cdot Z] =\mathbb{E}[Y\cdot Z]$ for all $ Z$ bounded $\mathcal{G}-$measurable random variables. 
\end{remark}

We now state and prove the main theorem of this section:

\begin{proof}{(Theorem \ref{thm: cond exp})}
	\underline{Uniqueness:} Let $ Y, Y'$ satisfy $ 1), 2)$. Let $ A = \{Y > Y'\}\in\mathcal{G}$. Then $ 2) $  
	\[\begin{array}{ll}
	&\implies \mathbb{E}[Y\cdot\mathbf{1}(A)] =\mathbb{E}[Y'\cdot\mathbf{1}(A)]=\mathbb{E}[X\cdot\mathbf{1}(A)] \\
	&\implies\mathbb{E}[(Y-Y')\cdot\mathbf{1}(A)] = 0\\ 
	&\implies \PP(A) = \PP(Y>Y') = 0\\ 
	&\implies Y\leq Y' \text{ a.s.}.
	\end{array}
	\]
	We similarly obtain $ Y\geq Y'$ a.s., hence we deduce that $ Y = Y'$ a.s.

	\underline{Existence:} three steps. 
	\begin{enumerate}
		\item Assume that $ X \in\mathcal{L}^{2}(\Omega, \F, \PP)$. Observe that $\mathcal{L}^{2}(\Omega,\mathcal{G},\PP)$ is a closed subspace of $\mathcal{L}^{2}(\Omega, \F, \PP)$. Hence, Theorem \ref{thm: orthog proj hilbert}, we have the decomposition  $\mathcal{L}^{2}(\Omega, \F, \PP) =\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)\oplus\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.
Then, we have the corresponding decomposition $ X = Y+Z$, where $ Y\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)$ and $ Z\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP) $ respectively. Define $\mathbb{E}[X\mathcal{G}]\coloneqq Y$, $ Y$ is $\mathcal{G}-$measurable and for all $ A\in\mathcal{G}$, $\mathbb{E}[X\cdot\mathbf{1}(A)]\mathbb{E}[Y\cdot\mathbf{1}(A)]=\mathbb{E}[Z\cdot\mathbf{1}(A)]$ since $ Z\in  \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.

\underline{Claim:} If $ X \geq 0$, a.s. then $ Y \geq 0$ a.s.
Indeed, let $ A = \{Y< 0\}\in\mathcal{G}$. Then observe that $ 0\leq\mathbb{E}[X\cdot\mathbf{1}(A)]=\mathbb{E}[Y\cdot\mathbf{1}(A)]\leq 0$. Hence $\mathbb{E}[Y\cdot\mathbf{1}(A)]=0$ and so $ \PP(A) = 0$, gibing $ Y = 0$ a.s.

\item Assume $ X\geq 0 $.\\ 
	Define $ X_{n} = X\land n\leq n $, meaning $ X_{n}$ is bounded for all $ n\in \N$. So $ X_{n}\in\mathcal{L}^{2}(\Omega, \F, \PP)$. Let $ Y_{n} =\mathbb{E}[X_{n}]$ a.s.. $ (X_{n})_{n\in \N}$ is an increasing sequence. By the claim abose, so is $ (Y_{n})_{n\in \N}$ a.s.\\
	Define $ Y = \displaystyle \limsup_{n}Y_{n}$ meaning $ Y$ is $\mathcal{G}-$measurable and $ Y = \uparrow \displaystyle \lim_{n\to \infty}Y_{n} $ a.s. Now, we have that for all $ A\in\mathcal{G}$, $\mathbb{E}[X_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]$. Thus, by theorem \ref{thm: MCT} (MCT), $\mathbb{E}[X\cdot\mathbf{1}(A)]= \displaystyle \lim_{n\to \infty} \mathbb{E}[X_{n}\cdot\mathbf{1}(A)] = \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y\cdot\mathbf{1}(A)]$.

\item $ X$ general in $\mathcal{L}^{1}$.\\ 
	Decompose as before $ X = X^{+}-X^{-}$. Define, $\mathbb{E}[X\mathcal{G}] =\mathbb{E}[X^{+}|\mathcal{G}]-\mathbb{E}[X^{-}|\mathcal{G}]$.
\end{enumerate}

\end{proof}

\mymark{Lecture 3}
\begin{remark}
From the second step of the proof of Theorem \ref{thm: cond exp} we see that we can define $\mathbb{E}[X|\mathcal{G}]$ for all $ X\geq 0$, not necessarily integrable. It satisfies all conditions $ 1) , 2)$ except for the integrability one.
\end{remark}

\begin{boxdef}\label{def: independence of sigma algebras}
$\underbrace{\mathcal{G}_{1},\mathcal{G}_{2}, \dots}_{\text{sigma algebras}} \subset \F$. We call them \underline{independent} if whenever $ G_{i}\in \mathcal{G}_{i}$ and $ i_{1}<\dots i_{k}$ for some $ k \in \N$, then $ \PP(G_{i_{1}}\cap \dots\cap G_{i_{k}}) = \displaystyle \prod^{k}_{j=1}\PP(G_{i_{j}})$.\\ 

Moreover, let $ X$ be a random variable and $\mathcal{G}$ a sigma algebra, then they are said to be int if $ \sigma(X)$ is independent of $\mathcal{G}$.
\end{boxdef}

\underline{Properties of conditional expectations:}
Fix $ X,y \in\mathcal{L}^{1}$, $ G\in \F$.
\begin{enumerate}
	\item $\mathbb{E}[\mathbb{E}[X\mathcal{G}]]=\mathbb{E}[X]$ (take $ A  = \Omega$)
	\item If $ X$ is $\mathcal{G}-$measurable, then $\mathbb{E}[X\mathcal{G}]=X$ a.s.
	\item If $ X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X\mathcal{G}]=\mathbb{E}[X]$
	\item If $ X\geq 0$ a.s., then $\mathbb{E}[X\mathcal{G}]\geq 0 $ a.s. 
	\item For $ \alpha, \beta \in \R$ $\mathbb{E}[\alpha X + \beta Y |\mathcal{G}] = \alpha\mathbb{E}[X]+\beta\mathbb{E}[Y]$
	\item $\mathbb{E}[X|\mathcal{G}]|\leq\mathbb{E}[|X| |\mathcal{G}]$ a.s. 
\end{enumerate}

Below we provid:we expensions of useful measure theoretic results for the expectation to their corresponding conditional counetparts. First recall:
\begin{boxlemma}[Fatou's Lemma]\label{lemma: Fatou}
Let $  X_{n}\geq 0$ for all $ n\in \N$. Then 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s}
\]


\end{boxlemma}


\begin{theorem}[Jensen's Inequality]\label{thm: jensen}
If $ X$ is integrable and $ \phi: \R \to \R$ is a convex function, then 
\[
	\phi(\mathbb{E}[X])\leq\mathbb{E}[\phi(X)]\quad   \text{ a.s.}
\]
\end{theorem}
Now the results themselves:

\begin{theorem}[Conditional Monotone Convergence theorem (MCT)]\label{thm: cond MCT}
Let $\mathcal{G}\subset \F$ be sigma algebras, $ X_{n}\geq 0$ a.a. and $ X_{n}\uparrow X$, as $ n\to \infty$ a.s. Then 
\[
	\mathbb{E}[X_{n}|\mathcal{G}]\uparrow\mathbb{E}[X|\mathcal{G}] \quad \text{ a.s.}
\]

\end{theorem}

\begin{proof}{Theorem \ref{thm: cond MCT}}
	Set $ Y_{n} =\mathbb{E}[X_{n}\mathcal{G}]$ a.s. Observe that $ Y_{n}$ is a.s. increasing. Set $ Y = \displaystyle\limsup_{n}Y_{n}$. $ Y_{n}$ is $\mathcal{G}-$measurable, hence, so is $ Y$ (as a $ \displaystyle \limsup $ of $\mathcal{G}-$measurable random variables) is also $\mathcal{G}-$measurable. Also, $ Y = \displaystyle \lim_{n\to \infty}Y_{n} $ a.s.\\ 

	\underline{Need to show:} $\mathbb{E}[Y\cdot\mathbf{1}(A)]\mathbb{E}[X\cdot\mathbf{1}(A)]$ for all $ A\in\mathcal{G}$.	Indeed,
	\[\begin{array}{ll}
	    \\
	    \mathbb{E}[Y\cdot\mathbf{1}(A)] &=\mathbb{E}[ \displaystyle \lim_{n\to \infty }Y_{n}\cdot\mathbf{1}(A) ] \stackrel{\text{MCT}}{=} \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]\\
																	      &=\displaystyle \lim_{n\to \infty }\mathbb{E}[X_{n}\cdot\mathbf{1}(A)]  =\mathbb{E}[X\cdot\mathbf{1}(A)].
	\end{array}
	\]
	
\end{proof}

\begin{proof}{Theorem \ref{lemma: Fatou}}
$ \displaystyle \liminf_{n}X_{n} = \displaystyle \lim_{n\to \infty }\left( \displaystyle\inf_{k\geq n}X_{k} \right) $, the limit of an increasing sequence. By Theorem \ref{thm: MCT}, we have 
\[
	\displaystyle \lim_{n\to \infty}\mathbb{E}[\displaystyle\inf_{k\geq n}X_{n}|\mathcal{G}] =\mathbb{E}[\displaystyle \liminf_{n}X_{n}|\mathcal{G}]
\]
and 
\[
	\mathbb{E}[\displaystyle \inf_{k\geq n}X_{k}|\mathcal{G}]\stackrel{\text{a.s.}}{\leq } \displaystyle\inf_{k\geq n}\mathbb{E}[X_{k}|\mathcal{G}]\footnote{\text{can take the infinum due to countability that preserves a.s.}}
\]
which gives the result 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s.}
\]

		
\end{proof}

\begin{theorem}[Conditional Dominated Convergence Theorem]\label{thm: cond DCT}
	SUppose $ X_{n}\to X$ a.s. $ n\to \infty $ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$ with $ Y$ integrable. Then $\mathbb{E}[X_{n}\mathcal{G}]\to\mathbb{E}[X\mathcal{G}]$ a.s. as $n\to \infty.$
\end{theorem}

\begin{proof}
	From $ -Y\leq X_{n}\leq Y$, we have $ X_{n}+Y\geq 0$ for all $ n\in \N$ and $ Y-X_{n}\geq 0 $a.s. By Theorem \ref{lemma: Fatou},
\[
		\begin{array}{ll}
		\mathbb{E}[X+Y\mathcal{G}] &=\mathbb{E}[\displaystyle\liminf_{n}(X_{n}+Y)|\mathcal{G}] \\
					   &\leq \displaystyle\liminf_{n}\mathbb{E}[X_{n}+Y|\mathcal{G}] = \displaystyle\liminf_{n}\mathbb{E}[X_{n}\mathcal{G}]+\mathbb{E}[X]
	\end{array}
\]
Thus, 
\[\begin{array}{ll}
	\mathbb{E}[|X-Y| |\mathcal{G}]&=\mathbb{E}[Y-\displaystyle\liminf_{n}X_{n}|\mathcal{G}]  \\
				     &\leq\mathbb{E}[Y]+\displaystyle\liminf_{n}  \mathbb{E}[X_{n}|\mathcal{G}] 
\end{array}
\]
Hence, 
\[
	\displaystyle\limsup_{n} \mathbb{E}[X_{n}|\mathcal{G}] \leq\mathbb{E}[X|\mathcal{G}]
\]
and 
\[
	\displaystyle\liminf_{n} \mathbb{E}[X_{n}|\mathcal{G}] \geq\mathbb{E}[X|\mathcal{G}]
\]
a.s., concluding the proof.

\end{proof}

\begin{theorem}[Conditional Jensen]\label{thm: cond jensen}
Let $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$, $ \phi:\R\to \R$ be a convex function s.t. $ \phi(X)$ is integrable or $ \phi(X)\geq 0 $
\[
	\phi(\mathbb{E}[X|\mathcal{G}])\leq\mathbb{E}[\phi(X)|\mathcal{G}] \quad \text{a.s.}
\]
\end{theorem}

\begin{proof}
	\underline{Claim:} (true for any convex function, no proof given) $ \phi(x)=\displaystyle\sup_{i\in\N}(a_{i}x+b_{i})$, $ a_{i}b_{i}\in\R$. 
Thus, 
\[
	\mathbb{E}[\phi(X)|\mathcal{G}]\geq a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i} \quad \text{ for all } i\in \N.
\]
Taking the supremum gives \footnote{can take the supremum due to countability which again preserves a.s.}
\[
\begin{array}{ll}
      
	\mathbb{E}[\phi(X)|\mathcal{G}]&\geq \displaystyle\sup_{i\in \N} \left(  a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i}  \right)\\ 
				       &= \phi(\mathbb{E}[X|\mathcal{G}]) \quad \text{ a.s.}
\end{array}
\]

\end{proof}

\begin{boxcor}\label{cor: norm contraction cond exp}
	For all $ 1\leq p <\infty \norm{\mathbb{E}[X|\mathcal{G}]}_{p}\leq \norm{X}_{p}$.
\end{boxcor}

\begin{proof}
    Apply conditional Jensen.
\end{proof}

\begin{boxprop}[Tower Property]\label{prop: tower ppty}
Let $ X$ be integrable and $\mathcal{H}\subseteq\mathcal{G}$ sigma algebras. Then 
\[
	\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]=\mathbb{E}[X|\mathcal{H}] \quad \text{ a.s.}
\]

\end{boxprop}

\begin{proof}
	\begin{enumerate}[(a)]
		\item $\mathbb{E}[X|\mathcal{H}] $ is $\mathcal{H}-$measurable.
		\item For all $ A\in\mathcal{H}$ NTS: 
			\[
				\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot\mathbf{1}(A)] =\mathbb{E}[\mathbb{E}[X|\mathcal{H}]\cdot\mathbf{1}(A)]
			\]
			Indeed, both terms above are equal to $\mathbb{E}[X\cdot\mathbf{1}(A)]$ since $ A\in\mathcal{G}\subseteq\mathcal{H}$.
			
    \end{enumerate}
    
\end{proof}

\begin{boxprop}\label{prop: meas factorisation cond exp}
Let $ X\in\mathcal{L}^{1}$, $\mathcal{G}\subseteq \F$, $ Y$ bounded $\mathcal{G}-$measurable. Then 
\[
	\mathbb{E}[X\cdot Y|\mathcal{G}] =  Y\cdot\mathbb{E}[X|\mathcal{G}].
\]

\end{boxprop}


\begin{proof}
	\begin{enumerate}[(a)]
		\item RHS is clearly $\mathcal{G}-$measurable.
		\item For all $ A\in\mathcal{G}$: 
			\[
			\begin{array}{ll}
				\mathbb{E}[X\cdot Y\cdot \mathbf{1}(A)] &=\mathbb{E}[Y\cdot\mathbb{E}[X\mathcal{G}]\cdot\mathbf{1}(A)] \\
				\mathbb{E}[X\cdot (\smash{\underbrace{Y\cdot\mathbf{1}(A)}_{\makebox[0pt]{$\mathcal{G}$-\text{meas. and bounded}}}})]&=\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot Y\cdot\mathbf{1}(A)]=RHS.
			\end{array}
			\]
			
    \end{enumerate}
    \vspace{1em} 
(Also observe that by a monotone class argument, we have for any integrable function $ f:\Omega \to \R$, $\mathbb{E}[X\cdot f] =\mathbb{E}[\mathbb{E}[X|\mathcal{ G}]\cdot f] $ ) 
\end{proof}


\mymark{Lecture 4}

We are building towards the Theorem
\begin{theorem}\label{thm: cond expectation sigma indep}
$ X\in \mathcal{L}^1, \mathcal{G}, \mathcal{H} \subseteq \F$. Assume $ \sigma( \mathcal{G}, \mathcal{H})\perp \mathcal{H}$, Then
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]

\end{theorem}


We begin with a definition
\begin{boxdef}\label{def: pi system}
	Let $ \mathcal{A} $ be a collection of sts. It is called a \underline{$ \pi-$system} if for all $ A,B\in \mathcal{A}$, we also have $ A\cap B\in \mathcal{A}$.
\end{boxdef}


\begin{theorem}[Uniquenes of extension]\label{thm: uniqueness meas extension}
Let $ (E, \xi)$be a measurable space and let $ \mathcal{A}$ be a $ \pi-$system generating the sigma algebra $ \xi$. Let $ \mu, \nu$ be two measures on $ (E, \xi)$ with $ \mu(E)=\nu(E)<\infty$. If $ \mu = \nu$ on $ \mathcal{A}$, then $ \mu = \nu$ on $ \xi$.
\end{theorem}

\begin{proof}{(Theorem \ref{thm: cond expectation sigma indep})}
    NTS: for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$
    \[
	    \mathbb{E}[X\cdot \mathbf{1}_{F}] =\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}_{F}]
    \]
    Now, set $  \mathcal{ A} = \{A\cap B : A\in \mathcal{ G}, B\in \mathcal{ H}\}$. It is easy to check that $  \mathcal{A}$ is a $ \pi-$system generating $ \sigma( \mathcal{G}, \mathcal{H})$. If $ F = A\cap B$ for some $ A\in \mathcal{G}$ and $ B \in \mathcal{H}$, Then 
    \[
    \begin{array}{ll}
	    \mathbb{E}[X\cdot \mathbf{1}(A\cap B)] &=\mathbb{E}[X\cdot \mathbf{1}(A)\cdot \mathbf{1}(B)] \\
						  & =\mathbb{E}[X\cdot \mathbf{1}(A)]\cdot\mathbb{E}[ \mathbf{1}(B)] \stackrel{H\perp \sigma( \mathcal{G}, \mathcal{H})}{=}\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}(A\cap B)].
    \end{array}
    \]

    Now assume $ X\geq 0$; in the general case, decompose $ X = X^{+}- X^{-}$ and apply same argument to both $ X^{\pm}$. Define the measures $ \mu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ and $ \nu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$. Observe that $ \mu(\Omega) = \nu(\Omega) =\mathbb{E}[X]<\infty$ and we have shown that $ \mu = \nu$ on $ \mathcal{A}$. Thus, $ \mu=\nu$ on $ \sigma( \mathcal{G}, \mathcal{H})$ which finally implies the result 
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]


\end{proof}


\begin{examplesblock}{Examples: }\label{examples: 1}

\begin{enumerate}
	\item 
\begin{boxdef}[Gaussian]\label{def: gaussian dist}
$ (X_{1}, X_{2}, \cdots, X_{n})\in \R^{n}$ has the Gaussian distribution if and only if for all scalars $ a_{1}, a_{2}, \cdots, a_{n}\in \R$, $ a_{1}X_{1}+\cdots a_{n}X_{n}$ has the Gaussian distrubition in $ \R$.
\end{boxdef}

A stochastic process (more on that later) $ (X_{t})_{t\geq 0}$ is a \underline{Gaussian process} if for all $ t_{1}<t_{2}<\cdots t_{n}$ the vector $ (X_{t_{1}}, X_{t_{2}}, \cdots, X_{t_{n}})$ is Gaussian.

Let $ (X,Y)$ be a Gaussian vector in $ \R^{2}$. We compute $\mathbb{E}[X|Y]$.\\ 
Let $ X' =\mathbb{E}[X|Y]$. Looking for $ f$ a Borel measurable function s.t. $ \mathbb{E}[X|Y] = f(Y)$ a.s. Let $ f(y) = ay+b$ for some $ a,b\in \R$ to be determined. Now, $ X' = aY+b$, $\mathbb{E}[X'] =\mathbb{E}[X] = a\mathbb{E}[Y]+b$ and $\mathbb{E}[X'\cdot Y] =\mathbb{E}[X\cdot Y]\implies\mathbb{E}[(X-X')\cdot Y]=0$. Thus $ \text{Cov}(X-X', Y)=0\implies \text{Cov}(X,Y) = a^{2}\text{Var}(Y)$.\\ 

\underline{Need to check:} that for all $ Z$ bounded $ \sigma(Y)-$measurable, $ \mathbb{E}[(X-X')\cdot Z] = 0$.\\ 
Indeed, observe that $ (X-X', Y)$ is a Gaussian vector and since $ \text{Cov}(X-X', Y) = 0\implies X-X'\perp Y\implies (X-X')\perp Z$.

\item Let $ (X,Y)$ be a random vector with density in $ \R^{2}$ with joint density function $ f_{X,Y}:\R^{2}\to \R$. Let $ h:\R\to \R$ be a Borel function such that  $ h(X)$ is integrable. We now compute $\mathbb{E}[h(X)| Y]$.\\ 
We have for all $ g$ bounded $ \sigma{Y}-$measurable functions.
	
\[
\begin{array}{ll}
	\displaystyle\int_{\R^{2}}h(x)g(y)f_{X,Y}(x,y) \diff x \diff y &=  \mathbb{E}[h(X)g(Y)]\\ 
								       &=\mathbb{E}[\mathbb{E}[h(X)|Y]g(Y)] =\mathbb{E}[\phi(Y)g(Y)]\\ 
								       &= \displaystyle\int_{\R^{2}}\phi(y)g(y)f_{Y(y)} \diff y  
\end{array}
\]
where $ f_{Y}(y) = \int_{\R}f_{X,Y}(x,y) \diff x  $ and $ \phi:\R\to \R$ is some Borel measurable function. Hence, 

\[
\phi(y) = \left\lbrace
\begin{array}{@{}l@{}}
    \displaystyle\int_{\R} h(x)\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \diff dx, \quad f_{Y}(y)>0   \\
    0, \quad \text{otherwise}
\end{array}\right.
\]
can be seen to work. Thus, we obtain 
\[
	\mathbb{E}[h(X)|Y] = \phi(Y) \quad \text{a.s.}
\]
	
\end{enumerate}
\end{examplesblock}


\section{Discrete Time Martingales}

\begin{boxdef}[Filtration]\label{def: filtration}
	Let $ (\Omega, \F, \PP)$ be a pobability space. A \underline{filtration} is a sequences of increasing sigma sub-algebras of $ \F$, $ (\F_{n})_{n\in \N}$, $ \F_{n}\subseteq \F_{n+1}$ for all $ n\in \N$. We call $ (\Omega, \F, (\F_{n})_{n\in\N})$ a \underline{filtered probability space}.\\ 

	Let $X =  (X_{n})_{n\in \N}$ be a sequence of random variables/a stochastic process. Then it induces $ (\F^{X}_{n})_{n\in N}$, where $ \F^{X}_{n}\coloneqq \sigma(X_{:k\leq n})$ for all $ n\in \N$: the canonical filtration associated to $ X$. We call $ X$ \underline{adapted} to a filtration $ (\F_{n})_{n\in \N}$ if $ X$is $ \F_{n}-$measurable for all $ n\in \N$.$ X$ is called \underline{integrable} if $ X_{n}$ is integrable for all $ n\in \N$.
\end{boxdef}

\begin{boxdef}[Martingale discrete time]\label{def: martingale}
Let $ (\Omega, \F, (\F_{n})_{n\in\N}, \PP)$ be a filtered probability space. Let $ X = (X_{n})_{n\in \N}$be an integrabl and adapted process. 
\begin{itemize}
	\item $ X$ is called a \underline{martingale} if $ \mathbb{E}[X_{n}| \F_{m}]=X_{m}$ a.s. for all $ n\geq m$.
	\item $ X$ is called a \underline{super-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\leq X_{m}$ a.s. for all $ n\geq m$.
        \item $ X$ is called a \underline{sub-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\geq X_{m}$ a.s. for all $ n\geq m$.
\end{itemize}

\end{boxdef}

\begin{remark}
	If $  X$ is a (super/sub)martingale with respect to $ (\F_{n})_{n\in\N}$, then it is also a martingale with respect to $ (\F^{X}_{n})_{n\in \N}$. To see this, one has to use the tower property \ref{prop: tower ppty}: $ \F^{X}_{n}\subseteq \F_{n}$ for all $ n\in\N$ implies $ \mathbb{E}[X_{n}|\F^{X}_{m}]= \mathbb{E}[ \mathbb{E}[X_{n}|\F_{m}]|\F^{X}_{m}]$ (since $ \mathbb{E}[X_{n}|\F_{m}]$ a.s.).
\end{remark}

\begin{examplesblock}{Examples: }\label{examples: 2}
\begin{enumerate}
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=0$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \xi_{1}+\cdots +\xi_{n}$ for all $ n\in\N$, $ X_{0} = 0$. $ X$ is a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=1$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \displaystyle\prod^{n}_{i=1}\xi_{i}$ for all $ n\in\N$, $ X_{0} = 1$. $ X$ is again a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.

\end{enumerate}

\end{examplesblock}

\mymark{Lecture 5}Let $ (\Omega, \F, (\F_{n})_{n\in \N}, \PP)$ be a filtered probability space.

\begin{boxdef}[Stopping time discrete time]\label{def: stopping time discrete}
	A \underline{stopping time} $ T$ is a random variable $ T:\Omega\to \Z_{+}\cup \{\infty\}$ s.t. $ \{T\leq n\}\in \F_{n}$ for all $ n\in \N$. Equivalently, if $ \{f=n\}\in \F_{n}$ for all $ n\in \N$ since 
	\[
		\{T=n\}=\underbrace{\{T\leq n\}}_{\F_{n}}\setminus \underbrace{\{T\leq n-1\}}_{\F_{n-1}\subset \F_{n}}\in \F_{n}.
	\]
and 
\[
	\{T\leq n\} =\displaystyle\bigcup^{n}_{k=1}\{T=k\}\in \F_{k}\subset \F_{n}.
\]

\end{boxdef}

\begin{examplesblock}{Examples: }\label{examples: 3}
\begin{enumerate}
	\item Constant time are trivially stopping times.
	\item Let $ X = (X_{n})_{n\in\N}$ be a stochastic process taking values in $ \R$ and $ A\in  \mathcal{B}(\R)$ ($ X$ adapted). Define 
		\[
			T_{A} = \{n\geq 0: X_{n\in A}\}.
		\]
		Then $ \{T_{A}\leq n \} =\displaystyle\bigcup^{n}_{k=0}\{X_{k\in A}\}\in \F_{n} $ for all $ n\in\N$ (with convention $ \inf \emptyset = \infty$).
	\item $ L_{A} = \sup\{n\geq 0: X_{n\in A}\}$ is \underline{NOT} a stopping time.	
\end{enumerate}
\end{examplesblock}

\underline{Properties:} $ S,T, (T_{n})_{n\in\N}$ stopping times. Then $ S\land T, S\lor T$, $ \displaystyle\inf_{n}T_{n}, \displaystyle\sup_{n}T_{n}$, $ \displaystyle\liminf_{n}T_{n}$, $ \displaystyle\limsup_{n}T_{n}$ are also stopping times.

\begin{boxdef}[Stopping time sigma algerbra]\label{def: stopping time sigma algebra}
It $ T$is a stopping time, define 
\[
	\F_{T}=\{A\in \F: A\cap \{T\leq t \}\in \F_{t}\}
\]
Let $ (X_{n})_{n\geq 0}$ be a process. Write $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ for $ \omega \in \Omega$ whenever $ T(\omega)<\infty$. Define the \underline{stopped process:} $ X^{T}_{t}\coloneqq X_{T\land t}$.
\end{boxdef}


\begin{boxprop}\label{prop: stopping time discrete}
	Let $ S$ and $ T$ be stopping times, and let $ X$ be an adapted process, then:
	\begin{enumerate}
		\item If $ S\leq T$, then $ \F_{S}\subseteq\F_{T}$.
		\item $ X_{T}\cdot$ is $ \F_{T}-$measurable.
		\item $ X^{T}$ is adapted. 
		\item If $ X$ is integrable, then the stopped process iss integrable.
	\end{enumerate}
	
\end{boxprop}
\begin{proof}
    \begin{enumerate}
	    \item Immediate from definition.
	    \item Let $ A\in \mathcal{B}(\R)$. Need to show: 
		    \[
			    \{X_{T} \mathbf{1}(T<\infty)\}\cap \{T\leq t\} \in A, \quad \text{ for all }t\geq 0.
		    \]
Indeed, we have that 
\[
	\{X_{T} \mathbf{1}(T<\infty)\} =\displaystyle\bigcup^{t}_{s=0}\underbrace{\{X_{s}\in A\}}_{\F_{s}\subseteq \F_{t}}\cap \underbrace{\{T = s\}}_{\F_{s}}\in \F_{t}.
\]

\item $ X^{T}_{t} = X_{T\land t}$, this being $ \F_{T\land t}-$measurable $ \subseteq\F_{t}-$measurable by $ 1)$, so we conclude it is $ \F_{t}-$measurable.

\item 
	\[
	\begin{array}{ll}
		\mathbb{E}[|X_{t}^{T}|] &=\mathbb{E}[|X_{T\land t}|] \\
					&=\displaystyle\sum^{t-1}_{s=0}\mathbb{E}[|X_{s}|\cdot \mathbf{1}(T = s)]+\mathbb{E}[|X_{t}|\cdot \mathbf{1}(T\geq t)]\\ 
					&\leq\displaystyle\sum^{ t}_{s=0}\mathbb{E}[|X_{s}|]\underbrace{<\infty}_{X_{t} \text{ is integrable}}.
	\end{array}
	\]
	
    \end{enumerate}
    
\end{proof}

We now state and prove a fundamental theorem in Martingale theory: 

\begin{theorem}[Optional Stopping Theorem discrete time]\label{thm: optional stopping discrete time}
Let $ (X_{n}$ be a martingale. 
\begin{enumerate}
	\item If $ T$ is a stopping time, then the stopped process $ X^{T}$ is also a martingale. In particular for all $ t\geq 0$:
		\[
			\mathbb{E}[X_{T\land t}] =\mathbb{E}[X_{0}].
		\]
	\item It $ S\leq T$ are bounded stopping times, then 
		\[
			\mathbb{E}[X_{T}|\F_{S}] = X_{T}, \quad \text{a.s.}
		\]
		and hence $ \mathbb{E}[X_{T}] \mathbb{E}[X_{S}]$.
	\item It there exists an integrable random variable $ Y$ such that $ |X_{n}\leq Y|$ for all $ n \geq 0 $ and $ T$ is finite, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
	\item If there exists $ M\geq 0$ such that $ |X_{n+1}-X_{n}|\leq M$ for all $ n\in \N$ and $ T$ is a stopping time with $ \mathbb{E}[T]<\infty$, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
\end{enumerate}

\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item NTS: for all $ t\geq 0$, $ \mathbb{E}[X_{T\land t}|\F_{t-1}]=X_{T\land t}$ a.s.
		    Indeed, 
\[
\begin{array}{ll}
	\mathbb{E}[X_{T\land t}|\F_{t-1}] &=\displaystyle\sum^{t-1}_{s =0}\mathbb{E}[X_{s}\cdot \mathbf{ 1}(T=s)|\F_{t-1}] \mathbb{E}[X--t]\cdot \mathbf{1}(T\geq t)|\F_{t-1}] \\
					  &=\displaystyle\sum^{ t-1}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t) \quad \text{ a.s.}\\ 
&=\displaystyle\sum^{ t-2}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t-1) \quad \text{ a.s.}\\ 
&= X_{T\land t-1} \quad \text{a.s.}
\end{array}
\]
\item $S\leq T\leq n, n\in \N$ fixed. Let $ A\in \F_{S}$. \underline{NTS:} $ \mathbb{E}[X_{T}\cdot \mathbf{1}(A)] = \mathbb{e}[X_{s}\cdot \mathbf{1}(A)]$. We compute
	\[
	\begin{array}{ll}
	    X_{T}-X_{S} &= (X_{T}-X_{T-1})+\cdots + (X_{S+1}-X_{S}) \\
			&=\displaystyle\sum^{ n-1}_{k=0}(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T). 
	\end{array}
	\]
	Thus, 
	\[
		\mathbb{E}[X_{T}\cdot \mathbf{1}(A)] \stackrel{ (A\in \F_{S}) }{=}\mathbb{E}[X_{S}\cdot \mathbf{1}(A)]+ \displaystyle\sum^{ n-1}_{k=0}\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)]
	\]
	Have, $ A\cap \{S\leq k\}\in \F_{k}$ and $ A\cap \{T>k\}\in F_{k}$. Thus, $ \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)$ is $ \F_{k}-$measurable. Using $ \mathbb{E}[X_{k+1}|\F_{k}]=X_{k}$ a.s. we deduce that 
	\[
	\begin{array}{ll}
		\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)]
		&= \mathbb{E}[\smash{\cancelto{0}{\mathbb{E}[(X_{k+1}-X_{k})|\F_{k}]}}\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)] \\
		&= 0
	\end{array}
\]
Thus, $ \mathbb{E}[X_{T}|\F_{S}]=X_{S}$ a.s. 

\item By the Optional Stopping Theorem applied to $ (X_{T\land n})_{n\geq 0}$, we have 
	\[
		\mathbb{E}[X_{T\land n}] =\mathbb{E}[X_{0}] \quad \text{for all } n\geq 0.
	\]
Now, $ T$ being finite a.s. implies that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ a.s. By assumption, have $ |X_{T\land n}|\leq Y$ a.s. for all $ n\in \N$ and so can apply DCT to conclude.	

\item Observe that for all $ n\geq 1$
	\[
	X_{T\land n}-X_{0} =\displaystyle\sum^{n-1}_{k=0}(X_{k}-X_{0})\cdot \mathbf{1}(T=k)+(X_{n} -X_{0})\mathbf{1}(T\geq n) 
	\]
Hence, we have the bound (using that $ |X_{k+1}-X_{k}|\leq M$ a.s. for all $ k\geq 0$)
\[
\begin{array}{ll}
    \\
|X_{T\land n}-X_{0}|&\leq M\displaystyle\sum^{n-1}_{k=0} k\mathbf{1}(T=k) + n\mathbf{1}(T\geq n)\\
			&\leq\mathbb{E}[T]<\infty \quad \text{a.s.}
\end{array}
\]
Now, $\mathbb{E}[T]<\infty$ gives $ T<\infty$ a.s. and so can deduce as before that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ and use the DCT to conclude the argument. 


 \end{enumerate}
    
\end{proof}


\begin{boxcor}\label{cor: pos supermg bound}
Let $ X$ be a positive superartingale, $ T$ a stopping time such that $ T<\infty$ a.s., then 
\[
	\mathbb{E}[X_{T}]\leq\mathbb{E}[X_{0}].
\]

\end{boxcor}

\begin{proof}
	Use Fatou \ref{lemma: Fatou}: $\mathbb{E}[\displaystyle\liminf_{t\uparrow \infty}X_{T\land t}]\leq \displaystyle\liminf_{t\uparrow \infty}\mathbb{E}[X_{T\land t}]\leq\mathbb{E}[X_{0}]$.
\end{proof}


\begin{examplesblock}{Simple random walk on $ \Z$}\label{def: SRW on ints}
	Let $ (\xi_{i})_{i\geq 0}$ be iid Bernoulli random variables with success probability $ 1/2$. Define the process $(X_{n})_{n\geq 0}$ by setting $ X_{n} = \xi_{1}+\dots+\xi_{n}$ for all $ n\geq 1$ and $ X_{0}=0$. Furthermore, let $ T = \inf\{n\geq 0: X_{n}= 1\}$. Using the analysis below, we will see that $ \PP(T<\infty)=1$. The Optional Stopping Theorem gives $\mathbb{E}[X_{T\land t}]=0$ for all $ t\geq 0$. Yet, $1 = \mathbb{E}[X)_{T}]\neq 0$. We thus see that the condition $\mathbb{E}[T]<\infty$ in $ 4)$ is necessary, since $\mathbb{E}[T] = \infty$. 
	\begin{figure}[H]
	    \centering
	    \includesvg[width=0.6\linewidth]{images/SRW-on-Z.svg}
	    \caption{Illustration of simple random walk (first step) on $ \Z$.}
	    \label{fig: SRW on Z}
	\end{figure}
\end{examplesblock}


\mymark{Lecture 6} We consider again the example of the simple random walk \ref{def: SRW on ints} $ (X_{n})_{n\in \N}$ and define the stopping times 
\[
	T_{c} = \displaystyle\inf {n\geq 0: X_{n = c}}, \quad c\in \Z
\]
Set $ T = T_{-a}\land T_{b}$ for $ a b \in \Z$. We now ask what is $ \PP(T_{-a}\land T_{b})$?\\ 

To answer this, note first that $ X^{T}_{n} = X_{T\land n}$ is a martingale by the Optional Stopping Theorem and we also have the bounded differences $ |X_{n+1}-X_{n}|\leq 1$ for all $ n\geq 1$.\\ 

\underline{Claim:} $\mathbb{E}[T]<\infty$.\\ 
To show this, we will \textit{stochastically dominate} $ T$ be a geometric random variable, which automatically has a finite expectation and then conclude using the non-negativity of $ T$. Now we have that for the sequence $ \xi_{1}, \xi_{2}, \cdots, \xi_{a+b}$ the probability that they all are either $ +1$ or $ -1$ is $ 2\cdot 2^{-(a+b)}$ by independence, call this event $ A_1$. The same is true for the shifted sequence $ \xi_{k(a+b)+1}\cdots \xi_{(k+1)(a+b)}$ for all $ k\in \N$, where we call the corresponding event $ A_{k}$.  \\ 

Thus, we can bound $ T$ by the the random variable
\[
	Z(\omega) =\displaystyle\inf\{n\geq 0: \omega \in A_{n}\}  
\]
which has the distribution $ Z\sim Geom(2\cdot2^{-(a+b)})$. Thus, $\mathbb{E}[T]<\mathbb{E}[Z]\leq (a+b)\cdot 2^{a+b-1}<\infty$. Thus, we conclude by the OST that $\mathbb{E}[X_{T}]=\mathbb{E}[X_{0}]=0$. Hence, $ -a\PP(T_{a}<T_{b})+b\PP(T_{b}<T_{-a}) = 0$ and so a quick computation yields that $ \PP(T_{-a}<T_{b}) = \frac{b}{a+b}$.\\ 

\section{Martingale Convergence Theorem}\label{sec: mg conv thm discrete case}

\begin{theorem}[Almost sure martingale convergence theorem]\label{thm: a.s. mg conv thm disc}
	Let $ X$ be a supermartingale bounded in $ \mathcal{L}^{1}$, i.e. satisfying $\displaystyle \sup_{n}\mathbb{E}[|X_{n}|]<\infty$. Then, there exists $ X_{\infty}\in \mathcal{ L}^{1}(\F_{\infty}), \F_{\infty} = \sigma(\F_{n}: n\geq 0)$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow} X_{\infty}$, a.s.
\end{theorem}

Before we embark on the proof of this theorem, we need so me supporting results. First we have a result from analysis and we set up some notation. Let $ x - (x_{n}_{n\in \N})$ be a real sequence and let $ a<b$ be reals. We proceed to define the \textit{number of upcrossings of the sequence } before time $ n\in \N$. Wec constructrecursively the sequence of times:
\[
	\begin{array}{ll}\label{eq: stopping times doob upcrossing}
    T_{0}(x) &= 0 \\
    S_{k+1}(x) &= \displaystyle\inf\{n\geq T_{k}(x): x_{n}\leq a\}\\
    T_{k+1}(x) &= \displaystyle\inf\{n\geq S_{k+1}(x): x_{n}\geq b\}\\
\end{array}
\]

and 
\[
	N_{n}([a,b], X) = \sup\{k\geq 0: T_{k}(x) \leq n\}
\]
Observe that as $ n\to \infty$, $ N_{n}([a,b], x)\uparrow N([a,b], x) = \sup\{k
geq 0: T_{k}(x)<\infty\}$ (see figure \ref{fig: doob upcrossing} for an illustration).

\begin{boxlemma}\label{lemma: upcrossing lemma}
	Let $ X = (X_{n})$ be a real sequence. Then $ X$ converges in $ \overline{\R} = \R \cup \{\pm \infty\}$ if and on ly if for all $ a<b$, $ a, b\in \Q$, $ N([a,b], X)<\infty$.
\end{boxlemma}

\begin{proof}
	\underline{$ \implies:$} Suppose $ x$ converges, if $ a<b$ such that $ N([a,b], x)=\infty$, then $ \displaystyle\liminf_{n}x_{n}\leq a < b \leq \displaystyle \limsup_{n}x_{n}$, a contradiction.\\ 
	\underline{$ \impliedby:$} if not, then $ \displaystyle\liminf_{n}x_{n}< \displaystyle \limsup_{n}x_{n}$ which implies that there exists $ a<b$ in $ \Q$ with $  \displaystyle\liminf_{n}x_{n}< a< b< \displaystyle \limsup_{n}x_{n}$, and hence $ N([a,n], x) = \infty$, a contradiction. 
\end{proof}

Now we state \text{it} Doob's upcrossing Inequality

\begin{boxlemma}[Doob's upcrossing inequality]\label{lemma: doob upcrossing}
Let $ X$ be a supermartingale, then for all $ n \in \N$: 
\[
	(b-a)\cdot\mathbb{E}[N_{n}([a,b], X)]\leq\mathbb{E}[(X_{n}-a)^{-}]
\]
\end{boxlemma}


\begin{proof}
	It is not hard to check that the sequences of times in \ref{eq: stopping times doob upcrossing} are stopping times. Now we have:

\[
\begin{array}{ll}
   &\displaystyle\sum^{n}_{k=1}(X_{T_{k}\land n}-X_{S_{k}\land n})\\
   &=\underbrace{\displaystyle\sum^{N_{n}}_{k=1} (X_{T_{k}}-X_{S_{k}})}_{\geq N_{n}\cdot (b-a)} + (X_{n}-X_{S_{N_{n}+1}}) \mathbf{1}(S_{N_{n}+1}\leq n)
\end{array}
\]
Since $ T_{k\land n}\geq S_{k\land n}$, the OST gives $\mathbb{E}[X_{T_{k}\land n}]\leq\mathbb{E}[X_{S_{k}\land n}]$. Note: 
\[
\underbrace{X_{n}-X_{S_{N_{n}}+1}}_{\geq (X_{n}-a) \land 0 = -(X_{n}-a)^{-}} \mathbf{1}(S_{N_{n}+1}\leq n).\]

Thus, taking expectations on both sides gives:	
	\[
		0 \geq (b-a)\cdot\mathbb{E}[N_{n}]-\mathbb{E}[(X_{n}-a)^{-}].
	\]
	thus concluding the proof.
\end{proof}


\begin{figure}[H]
    \centering
    \includesvg[width=0.8\linewidth]{images/Doob-crossing.svg}
    \caption{Illustration of upcrossings for the process $ (X_{n})_{n\in\N}$.}
    \label{fig: doob upcrossing}
\end{figure}

Now we proceed to the proof of the martingale convergence theorem:

\begin{proof}{(Theorem \ref{thm: a.s. mg conv thm disc})}
Fix $ a<b$, in $ \Q$. Have 
\[
\begin{array}{ll}
	\mathbb{E}[N_{n([a,b], X)}] &\leq(b-a)^{-}\underbrace{\mathbb{E}[(X_{n}-a)^{-}]}_{\leq\mathbb{E}[|X_{n}|+a]} \\
				    &\leq  (b-a)^{-} \left( \displaystyle\sup_{n\geq 0}\underbrace{\mathbb{E}[|X_{n}|]}_{<\infty}+a \right)
\end{array}
\]
Also have $ N_{n}([a,b], X)\uparrow N([a,b], X)$
as $ n\to \infty$. By monotone convergence: $ \mathbb{E}[N([a,b], X)]<\infty$. Set 
\[
	\Omega_{0} =\displaystyle\bigcap_{a<b \\ a,b, \in \Q}\{N([a,b], X)<\infty\}\in \F_{\infty} 
\]
and $ \PP(\Omega_{0})=1$. On $ \Omega_{0}$, $ X$ converges. set
\[
X_{\infty} = \left\lbrace
\begin{array}{@{}l@{}}
	\displaystyle \lim_{n\to \infty}X_{n} \quad \text{ on } \Omega_{0}  \\
	0, \quad \text{ on } \Omega\setminus \Omega_{0}.
    
\end{array}\right.
\]
So, $ X_{\infty}$ is $ \F_{\infty}-$measurable, $X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty} $ a.s. and 
\[
	\mathbb{E}[|X_{\infty}|] =\mathbb{E}[\displaystyle\liminf_{n}|X_{n}| ]\leq \displaystyle\liminf_{\mathbb{E}[X_{n}]}<\infty.  
\]
\end{proof}

\begin{boxcor}\label{cor: a.s. disc conv pos sup mg}
Let $ B$ be a upermaartingale. Then, $ X$ converges a.s.
\end{boxcor}

\begin{proof}
	$\mathbb{E}[|X_{n}|] =\mathbb{E}[X_{n}]\leq\mathbb{E}[X_{0}]$. Apply the martingale convergence theorem to conclude.
\end{proof}

\mymark{Lecture 7} \section{Doob's inequalities}


\begin{theorem}[Doob's maximal inequality]\label{thm: doob maximal ineq discrete}
Let $ X$ be a non-negative submartingale and set $ X^{*}_{n} = \displaystyle\sup_{ 0\leq k \leq n}X_{k}$ . Then for all $ \lambda \geq 0$, 
\[
\begin{array}{ll}
	\lambda \cdot \PP(X^{*}_{n}\geq \lambda ) &\leq\mathbb{E}[X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq \lambda)] \\
						  &\leq\mathbb{E}[X_{n}].
\end{array}
\]


\end{theorem}


\begin{proof}
	Let $ T = \displaystyle\inf\{k\geq 0 : X_{k}\geq \lambda\}$ (it is a stopping time). Then $ \{X^{*}_{n}\geq \lambda\} = \{T\leq n\}$. Also have that $ X_{T\land n}$ is a submartingale by the OST. Then $\mathbb{E}[X_{T\land n}]\leq\mathbb{E}[X_{n}]$. Now, 

	\[
	\begin{array}{ll}
		\mathbb{E}[X_{T\land n}] &= \mathbb{E}[X_{T}\cdot \mathbf{1}(T\leq n)] \\
					 &+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\geq \lambda \cdot \PP(T\leq n)+\mathbb{E}[X_{n}\cdot \mathbf{1}(T>n)]\\ 
					 &\implies \lambda\cdot \PP(T\leq n) \leq \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(\underbrace{T\leq n}_{=\{X^{*}_{n}\geq \lambda\}}) \right]\\ 
					 &\leq \mathbb{E}\left[ X_{n} \right] 
	\end{array}
	\]
	
\end{proof}


\begin{theorem}[Doob's $  \mathcal{L}^{1}$ inequality]\label{thm: doob L1 ineq disc}
Lte $ p>1$ and let $ X$ be a martingale or a non-negative submartingale. Set $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n }|X_{k}|$. Then 

\[
	\norm{X^{8}_{n}}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}.\label{eq: doob L1 disc}
\]

\end{theorem}

\begin{proof}
	By Jensen, it is enough to prove  \ref{eq: doob L1 disc} for a non-negative submartingale. Now, observe that 

	\[
	\begin{array}{ll}
	     &= b \\
		(y\land k)^{p} &= \displaystyle\int_{k}^{0} px^{p-1} \mathbf{1(y\geq x)}\diff x  
			       =\mathbb{E}[ \displaystyle\int_{0}^{k}[x^{p-1} \mathbf{1}(X^{8}_{n}) \diff x  ]\\ 
			       &\stackrel{\text{Fubini}}{=} \displaystyle\int_{0}^{k} px^{p-1}\underline{\PP(X^{*}_{n}\geq x)}_{\leq \frac{1}{x}  \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(X^{*}_{n}\geq x) \right] }\diff x \\ 
			       &\leq \mathbb{E}\left[ \displaystyle\int_{0}^{k} px^{p-2}\cdot \mathbf{ 1}(X^{*}_{n}\geq x) \diff  x \cdot X_{n} \right] \\ 
			       &= \mathbb{E}\left[ \frac{p}{p-1}(X^{*}_{n}\land k)^{p-1}\cdot X_{n} \right]\\ 
			       &\stackrel{\text{H\"{o}lder}}{\leq} \frac{p}{p-1}\cdot \norm{X_{n}}_{p}\cdot \norm{X^{*}_{n}\land k}^{p-1}_{p}. 
	\end{array}
	\]
	So we proved $ \norm{X^{*}_{n}\land k}^{p}_{p}\leq \frac{p}{p-1}\norm{X_{n}}_{p}\cdot\norm{X^{*}_{n}\land k}^{p-1}_{p}$, which implies $ \norm{X^{*}_{n}\land k}_{p}\leq \frac{p}{p-1}\cdot \norm{X_{n}}_{p}$. Now take $ k\to \infty$ and use monotone convergence to conclude the argument.
\end{proof}



\begin{theorem}[$  \mathcal{L}^{p}$-convergence theorem]\label{thm: Lp convergence theorem discrete}
	Let $ X$ be a martingale and $ 1<p<\infty$, then the following are equivalent: 

	\begin{enumerate}
		\item $ X$ is bounded in $ \mathcal{L^{p}}$, i.e. $ \displaystyle\sup_{n\geq 0}\norm{X_{n}}_{p}<\infty $.
		\item $ X$ converges 'underline{almost surely} and in $ \mathcal{L}^{p}$ to a limit $ X_{\infty}\in \mathcal{L}^{p}$.
		\item There exists $ Z\in \mathcal{L}^{p} $ s.t. $ X_{n} = \mathbb{E}\left[Z | \F_{n}  \right]$ a.s.
	\end{enumerate}
	
\end{theorem}


\begin{proof}
\underline{$1) \implies 2)$:} $ X$ bounded in $ \mathcal{L}^{p}$ implies $ X$ is bounded in $ \mathcal{L}^{1} $. So there exists $ X_{\infty}$ such that $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ \underline{a.s.} \\ 
	Also, $ \mathbb{E}\left[ |X_{\infty}|^{p} \right] = \mathbb{E}\left[  \displaystyle\liminf_{n}|X_{n}|^{p} \right]\stackrel{\text{Fatou}}{\leq} \displaystyle\liminf_{ \mathbb{E}\left[ |X_{n}|^{p} \right]}<\infty$. Thus, $ X_{\infty}\in \mathcal{L}^{p} $.\\ 

	Now, let $ X^{*}_{n} = \displaystyle\sup_{0\leq k \leq n}|X_{k}|$, $ X^{*}_{\infty} = \displaystyle\sup_{k\in \N}|X_{k}|$. Thus, 
	\[
		|X_{n}-X_{\infty}|\leq 2X^{*}_{\infty}
	\]
for all $ n\in \N$. Thus, it is enough to show by DCT that $ X^{*}_{\infty}\in \mathcal{L}^{p} $. By Doob's $ \mathcal{L}^{p}- $inequality,
	$\norm{X^{*}_{n}}_{p} &= \frac{p}{p-1}\cdot \displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty $
By MCT ($ X^{*}_{n}\uparrow X^{*}_{\infty}$):
$\norm{X^{*}_{\infty}}_{p}\leq \frac{p}{p-1}\displaystyle\sup_{n\in \N}\norm{X_{n}}_{p}<\infty$
Thus, $ X^{*}_{\infty}\in \mathcal{L}^{p} $.

\underline{$2)\implies 3)$:} $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X_{\infty}$ a.s. and in $ \mathcal{L}^{p} $. Set $ Z = X_{\infty}$. Need to show: $ X_{n} = \mathbb{E}\left[ X_{\infty} | \F_{n}\right]$ for all $ n\in \N$. 
\[
\begin{array}{ll}
	\norm{X_{n}- \mathbb{E}\left[ X_{oo}|\F_{n} \right]}_{p} &\stackrel{m\geq n}{=} \norm{\mathbb{E}\left[ X_{m}-X_{\infty} |\F_{n}\right]}_{p} \\ 
								 &\stackrel{\text{contraction (Jensen)}}{\leq} \norm{X_{m}-X_{\infty}}_{p}\to 0, \quad m\to \infty.
\end{array}
\]
\underline{3)\implies  1):} By conditional Jensen, we can conclude.
\end{proof}

\begin{boxdef}\label{def: lp closed mg disc}
	A martingale of the form $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$, $ Z\in \mathcal{L}^{p} $ is called a martingale \underline{closed in $ \mathcal{L}^{p} $}.
\end{boxdef}

\begin{boxcor}\label{cor: lp closed a.s. conv}
	Let $ Z\in \mathcal{L}^{p} $, $ X_{n} = \mathbb{E}\left[ Z|\F_{n} \right]$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ Z|\F_{\infty} \right]$ a.s. and in $ \mathcal{L}^{p} $ where $ F_{\infty} = \sigma(X_{n}, n\geq 0)$.
\end{boxcor}

\begin{proof}
	By theorem \ref{thm: Lp convergence theorem discrete}, we have $ X_{n}\stackrel{n\to \infty\longrightarrow}X_{\infty}$ a.s. and in $ \mathcal{L}^{p} $. Now, we need to show:
	\[
	X_{\infty} = \mathbb{E}\left[ Z|\F_{\infty} \right]\quad \text{a.s.}
	\]
Now, we have that $ X_{\infty}$ is $ \F_{\infty}-$measurable (being the pointwise limit of $ X_{n}, n\geq 0$) and for all $ A\in \F_{\infty}$, $ \mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right]$. Fix $ A\in\displaystyle\bigcup_{n\geq 0}\F_{n} $, a $ \pi-$system generating $ \F_{\infty}$. There exists $ N\in \N$ such that $ A\in \F_{N}$. Let $ n\geq N$, now 
\[
	\mathbb{E}\left[ Z\cdot \mathbf{1}(A) \right] = \mathbb{E}\left[ X_{n}\cdot \mathbf{1}(A) \right]\stackrel{n\to \infty}{\longrightarrow} \mathbb{E}\left[ X_{\infty}\cdot \mathbf{1}(A) \right].
\]
\end{proof}

\begin{boxdef}[Uniform integrability]\label{def: UI}
A collection of variables $ (X_{i})_{i\in I}$ is called uniformly integrable (UI) if 

\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(|X_{i}|> M) \right]\stackrel{M\to \infty}{\longrightarrow}0.
\]
\end{boxdef}

Equivalently, $ (X_{i})_{i\in I}$ is UI if $ (X_{i})$ is bounded in $ \mathcal{L}^{1} $ and for all $ \epsilon>0$, there exists $ \delta >0$ such that for all $ A\in \F$ with $ \PP(A)<\delta$, 
\[
	\displaystyle\sup_{i\in I} \mathbb{E}\left[ |X_{i}|\cdot \mathbf{1}(A_{i}) \right]<\epsilon.
\]
\begin{enumerate}[label = (\roman*)]
	\item A UI family is bounded in $ \mathcal{L}^{1} $. 
	\item If a family $ (X_{i})$ is bounded in $ \mathcal{L}^{p} $, $ p>1$, then it is also UI.
\end{enumerate}

\begin{boxlemma}\label{lemma: UI a.s. conv}
	Let $ (X_{n})_{n\in \N}$, $ X$ be in $ \mathcal{L}^{1} $ and $ X_{n}\stackrel{n\to \infty}{\longrightarrow}X$ a.s. Then $ X_{n}\stackrel{n\to \infty}{\longrightarrow}$ in $ \mathcal{L}^{1} $ if and only if $ (X_{n})_{n\in \N}$ is UI.
\end{boxlemma}

\end{document}

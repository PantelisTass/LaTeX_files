\documentclass{article}
\input{preamble}  % Include the preamble from an external file
%\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}\addtocounter{page}{-1}}
\fancyhead{}
\fancyhead[L]{\text{Advanced Probability}}
\fancyhead[R]{\text{Pantelis Tassopoulos}}

\title{\Huge Part III Advanced Probability \\ 
\huge Based on lectures by P. Sousi}
\author{\Large Notes taken by Pantelis Tassopoulos}
\date{\Large Michaelmas 2023}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage 

\section{Conditional Expectation}
\mymark{Lecture 1}\subsection{Basic definitions}
Let $ (\Omega, \F, \PP) $ be a probability space. Remember the following definitions 
\begin{boxdef}[Sigma algebra]\label{def: sigma algebra}
	$ \F $ is a sigma algebra if and only if: ($ \F\in \mathcal{P}{\Omega} $)
	\begin{enumerate}
		\item $ \Omega \in \F $
		\item $ A\in \F \implies A^c\in \F$
		\item $ (A_n)_{n\in \N}\subseteq \F \implies \displaystyle\bigcup_{n\in\N}A_n\in\F $
	\end{enumerate}
\end{boxdef}
\begin{boxdef}[Probability measure]\label{def: prob measure}
	$ \PP $ is a probability measure if
	\begin{enumerate}
		\item $ \PP:\F\to [0,1] $ (i.e. a set function)
		\item $ \PP(\Omega) = 1 $, and $ \PP(\emptyset) = 0 $
		\item $ (A_n)_{n\in \N} $ pairwise disjoint $ \implies \PP\left(\displaystyle\bigcup_{n\in\N}A_n\right) =\displaystyle\sum^{\infty}_{n=1}\PP(A_n)$.
	\end{enumerate}
	
\end{boxdef} 
\begin{boxdef}[Random Variable]\label{def: rv}
	$ X:\Omega\to \R $ is a \underline{random variable} if for all $ B $ open in $ \R $, $ X^{-1}(B)\in \F $.
\end{boxdef}
\begin{remark}
	Observe that the sigma algebra $ \mathcal{G}=\{B\subseteq\R: X(B)\in\F\}\supseteq \mathcal{O} \implies \mathcal{G}\supseteq \mathcal{B}(\R) $, the former being the collection of open sets in $ \R $ and the latter the Borel sigma algebra on $ \R $ with the usual topology, namely, $ \sigma(\mathcal{O})$ (see below for the notation).
\end{remark}

Let $ \mathcal{A} $ be a collection of subsets of $ \Omega $. We define 
\[\begin{array}{ll}
	\sigma(\mathcal{A}) &= \text{smallest sigma algebra containing $ \mathcal{A} $} \\
     &=\displaystyle \bigcap \{\mathcal{T}:\mathcal{T} \text{ sigma algebra containing }\mathcal{A}\}.
\end{array}
\]

\begin{boxdef}[Borel sigma algebra on $ \R $]\label{def: borel sigma alg}
	Let $ \mathcal{O} = \{\text{open sets} \R\} $. Then, the Borel sigma algebra $ \mathcal{B}(\R)( \coloneq \mathcal{B} ) $ is defined as above, namely, 
	\[\mathcal{B}(\R)\coloneq \sigma(\mathcal{O}).\]
\end{boxdef}

Let $ (X_i)_{i\in I} $ be a family of random variables, then $ \sigma(X_{i}:i\in I) =$ the smallest sigma algebra that makes them all measurable. We also have the characterisation 
$ \sigma(X_{i}: i\in I) = \sigma(\{\underbrace{\{\omega\in \Omega: X_{i}(\omega)\in B\}}_{X^{-1}_{i}(B)}, i\in I, B\in \mathcal{B}(\R)\})$.

\subsection{Expectation}

Note we use the following for the indicator function on some event $ A $
\[
    \mathbf{1}(A)(x) = \mathbf{1}(x\in A) 
     \coloneqq \left. \begin{array}{@{}l@{}}
    1, \quad x\in A \\
    0, \quad x\notin A
     \end{array}\right\rbrace, \quad A\in \F.
\]


We now begin the construction of the expectation of generic random variables.\\

\underline{Positive simple random variables:} $X = \displaystyle\sum^{
n}_{i=1}\mathbf{1}(A_{i}), c_{i}\geq 0, A_{i}\in\F.  $.
\[
	\mathbb{E}[X]\coloneqq\displaystyle\sum^{n}_{i=1}c_{i}\PP(A_{i}). 
\]


\underline{Non-negative random variables:} $ (X\geq 0). $
We proceed by approximation. Namely, let $ X_{n}(\omega)\coloneqq 2^{-n}\lfloor 2^{-n}\cdot X(\omega)\rfloor \land n \uparrow X(\omega) , n\to \infty$. Now, by monotone convergence, 
\[
	\mathbb{E}[X]\coloneqq \uparrow \displaystyle\lim_{n\to\infty}\mathbb{E}[X_{n}]=\displaystyle\sup\mathbb{E}[X].
\]

\underline{General random variables:} Have the decomposition $ X = X^{+}-X^{-} $, where $ X^{+} = X\lor 0$, $X^{-}=-X\land 0 $. If one of $ \mathbb{E}[X^{+}], \mathbb{E}[X^{-}]  <\infty $ then set 
\[
	\mathbb{E}[X]\coloneqq \mathbb{E}[X^{+}]-\mathbb{E}[X^{-}].  
\]

\begin{boxdef}\label{def: integrable rv}
	$ X $ is called \underline{integrable} if $ \mathbb{E}[|X|]<\infty $.
\end{boxdef}

\begin{boxdef}\label{def: cond prob event}
Let $ B\in \F $ with $ \PP(B)>0 $. Then for all $ A\in \F $, set 
\[
\PP(A|B)\coloneqq \frac{\PP(A\cap B)}{\PP(B)}
\] 

\end{boxdef}


Now for an integer-valued random variable $ X $, we set:
\[
	\mathbb{E}[X|B]\coloneqq \frac{\mathbb{E}[X\cdot \mathbf{1}_{B}]}{\PP(B)}
\]


\subsection{Conditional expectation with respect to countably generated sigma algebras}

\mymark{Lecture 2}We now extend the definition of the conditional expectation for a \underline{countably generated sigma algebra}. Let $ (\Omega, \F, \PP) $ be a probability space. We call the sigma algebra $\mathcal{G} $ coutnably generated if there exists a colection $ (B_{n})_{n\in \N} $ of pairwise disjoint events such that $\displaystyle\bigcup_{n\in I}B_{n} = \Omega$ with ($ I $ countable) and $\mathcal{ G} = \sigma(B_{i}:i\in I)$.\\ 

Let $X$ be an integrable random variable. We want to define $\mathbb{E}[X|\mathcal{G}]$.\\ 

Define $X'(\omega) = \mathbb{E}[X|B_{i}]$, whenever $w\in B_{i}$, i.e. 
\[
	X' =\displaystyle\sum_{i\in I}\mathbf{1}(B_{i})\cdot\mathbb{E}[X|B_{i}]. 
\]

We make the convention that $\mathbb{E}[X|B_{i}] = 0$ if $\PP(B_{i}) = 0$. It is easy to check that $X'$ is $\mathcal{G}-$measurable. We also have that 
\[
\mathcal{G}  = \left\{\displaystyle\bigcup_{j\in } B_{j}: J\subseteq I \right\}
\]
and $X'$ satisfies for all $ G\in\mathcal{G}$:$\mathbb{E}[X\cdot\mathbf{1}_{G}]=\mathbb{E}[X'\cdot\mathbf{1}_{G}] $ and 
\[\begin{array}{ll}
	\mathbb{E}[|X'|] &\leq\mathbb{E} \left[\displaystyle\sum_{i\in I}|\mathbb{E}[X|B_{i}]\mathbf{1}(B_{i})  \right] \\
			 &=\displaystyle\sum_{i \in I}\PP(B_{i})\cdot \left|\mathbb{E}[X|B_{i}] \right|\\ 
			 &\leq\displaystyle\sum_{i\in I}\PP(B_{i})\cdot \underbrace{\mathbb{E}[X\cdot\mathbf{1}(B_{i})]}_{\PP(B_{i})}\\ 
			 &=\mathbb{E}[|X|]<\infty.
\end{array}
\]

\subsection{General case}

We say $ A\in \F$ happens \underline{a.s.} if $ \PP(A) = 1$. \underline{Recall} (from measure theory and basic functional analysis): 
\begin{theorem}[Monotone Convergence Theorem (MCT)]\label{thm: MCT}
	Let $(X_{n})_{n\in \N}$ be such that $ X_{n}\geq 0, X$ be random variables such that $ X_{n}\uparrow X$ as $ n\to \infty$. Then, $\mathbb{E}[X_{n}]\uparrow\mathbb{E}[X]$ as $ n\to \infty$.
\end{theorem}
 
\begin{theorem}[Dominanted Convergenec Theorem (DCT)]\label{thm: DCT}
	Let $ (X_{n})_{n\in \N}$ be random variables such that $ X_{n}\to X$ a.s. as $ n\to \infty$ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$, where $ Y$ is integrable, then $\mathbb{E}[X_{n}]\to\mathbb{E}[X]$, as $ n\to \infty$.
\end{theorem}

Let $ 1\leq p <\infty$ and $ f $ a measurable function, then set $ \norm{f}_{p}\coloneqq \left(\mathbb{E}[\norm{f}^{p}]\right)^{\frac{1}{p}}$. If $ p =\infty$, then set $ \norm{f}_{\infty}\coloneqq \displaystyle \inf \{\lambda: |f|\leq \lambda \text{ a.s.}\}$. Recall for all $ p$, the Lebesgue spaces, $\mathcal{L}^{p}(\Omega, \F, \PP)=\{f: \norm{f}_{p}<\infty\}$.

\begin{theorem}\label{thm: orthog proj hilbert}
	$ \mathcal{L}^{2}(\Omega, \F, \PP) $ is a Hilbert space, with inner product $  \bracket{u}{v}_{2}=\mathbb{E}[u\cdot v]$. Furthermore, for any closed subspace $\mathcal{H}$, if $ f\in\mathcal{L}^{2}$, there exists a unique $ g\in\mathcal{H}$ s.t. $ \norm{f-g}_{\mathcal{L}^{2}}=\displaystyle\inf_{h\in\mathcal{H}}\norm{f-h}_{\mathcal{L}^{2}}$ and $ \bracket{f-g}{h}=0$, for all $ h\in\mathcal{H}$. We say that $ g$ is the \underline{orthogonal projection} of $ f$ in $\mathcal{H}$.
\end{theorem}


We now construct the conditional expectation in the general case, for any integrably random variable with respect to an arbitrary sigma algebras.

\begin{theorem}[Conditional Expectation]\label{thm: cond exp}
Let $ (\Omega, \F, \PP)$ be a probability space, $\mathcal{G}\subseteq \F$ a sub-sigma algebra, $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$. Then there exists an integrable random variable $ Y$ satisfying:
\begin{enumerate}
	\item $ Y$ is $\mathcal{G}-$measurable
	\item for all $ G\in\mathcal{G},\mathbb{E}[X\cdot\mathbf{1}(G)]= \mathbb{E}[Y\cdot\mathbf{1}(G)]$.
\end{enumerate}
Moreover, $ Y$ unique in the sense that if $ Y'$ also satisfies the above $ 1),2)$, then $ Y = Y'$ a.s.. We call $ Y$ a version of the conditional expectation of $ X$ given $ G$. We write $ Y =\mathbb{E}[X\mathcal{G}]$ a.s. If $\mathcal{G} = \sigma(Z)$, where $ Z$ is a random variable, then we write $\mathbb{E}[Z] =\mathbb{E}[X|\mathcal{G}]$.

\end{theorem}

\begin{remark}
	$ 2)$ could be replaced by $\mathbb{E}[X\cdot Z] =\mathbb{E}[Y\cdot Z]$ for all $ Z$ bounded $\mathcal{G}-$measurable random variables. 
\end{remark}

We now state and prove the main theorem of this section:

\begin{proof}{(Theorem \ref{thm: cond exp})}
	\underline{Uniqueness:} Let $ Y, Y'$ satisfy $ 1), 2)$. Let $ A = \{Y > Y'\}\in\mathcal{G}$. Then $ 2) $  
	\[\begin{array}{ll}
	&\implies \mathbb{E}[Y\cdot\mathbf{1}(A)] =\mathbb{E}[Y'\cdot\mathbf{1}(A)]=\mathbb{E}[X\cdot\mathbf{1}(A)] \\
	&\implies\mathbb{E}[(Y-Y')\cdot\mathbf{1}(A)] = 0\\ 
	&\implies \PP(A) = \PP(Y>Y') = 0\\ 
	&\implies Y\leq Y' \text{ a.s.}.
	\end{array}
	\]
	We similarly obtain $ Y\geq Y'$ a.s., hence we deduce that $ Y = Y'$ a.s.

	\underline{Existence:} three steps. 
	\begin{enumerate}
		\item Assume that $ X \in\mathcal{L}^{2}(\Omega, \F, \PP)$. Observe that $\mathcal{L}^{2}(\Omega,\mathcal{G},\PP)$ is a closed subspace of $\mathcal{L}^{2}(\Omega, \F, \PP)$. Hence, Theorem \ref{thm: orthog proj hilbert}, we have the decomposition  $\mathcal{L}^{2}(\Omega, \F, \PP) =\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)\oplus\mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.
Then, we have the corresponding decomposition $ X = Y+Z$, where $ Y\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)$ and $ Z\in \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP) $ respectively. Define $\mathbb{E}[X\mathcal{G}]\coloneqq Y$, $ Y$ is $\mathcal{G}-$measurable and for all $ A\in\mathcal{G}$, $\mathbb{E}[X\cdot\mathbf{1}(A)]\mathbb{E}[Y\cdot\mathbf{1}(A)]=\mathbb{E}[Z\cdot\mathbf{1}(A)]$ since $ Z\in  \mathcal{L}^{2}(\Omega,\mathcal{G}, \PP)^{\perp}$.

\underline{Claim:} If $ X \geq 0$, a.s. then $ Y \geq 0$ a.s.
Indeed, let $ A = \{Y< 0\}\in\mathcal{G}$. Then observe that $ 0\leq\mathbb{E}[X\cdot\mathbf{1}(A)]=\mathbb{E}[Y\cdot\mathbf{1}(A)]\leq 0$. Hence $\mathbb{E}[Y\cdot\mathbf{1}(A)]=0$ and so $ \PP(A) = 0$, gibing $ Y = 0$ a.s.

\item Assume $ X\geq 0 $.\\ 
	Define $ X_{n} = X\land n\leq n $, meaning $ X_{n}$ is bounded for all $ n\in \N$. So $ X_{n}\in\mathcal{L}^{2}(\Omega, \F, \PP)$. Let $ Y_{n} =\mathbb{E}[X_{n}]$ a.s.. $ (X_{n})_{n\in \N}$ is an increasing sequence. By the claim abose, so is $ (Y_{n})_{n\in \N}$ a.s.\\
	Define $ Y = \displaystyle \limsup_{n}Y_{n}$ meaning $ Y$ is $\mathcal{G}-$measurable and $ Y = \uparrow \displaystyle \lim_{n\to \infty}Y_{n} $ a.s. Now, we have that for all $ A\in\mathcal{G}$, $\mathbb{E}[X_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]$. Thus, by theorem \ref{thm: MCT} (MCT), $\mathbb{E}[X\cdot\mathbf{1}(A)]= \displaystyle \lim_{n\to \infty} \mathbb{E}[X_{n}\cdot\mathbf{1}(A)] = \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)] =\mathbb{E}[Y\cdot\mathbf{1}(A)]$.

\item $ X$ general in $\mathcal{L}^{1}$.\\ 
	Decompose as before $ X = X^{+}-X^{-}$. Define, $\mathbb{E}[X\mathcal{G}] =\mathbb{E}[X^{+}|\mathcal{G}]-\mathbb{E}[X^{-}|\mathcal{G}]$.
\end{enumerate}

\end{proof}

\mymark{Lecture 3}
\begin{remark}
From the second step of the proof of Theorem \ref{thm: cond exp} we see that we can define $\mathbb{E}[X|\mathcal{G}]$ for all $ X\geq 0$, not necessarily integrable. It satisfies all conditions $ 1) , 2)$ except for the integrability one.
\end{remark}

\begin{boxdef}\label{def: independence of sigma algebras}
$\underbrace{\mathcal{G}_{1},\mathcal{G}_{2}, \dots}_{\text{sigma algebras}} \subset \F$. We call them \underline{independent} if whenever $ G_{i}\in \mathcal{G}_{i}$ and $ i_{1}<\dots i_{k}$ for some $ k \in \N$, then $ \PP(G_{i_{1}}\cap \dots\cap G_{i_{k}}) = \displaystyle \prod^{k}_{j=1}\PP(G_{i_{j}})$.\\ 

Moreover, let $ X$ be a random variable and $\mathcal{G}$ a sigma algebra, then they are said to be int if $ \sigma(X)$ is independent of $\mathcal{G}$.
\end{boxdef}

\underline{Properties of conditional expectations:}
Fix $ X,y \in\mathcal{L}^{1}$, $ G\in \F$.
\begin{enumerate}
	\item $\mathbb{E}[\mathbb{E}[X\mathcal{G}]]=\mathbb{E}[X]$ (take $ A  = \Omega$)
	\item If $ X$ is $\mathcal{G}-$measurable, then $\mathbb{E}[X\mathcal{G}]=X$ a.s.
	\item If $ X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X\mathcal{G}]=\mathbb{E}[X]$
	\item If $ X\geq 0$ a.s., then $\mathbb{E}[X\mathcal{G}]\geq 0 $ a.s. 
	\item For $ \alpha, \beta \in \R$ $\mathbb{E}[\alpha X + \beta Y |\mathcal{G}] = \alpha\mathbb{E}[X]+\beta\mathbb{E}[Y]$
	\item $\mathbb{E}[X|\mathcal{G}]|\leq\mathbb{E}[|X| |\mathcal{G}]$ a.s. 
\end{enumerate}

Below we provide expensions of useful measure theoretic results for the expectation to their corresponding conditional counetparts. First recall:
\begin{boxlemma}[Fatou's Lemma]\label{lemma: Fatou}
Let $  X_{n}\geq 0$ for all $ n\in \N$. Then 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s}
\]


\end{boxlemma}


\begin{theorem}[Jensen's Inequality]\label{thm: jensen}
If $ X$ is integrable and $ \phi: \R \to \R$ is a convex function, then 
\[
	\phi(\mathbb{E}[X])\leq\mathbb{E}[\phi(X)]\quad   \text{ a.s.}
\]
\end{theorem}
Now the results themselves:

\begin{theorem}[Conditional Monotone Convergence theorem (MCT)]\label{thm: cond MCT}
Let $\mathcal{G}\subset \F$ be sigma algebras, $ X_{n}\geq 0$ a.a. and $ X_{n}\uparrow X$, as $ n\to \infty$ a.s. Then 
\[
	\mathbb{E}[X_{n}|\mathcal{G}]\uparrow\mathbb{E}[X|\mathcal{G}] \quad \text{ a.s.}
\]

\end{theorem}

\begin{proof}{Theorem \ref{thm: cond MCT}}
	Set $ Y_{n} =\mathbb{E}[X_{n}\mathcal{G}]$ a.s. Observe that $ Y_{n}$ is a.s. increasing. Set $ Y = \displaystyle\limsup_{n}Y_{n}$. $ Y_{n}$ is $\mathcal{G}-$measurable, hence, so is $ Y$ (as a $ \displaystyle \limsup $ of $\mathcal{G}-$measurable random variables) is also $\mathcal{G}-$measurable. Also, $ Y = \displaystyle \lim_{n\to \infty}Y_{n} $ a.s.\\ 

	\underline{Need to show:} $\mathbb{E}[Y\cdot\mathbf{1}(A)]\mathbb{E}[X\cdot\mathbf{1}(A)]$ for all $ A\in\mathcal{G}$.	Indeed,
	\[\begin{array}{ll}
	    \\
	    \mathbb{E}[Y\cdot\mathbf{1}(A)] &=\mathbb{E}[ \displaystyle \lim_{n\to \infty }Y_{n}\cdot\mathbf{1}(A) ] \stackrel{\text{MCT}}{=} \displaystyle \lim_{n\to \infty} \mathbb{E}[Y_{n}\cdot\mathbf{1}(A)]\\
																	      &=\displaystyle \lim_{n\to \infty }\mathbb{E}[X_{n}\cdot\mathbf{1}(A)]  =\mathbb{E}[X\cdot\mathbf{1}(A)].
	\end{array}
	\]
	
\end{proof}

\begin{proof}{Theorem \ref{lemma: Fatou}}
$ \displaystyle \liminf_{n}X_{n} = \displaystyle \lim_{n\to \infty }\left( \displaystyle\inf_{k\geq n}X_{k} \right) $, the limit of an increasing sequence. By Theorem \ref{thm: MCT}, we have 
\[
	\displaystyle \lim_{n\to \infty}\mathbb{E}[\displaystyle\inf_{k\geq n}X_{n}|\mathcal{G}] =\mathbb{E}[\displaystyle \liminf_{n}X_{n}|\mathcal{G}]
\]
and 
\[
	\mathbb{E}[\displaystyle \inf_{k\geq n}X_{k}|\mathcal{G}]\stackrel{\text{a.s.}}{\leq } \displaystyle\inf_{k\geq n}\mathbb{E}[X_{k}|\mathcal{G}]\footnote{\text{can take the infinum due to countability that preserves a.s.}}
\]
which gives the result 
\[
	\mathbb{E}[\displaystyle\liminf_{n}X_{n}]\leq \liminf_{n}\mathbb{E}[X_{n}] \quad \text{a.s.}
\]

		
\end{proof}

\begin{theorem}[Conditional Dominated Convergence Theorem]\label{thm: cond DCT}
	SUppose $ X_{n}\to X$ a.s. $ n\to \infty $ and $ |X_{n}|\leq Y$ a.s. for all $ n\in \N$ with $ Y$ integrable. Then $\mathbb{E}[X_{n}\mathcal{G}]\to\mathbb{E}[X\mathcal{G}]$ a.s. as $n\to \infty.$
\end{theorem}

\begin{proof}
	From $ -Y\leq X_{n}\leq Y$, we have $ X_{n}+Y\geq 0$ for all $ n\in \N$ and $ Y-X_{n}\geq 0 $a.s. By Theorem \ref{lemma: Fatou},
\[
		\begin{array}{ll}
		\mathbb{E}[X+Y\mathcal{G}] &=\mathbb{E}[\displaystyle\liminf_{n}(X_{n}+Y)|\mathcal{G}] \\
					   &\leq \displaystyle\liminf_{n}\mathbb{E}[X_{n}+Y|\mathcal{G}] = \displaystyle\liminf_{n}\mathbb{E}[X_{n}\mathcal{G}]+\mathbb{E}[X]
	\end{array}
\]
Thus, 
\[\begin{array}{ll}
	\mathbb{E}[|X-Y| |\mathcal{G}]&=\mathbb{E}[Y-\displaystyle\liminf_{n}X_{n}|\mathcal{G}]  \\
				     &\leq\mathbb{E}[Y]+\displaystyle\liminf_{n}  \mathbb{E}[X_{n}|\mathcal{G}] 
\end{array}
\]
Hence, 
\[
	\displaystyle\limsup_{n} \mathbb{E}[X_{n}|\mathcal{G}] \leq\mathbb{E}[X|\mathcal{G}]
\]
and 
\[
	\displaystyle\liminf_{n} \mathbb{E}[X_{n}|\mathcal{G}] \geq\mathbb{E}[X|\mathcal{G}]
\]
a.s., concluding the proof.

\end{proof}

\begin{theorem}[Conditional Jensen]\label{thm: cond jensen}
Let $ X\in\mathcal{L}^{1}(\Omega, \F, \PP)$, $ \phi:\R\to \R$ be a convex function s.t. $ \phi(X)$ is integrable or $ \phi(X)\geq 0 $
\[
	\phi(\mathbb{E}[X|\mathcal{G}])\leq\mathbb{E}[\phi(X)|\mathcal{G}] \quad \text{a.s.}
\]
\end{theorem}

\begin{proof}
	\underline{Claim:} (true for any convex function, no proof given) $ \phi(x)=\displaystyle\sup_{i\in\N}(a_{i}x+b_{i})$, $ a_{i}b_{i}\in\R$. 
Thus, 
\[
	\mathbb{E}[\phi(X)|\mathcal{G}]\geq a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i} \quad \text{ for all } i\in \N.
\]
Taking the supremum gives \footnote{can take the supremum due to countability which again preserves a.s.}
\[
\begin{array}{ll}
      
	\mathbb{E}[\phi(X)|\mathcal{G}]&\geq \displaystyle\sup_{i\in \N} \left(  a_{i}\mathbb{E}[X|\mathcal{G}]+b_{i}  \right)\\ 
				       &= \phi(\mathbb{E}[X|\mathcal{G}]) \quad \text{ a.s.}
\end{array}
\]

\end{proof}

\begin{boxcor}\label{cor: norm contraction cond exp}
	For all $ 1\leq p <\infty \norm{\mathbb{E}[X|\mathcal{G}]}_{p}\leq \norm{X}_{p}$.
\end{boxcor}

\begin{proof}
    Apply conditional Jensen.
\end{proof}

\begin{boxprop}[Tower Property]\label{prop: tower ppty}
Let $ X$ be integrable and $\mathcal{H}\subseteq\mathcal{G}$ sigma algebras. Then 
\[
	\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]=\mathbb{E}[X|\mathcal{H}] \quad \text{ a.s.}
\]

\end{boxprop}

\begin{proof}
	\begin{enumerate}[(a)]
		\item $\mathbb{E}[X|\mathcal{H}] $ is $\mathcal{H}-$measurable.
		\item For all $ A\in\mathcal{H}$ NTS: 
			\[
				\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot\mathbf{1}(A)] =\mathbb{E}[\mathbb{E}[X|\mathcal{H}]\cdot\mathbf{1}(A)]
			\]
			Indeed, both terms above are equal to $\mathbb{E}[X\cdot\mathbf{1}(A)]$ since $ A\in\mathcal{G}\subseteq\mathcal{H}$.
			
    \end{enumerate}
    
\end{proof}

\begin{boxprop}\label{prop: meas factorisation cond exp}
Let $ X\in\mathcal{L}^{1}$, $\mathcal{G}\subseteq \F$, $ Y$ bounded $\mathcal{G}-$measurable. Then 
\[
	\mathbb{E}[X\cdot Y|\mathcal{G}] =  Y\cdot\mathbb{E}[X|\mathcal{G}].
\]

\end{boxprop}


\begin{proof}
	\begin{enumerate}[(a)]
		\item RHS is clearly $\mathcal{G}-$measurable.
		\item For all $ A\in\mathcal{G}$: 
			\[
			\begin{array}{ll}
				\mathbb{E}[X\cdot Y\cdot \mathbf{1}(A)] &=\mathbb{E}[Y\cdot\mathbb{E}[X\mathcal{G}]\cdot\mathbf{1}(A)] \\
				\mathbb{E}[X\cdot (\smash{\underbrace{Y\cdot\mathbf{1}(A)}_{\makebox[0pt]{$\mathcal{G}$-\text{meas. and bounded}}}})]&=\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\cdot Y\cdot\mathbf{1}(A)]=RHS.
			\end{array}
			\]
			
    \end{enumerate}
    \vspace{1em} 
(Also observe that by a monotone class argument, we have for any integrable function $ f:\Omega \to \R$, $\mathbb{E}[X\cdot f] =\mathbb{E}[\mathbb{E}[X|\mathcal{ G}]\cdot f] $ ) 
\end{proof}


\mymark{Lecture 4}

We are building towards the Theorem
\begin{theorem}\label{thm: cond expectation sigma indep}
$ X\in \mathcal{L}^1, \mathcal{G}, \mathcal{H} \subseteq \F$. Assume $ \sigma( \mathcal{G}, \mathcal{H})\perp \mathcal{H}$, Then
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]

\end{theorem}


We begin with a definition
\begin{boxdef}\label{def: pi system}
	Let $ \mathcal{A} $ be a collection of sts. It is called a \underline{$ \pi-$system} if for all $ A,B\in \mathcal{A}$, we also have $ A\cap B\in \mathcal{A}$.
\end{boxdef}


\begin{theorem}[Uniquenes of extension]\label{thm: uniqueness meas extension}
Let $ (E, \xi)$be a measurable space and let $ \mathcal{A}$ be a $ \pi-$system generating the sigma algebra $ \xi$. Let $ \mu, \nu$ be two measures on $ (E, \xi)$ with $ \mu(E)=\nu(E)<\infty$. If $ \mu = \nu$ on $ \mathcal{A}$, then $ \mu = \nu$ on $ \xi$.
\end{theorem}

\begin{proof}{(Theorem \ref{thm: cond expectation sigma indep})}
    NTS: for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$
    \[
	    \mathbb{E}[X\cdot \mathbf{1}_{F}] =\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}_{F}]
    \]
    Now, set $  \mathcal{ A} = \{A\cap B : A\in \mathcal{ G}, B\in \mathcal{ H}\}$. It is easy to check that $  \mathcal{A}$ is a $ \pi-$system generating $ \sigma( \mathcal{G}, \mathcal{H})$. If $ F = A\cap B$ for some $ A\in \mathcal{G}$ and $ B \in \mathcal{H}$, Then 
    \[
    \begin{array}{ll}
	    \mathbb{E}[X\cdot \mathbf{1}(A\cap B)] &=\mathbb{E}[X\cdot \mathbf{1}(A)\cdot \mathbf{1}(B)] \\
						  & =\mathbb{E}[X\cdot \mathbf{1}(A)]\cdot\mathbb{E}[ \mathbf{1}(B)] \stackrel{H\perp \sigma( \mathcal{G}, \mathcal{H})}{=}\mathbb{E}[\mathbb{E}[X| \mathcal{G}]\cdot \mathbf{1}(A\cap B)].
    \end{array}
    \]

    Now assume $ X\geq 0$; in the general case, decompose $ X = X^{+}- X^{-}$ and apply same argument to both $ X^{\pm}$. Define the measures $ \mu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ and $ \nu(F) =\mathbb{E}[X\cdot \mathbf{1}(F)]$ for all $ F\in \sigma( \mathcal{G}, \mathcal{H})$. Observe that $ \mu(\Omega) = \nu(\Omega) =\mathbb{E}[X]<\infty$ and we have shown that $ \mu = \nu$ on $ \mathcal{A}$. Thus, $ \mu=\nu$ on $ \sigma( \mathcal{G}, \mathcal{H})$ which finally implies the result 
\[
	\mathbb{E}[X|\sigma( \mathcal{G}, \mathcal{H})] =\mathbb{E}[X| \mathcal{G}] \quad \text{a.s.}
\]


\end{proof}


\begin{examplesblock}{Examples: }\label{examples: 1}

\begin{enumerate}
	\item 
\begin{boxdef}[Gaussian]\label{def: gaussian dist}
$ (X_{1}, X_{2}, \cdots, X_{n})\in \R^{n}$ has the Gaussian distribution if and only if for all scalars $ a_{1}, a_{2}, \cdots, a_{n}\in \R$, $ a_{1}X_{1}+\cdots a_{n}X_{n}$ has the Gaussian distrubition in $ \R$.
\end{boxdef}

A stochastic process (more on that later) $ (X_{t})_{t\geq 0}$ is a \underline{Gaussian process} if for all $ t_{1}<t_{2}<\cdots t_{n}$ the vector $ (X_{t_{1}}, X_{t_{2}}, \cdots, X_{t_{n}})$ is Gaussian.

Let $ (X,Y)$ be a Gaussian vector in $ \R^{2}$. We compute $\mathbb{E}[X|Y]$.\\ 
Let $ X' =\mathbb{E}[X|Y]$. Looking for $ f$ a Borel measurable function s.t. $ \mathbb{E}[X|Y] = f(Y)$ a.s. Let $ f(y) = ay+b$ for some $ a,b\in \R$ to be determined. Now, $ X' = aY+b$, $\mathbb{E}[X'] =\mathbb{E}[X] = a\mathbb{E}[Y]+b$ and $\mathbb{E}[X'\cdot Y] =\mathbb{E}[X\cdot Y]\implies\mathbb{E}[(X-X')\cdot Y]=0$. Thus $ \text{Cov}(X-X', Y)=0\implies \text{Cov}(X,Y) = a^{2}\text{Var}(Y)$.\\ 

\underline{Need to check:} that for all $ Z$ bounded $ \sigma(Y)-$measurable, $ \mathbb{E}[(X-X')\cdot Z] = 0$.\\ 
Indeed, observe that $ (X-X', Y)$ is a Gaussian vector and since $ \text{Cov}(X-X', Y) = 0\implies X-X'\perp Y\implies (X-X')\perp Z$.

\item Let $ (X,Y)$ be a random vector with density in $ \R^{2}$ with joint density function $ f_{X,Y}:\R^{2}\to \R$. Let $ h:\R\to \R$ be a Borel function such that  $ h(X)$ is integrable. We now compute $\mathbb{E}[h(X)| Y]$.\\ 
We have for all $ g$ bounded $ \sigma{Y}-$measurable functions.
	
\[
\begin{array}{ll}
	\displaystyle\int_{\R^{2}}h(x)g(y)f_{X,Y}(x,y) \diff x \diff y &=  \mathbb{E}[h(X)g(Y)]\\ 
								       &=\mathbb{E}[\mathbb{E}[h(X)|Y]g(Y)] =\mathbb{E}[\phi(Y)g(Y)]\\ 
								       &= \displaystyle\int_{\R^{2}}\phi(y)g(y)f_{Y(y)} \diff y  
\end{array}
\]
where $ f_{Y}(y) = \int_{\R}f_{X,Y}(x,y) \diff x  $ and $ \phi:\R\to \R$ is some Borel measurable function. Hence, 

\[
\phi(y) = \left\lbrace
\begin{array}{@{}l@{}}
    \displaystyle\int_{\R} h(x)\frac{f_{X,Y}(x,y)}{f_{Y}(y)} \diff dx, \quad f_{Y}(y)>0   \\
    0, \quad \text{otherwise}
\end{array}\right.
\]
can be seen to work. Thus, we obtain 
\[
	\mathbb{E}[h(X)|Y] = \phi(Y) \quad \text{a.s.}
\]
	
\end{enumerate}
\end{examplesblock}


\section{Discrete Time Martingales}

\begin{boxdef}[Filtration]\label{def: filtration}
	Let $ (\Omega, \F, \PP)$ be a pobability space. A \underline{filtration} is a sequences of increasing sigma sub-algebras of $ \F$, $ (\F_{n})_{n\in \N}$, $ \F_{n}\subseteq \F_{n+1}$ for all $ n\in \N$. We call $ (\Omega, \F, (\F_{n})_{n\in\N})$ a \underline{filtered probability space}.\\ 

	Let $X =  (X_{n})_{n\in \N}$ be a sequence of random variables/a stochastic process. Then it induces $ (\F^{X}_{n})_{n\in N}$, where $ \F^{X}_{n}\coloneqq \sigma(X_{:k\leq n})$ for all $ n\in \N$: the canonical filtration associated to $ X$. We call $ X$ \underline{adapted} to a filtration $ (\F_{n})_{n\in \N}$ if $ X$is $ \F_{n}-$measurable for all $ n\in \N$.$ X$ is called \underline{integrable} if $ X_{n}$ is integrable for all $ n\in \N$.
\end{boxdef}

\begin{boxdef}[Martingale discrete time]\label{def: martingale}
Let $ (\Omega, \F, (\F_{n})_{n\in\N}, \PP)$ be a filtered probability space. Let $ X = (X_{n})_{n\in \N}$be an integrabl and adapted process. 
\begin{itemize}
	\item $ X$ is called a \underline{martingale} if $ \mathbb{E}[X_{n}| \F_{m}]=X_{m}$ a.s. for all $ n\geq m$.
	\item $ X$ is called a \underline{super-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\leq X_{m}$ a.s. for all $ n\geq m$.
        \item $ X$ is called a \underline{sub-martingale} if $ \mathbb{E}[X_{n}| \F_{m}]\geq X_{m}$ a.s. for all $ n\geq m$.
\end{itemize}

\end{boxdef}

\begin{remark}
	If $  X$ is a (super/sub)martingale with respect to $ (\F_{n})_{n\in\N}$, then it is also a martingale with respect to $ (\F^{X}_{n})_{n\in \N}$. To see this, one has to use the tower property \ref{prop: tower ppty}: $ \F^{X}_{n}\subseteq \F_{n}$ for all $ n\in\N$ implies $ \mathbb{E}[X_{n}|\F^{X}_{m}]= \mathbb{E}[ \mathbb{E}[X_{n}|\F_{m}]|\F^{X}_{m}]$ (since $ \mathbb{E}[X_{n}|\F_{m}]$ a.s.).
\end{remark}

\begin{examplesblock}{Examples: }\label{examples: 2}
\begin{enumerate}
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=0$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \xi_{1}+\cdots +\xi_{n}$ for all $ n\in\N$, $ X_{0} = 0$. $ X$ is a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.
	\item Let  $ (\xi_{i})_{i\in \N}$ be iid. s.t. $ \mathbb{E}[\xi_{i}]=1$ for all $ i\in\N$ and define $ X = (X_{n})_{n\in\N}$ by $ X_{n} = \displaystyle\prod^{n}_{i=1}\xi_{i}$ for all $ n\in\N$, $ X_{0} = 1$. $ X$ is again a martingales with respect to $ (\F^{\xi}_{n})_{n\in\N}$.

\end{enumerate}

\end{examplesblock}

\mymark{Lecture 5}Let $ (\Omega, \F, (\F_{n})_{n\in \N}, \PP)$ be a filtered probability space.

\begin{boxdef}[Stopping time discrete time]\label{def: stopping time discrete}
	A \underline{stopping time} $ T$ is a random variable $ T:\Omega\to \Z_{+}\cup \{\infty\}$ s.t. $ \{T\leq n\}\in \F_{n}$ for all $ n\in \N$. Equivalently, if $ \{f=n\}\in \F_{n}$ for all $ n\in \N$ since 
	\[
		\{T=n\}=\underbrace{\{T\leq n\}}_{\F_{n}}\setminus \underbrace{\{T\leq n-1\}}_{\F_{n-1}\subset \F_{n}}\in \F_{n}.
	\]
and 
\[
	\{T\leq n\} =\displaystyle\bigcup^{n}_{k=1}\{T=k\}\in \F_{k}\subset \F_{n}.
\]

\end{boxdef}

\begin{examplesblock}{Examples: }\label{examples: 3}
\begin{enumerate}
	\item Constant time are trivially stopping times.
	\item Let $ X = (X_{n})_{n\in\N}$ be a stochastic process taking values in $ \R$ and $ A\in  \mathcal{B}(\R)$ ($ X$ adapted). Define 
		\[
			T_{A} = \{n\geq 0: X_{n\in A}\}.
		\]
		Then $ \{T_{A}\leq n \} =\displaystyle\bigcup^{n}_{k=0}\{X_{k\in A}\}\in \F_{n} $ for all $ n\in\N$ (with convention $ \inf \emptyset = \infty$).
	\item $ L_{A} = \sup\{n\geq 0: X_{n\in A}\}$ is \underline{NOT} a stopping time.	
\end{enumerate}
\end{examplesblock}

\underline{Properties:} $ S,T, (T_{n})_{n\in\N}$ stopping times. Then $ S\land T, S\lor T$, $ \displaystyle\inf_{n}T_{n}, \displaystyle\sup_{n}T_{n}$, $ \displaystyle\liminf_{n}T_{n}$, $ \displaystyle\limsup_{n}T_{n}$ are also stopping times.

\begin{boxdef}[Stopping time sigma algerbra]\label{def: stopping time sigma algebra}
It $ T$is a stopping time, define 
\[
	\F_{T}=\{A\in \F: A\cap \{T\leq t \}\in \F_{t}\}
\]
Let $ (X_{n})_{n\geq 0}$ be a process. Write $ X_{T}(\omega) = X_{T(\omega)}(\omega)$ for $ \omega \in \Omega$ whenever $ T(\omega)<\infty$. Define the \underline{stopped process:} $ X^{T}_{t}\coloneqq X_{T\land t}$.
\end{boxdef}


\begin{boxprop}\label{prop: stopping time discrete}
	Let $ S$ and $ T$ be stopping times, and let $ X$ be an adapted process, then:
	\begin{enumerate}
		\item If $ S\leq T$, then $ \F_{S}\subseteq\F_{T}$.
		\item $ X_{T}\cdot$ is $ \F_{T}-$measurable.
		\item $ X^{T}$ is adapted. 
		\item If $ X$ is integrable, then the stopped process iss integrable.
	\end{enumerate}
	
\end{boxprop}
\begin{proof}
    \begin{enumerate}
	    \item Immediate from definition.
	    \item Let $ A\in \mathcal{B}(\R)$. Need to show: 
		    \[
			    \{X_{T} \mathbf{1}(T<\infty)\}\cap \{T\leq t\} \in A, \quad \text{ for all }t\geq 0.
		    \]
Indeed, we have that 
\[
	\{X_{T} \mathbf{1}(T<\infty)\} =\displaystyle\bigcup^{t}_{s=0}\underbrace{\{X_{s}\in A\}}_{\F_{s}\subseteq \F_{t}}\cap \underbrace{\{T = s\}}_{\F_{s}}\in \F_{t}.
\]

\item $ X^{T}_{t} = X_{T\land t}$, this being $ \F_{T\land t}-$measurable $ \subseteq\F_{t}-$measurable by $ 1)$, so we conclude it is $ \F_{t}-$measurable.

\item 
	\[
	\begin{array}{ll}
		\mathbb{E}[|X_{t}^{T}|] &=\mathbb{E}[|X_{T\land t}|] \\
					&=\displaystyle\sum^{t-1}_{s=0}\mathbb{E}[|X_{s}|\cdot \mathbf{1}(T = s)]+\mathbb{E}[|X_{t}|\cdot \mathbf{1}(T\geq t)]\\ 
					&\leq\displaystyle\sum^{ t}_{s=0}\mathbb{E}[|X_{s}|]\underbrace{<\infty}_{X_{t} \text{ is integrable}}.
	\end{array}
	\]
	
    \end{enumerate}
    
\end{proof}

We now state and prove a fundamental theorem in Martingale theory: 

\begin{theorem}[Optional Stopping Theorem discrete time]\label{thm: optional stopping discrete time}
Let $ (X_{n}$ be a martingale. 
\begin{enumerate}
	\item If $ T$ is a stopping time, then the stopped process $ X^{T}$ is also a martingale. In particular for all $ t\geq 0$:
		\[
			\mathbb{E}[X_{T\land t}] =\mathbb{E}[X_{0}].
		\]
	\item It $ S\leq T$ are bounded stopping times, then 
		\[
			\mathbb{E}[X_{T}|\F_{S}] = X_{T}, \quad \text{a.s.}
		\]
		and hence $ \mathbb{E}[X_{T}] \mathbb{E}[X_{S}]$.
	\item It there exists an integrable random variable $ Y$ such that $ |X_{n}\leq Y|$ for all $ n \geq 0 $ and $ T$ is finite, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
	\item If there exists $ M\geq 0$ such that $ |X_{n+1}-X_{n}|\leq M$ for all $ n\in \N$ and $ T$ is a stopping time with $ \mathbb{E}[T]<\infty$, then $ \mathbb{E}[X_{T}]= \mathbb{E}[X_{0}]$.
\end{enumerate}

\end{theorem}

\begin{proof}
    \begin{enumerate}
	    \item NTS: for all $ t\geq 0$, $ \mathbb{E}[X_{T\land t}|\F_{t-1}]=X_{T\land t}$ a.s.
		    Indeed, 
\[
\begin{array}{ll}
	\mathbb{E}[X_{T\land t}|\F_{t-1}] &=\displaystyle\sum^{t-1}_{s =0}\mathbb{E}[X_{s}\cdot \mathbf{ 1}(T=s)|\F_{t-1}] \mathbb{E}[X--t]\cdot \mathbf{1}(T\geq t)|\F_{t-1}] \\
					  &=\displaystyle\sum^{ t-1}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t) \quad \text{ a.s.}\\ 
&=\displaystyle\sum^{ t-2}_{s =0} \mathbf{1}(T=s)\cdot X_{s}+X_{t-1}\cdot \mathbf{1}(T\geq t-1) \quad \text{ a.s.}\\ 
&= X_{T\land t-1} \quad \text{a.s.}
\end{array}
\]
\item $S\leq T\leq n, n\in \N$ fixed. Let $ A\in \F_{S}$. \underline{NTS:} $ \mathbb{E}[X_{T}\cdot \mathbf{1}(A)] = \mathbb{e}[X_{s}\cdot \mathbf{1}(A)]$. We compute
	\[
	\begin{array}{ll}
	    X_{T}-X_{S} &= (X_{T}-X_{T-1})+\cdots + (X_{S+1}-X_{S}) \\
			&=\displaystyle\sum^{ n-1}_{k=0}(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T). 
	\end{array}
	\]
	Thus, 
	\[
		\mathbb{E}[X_{T}\cdot \mathbf{1}(A)] \stackrel{ (A\in \F_{S}) }{=}\mathbb{E}[X_{S}\cdot \mathbf{1}(A)]+ \displaystyle\sum^{ n-1}_{k=0}\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)]
	\]
	Have, $ A\cap \{S\leq k\}\in \F_{k}$ and $ A\cap \{T>k\}\in F_{k}$. Thus, $ \mathbf{1}(S\leq k <T)\cdot \mathbf{1}(A)$ is $ \F_{k}-$measurable. Using $ \mathbb{E}[X_{k+1}|\F_{k}]=X_{k}$ a.s. we deduce that 
	\[
	\begin{array}{ll}
		\mathbb{E}[(X_{k+1}-X_{k})\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)]
		&= \mathbb{E}[\smash{\cancelto{0}{\mathbb{E}[(X_{k+1}-X_{k})|\F_{k}]}}\cdot \mathbf{ 1}(S\leq k <T]\cdot \mathbf{1}(A)] \\
		&= 0
	\end{array}
\]
Thus, $ \mathbb{E}[X_{T}|\F_{S}]=X_{S}$ a.s. 

\item By the Optional Stopping Theorem applied to $ (X_{T\land n})_{n\geq 0}$, we have 
	\[
		\mathbb{E}[X_{T\land n}] =\mathbb{E}[X_{0}] \quad \text{for all } n\geq 0.
	\]
Now, $ T$ being finite a.s. implies that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ a.s. By assumption, have $ |X_{T\land n}|\leq Y$ a.s. for all $ n\in \N$ and so can apply DCT to conclude.	

\item Observe that for all $ n\geq 1$
	\[
	X_{T\land n}-X_{0} =\displaystyle\sum^{n-1}_{k=0}(X_{k}-X_{0})\cdot \mathbf{1}(T=k)+(X_{n} -X_{0})\mathbf{1}(T\geq n) 
	\]
Hence, we have the bound (using that $ |X_{k+1}-X_{k}|\leq M$ a.s. for all $ k\geq 0$)
\[
\begin{array}{ll}
    \\
|X_{T\land n}-X_{0}|&\leq M\displaystyle\sum^{n-1}_{k=0} k\mathbf{1}(T=k) + n\mathbf{1}(T\geq n)\\
			&\leq\mathbb{E}[T]<\infty \quad \text{a.s.}
\end{array}
\]
Now, $\mathbb{E}[T]<\infty$ gives $ T<\infty$ a.s. and so can deduce as before that $ X_{T} = \displaystyle \lim_{n\to \infty} X_{T\land n} $ and use the DCT to conclude the argument. 


 \end{enumerate}
    
\end{proof}


\begin{boxcor}\label{cor: pos supermg bound}
Let $ X$ be a positive superartingale, $ T$ a stopping time such that $ T<\infty$ a.s., then 
\[
	\mathbb{E}[X_{T}]\leq\mathbb{E}[X_{0}].
\]

\end{boxcor}

\begin{proof}
	Use Fatou \ref{lemma: Fatou}: $\mathbb{E}[\displaystyle\liminf_{t\uparrow \infty}X_{T\land t}]\leq \displaystyle\liminf_{t\uparrow \infty}\mathbb{E}[X_{T\land t}]\leq\mathbb{E}[X_{0}]$.
\end{proof}


\begin{examplesblock}{Simple random walk on $ \Z$}\label{def: SRW on ints}
	Let $ (\xi_{i})_{i\geq 0}$ be iid Bernoulli random variables with success probability $ 1/2$. Define the process $(X_{n})_{n\geq 0}$ by setting $ X_{n} = \xi_{1}+\dots+\xi_{n}$ for all $ n\geq 1$ and $ X_{0}=0$. Furthermore, let $ T = \inf\{n\geq 0: X_{n}= 1\}$. Using the analysis below, we will see that $ \PP(T<\infty)=1$. The Optional Stopping Theorem gives $\mathbb{E}[X_{T\land t}]=0$ for all $ t\geq 0$. Yet, $1 = \mathbb{E}[X)_{T}]\neq 0$. We thus see that the condition $\mathbb{E}[T]<\infty$ in $ 4)$ is necessary, since $\mathbb{E}[T] = \infty$. 
	\begin{figure}[H]
	    \centering
	    \includesvg[width=0.6\linewidth]{images/SRW-on-Z.svg}
	    \caption{Illustration of simple random walk (first step) on $ \Z$.}
	    \label{fig: SRW on Z}
	\end{figure}
\end{examplesblock}


\end{document}

\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{CMV{\etalchar{+}}03}

\bibitem[ABGM14]{arora2014provable}
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.
\newblock Provable bounds for learning some deep representations.
\newblock In {\em Proc. of International Conference on Machine Learning
  (ICML)}, pages 584--592, 2014.

\bibitem[AGS08]{ambrosio2008gradient}
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar{\'e}.
\newblock {\em Gradient flows: in metric spaces and in the space of probability
  measures}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Bar98]{bartlett1998sample}
Peter~L Bartlett.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock {\em IEEE transactions on Information Theory}, 44(2):525--536, 1998.

\bibitem[BG17]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock \texttt{arXiv:1702.07966}, 2017.

\bibitem[Bot10]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem[BRV{\etalchar{+}}06]{bengio2006convex}
Yoshua Bengio, Nicolas~L Roux, Pascal Vincent, Olivier Delalleau, and Patrice
  Marcotte.
\newblock Convex neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  123--130, 2006.

\bibitem[CB18]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock {\em {\sf arXiv:1805.09545}}, 2018.

\bibitem[CDF{\etalchar{+}}11]{carrillo2011global}
Jos{\'e}~A Carrillo, Marco DiFrancesco, Alessio Figalli, Thomas Laurent, Dejan
  Slep{\v{c}}ev, et~al.
\newblock Global-in-time weak measure solutions and finite-time aggregation for
  nonlocal interaction equations.
\newblock {\em Duke Mathematical Journal}, 156(2):229--271, 2011.

\bibitem[CMV{\etalchar{+}}03]{carrillo2003kinetic}
Jos{\'e}~A Carrillo, Robert~J McCann, C{\'e}dric Villani, et~al.
\newblock Kinetic equilibration rates for granular media and related equations:
  entropy dissipation and mass transportation estimates.
\newblock {\em Revista Matematica Iberoamericana}, 19(3):971--1018, 2003.

\bibitem[CMV06]{carrillo2006contractions}
Jos{\'e}~A Carrillo, Robert~J McCann, and C{\'e}dric Villani.
\newblock Contractions in the 2-wasserstein length space and thermalization of
  granular media.
\newblock {\em Archive for Rational Mechanics and Analysis}, 179(2):217--263,
  2006.

\bibitem[Eva09]{evans2009partial}
Lawrence~C. Evans.
\newblock {\em Partial Differential Equations}.
\newblock Springer, 2009.

\bibitem[GBCB16]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock {\em Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[GLM17]{ge2017learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock \texttt{arXiv:1711.00501}, 2017.

\bibitem[GP10]{guillemin2010differential}
Victor Guillemin and Alan Pollack.
\newblock {\em Differential topology}, volume 370.
\newblock American Mathematical Soc., 2010.

\bibitem[JKO98]{jordan1998variational}
Richard Jordan, David Kinderlehrer, and Felix Otto.
\newblock The variational formulation of the fokker--planck equation.
\newblock {\em SIAM journal on mathematical analysis}, 29(1):1--17, 1998.

\bibitem[JSA15]{janzamin2015beating}
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.
\newblock Beating the perils of non-convexity: {G}uaranteed training of neural
  networks using tensor methods.
\newblock \texttt{arXiv:1506.08473}, 2015.

\bibitem[KSH12]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Lan13]{lang2013complex}
Serge Lang.
\newblock {\em Complex analysis}, volume 103.
\newblock Springer Science \& Business Media, 2013.

\bibitem[LBW96]{lee1996efficient}
Wee~Sun Lee, Peter~L Bartlett, and Robert~C Williamson.
\newblock Efficient agnostic learning of neural networks with bounded fan-in.
\newblock {\em IEEE Transactions on Information Theory}, 42(6):2118--2132,
  1996.

\bibitem[LSU88]{ladyzhenskaia1988linear}
Olga~Aleksandrovna Ladyzhenskaia, Vsevolod~Alekseevich Solonnikov, and Nina~N
  Ural'tseva.
\newblock {\em Linear and quasi-linear equations of parabolic type}, volume~23.
\newblock American Mathematical Soc., 1988.

\bibitem[MBM16]{mei2016landscape}
Song Mei, Yu~Bai, and Andrea Montanari.
\newblock The landscape of empirical risk for non-convex losses.
\newblock {\em {\sf arXiv:1607.06534}}, 2016.

\bibitem[Mit15]{mityagin2015zero}
Boris Mityagin.
\newblock The zero set of a real analytic function.
\newblock {\em {\sf arXiv:1512.07276}}, 2015.

\bibitem[MP99]{mezard1999thermodynamics}
Marc M{\'e}zard and Giorgio Parisi.
\newblock Thermodynamics of glasses: A first principles computation.
\newblock {\em Journal of Physics: Condensed Matter}, 11(10A):A157, 1999.

\bibitem[MV00]{markowich2000trend}
Peter~A Markowich and C{\'e}dric Villani.
\newblock On the trend to equilibrium for the fokker-planck equation: an
  interplay between physics and functional analysis.
\newblock {\em Mat. Contemp}, 19:1--29, 2000.

\bibitem[RM51]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Ros62]{rosenblatt1962principles}
Frank Rosenblatt.
\newblock {\em Principles of neurodynamics}.
\newblock Spartan Book, 1962.

\bibitem[RVE18]{rotskoff2018neural}
Grant~M Rotskoff and Eric Vanden-Eijnden.
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock {\em {\sf arXiv:1805.00915}}, 2018.

\bibitem[SA15]{sedghi2015provable}
Hanie Sedghi and Anima Anandkumar.
\newblock Provable methods for training neural networks with sparse
  connectivity.
\newblock In {\em Proc. of International Conference on Learning Representation
  (ICLR)}, 2015.

\bibitem[San15]{santambrogio2015optimal}
Filippo Santambrogio.
\newblock {\em Optimal Transport for Applied Mathematicians: Calculus of
  Variations, PDEs, and Modeling}, volume~87.
\newblock Birkh{\"a}user, 2015.

\bibitem[SJL17]{soltanolkotabi2017theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D. Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \texttt{arXiv:1707.04926}, 2017.

\bibitem[SS18]{sirignano2018mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks.
\newblock {\em {\sf arXiv:1805.01053}}, 2018.

\bibitem[SSBD14]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Szn91]{sznitman1991topics}
Alain-Sol Sznitman.
\newblock Topics in propagation of chaos.
\newblock In {\em Ecole d'{\'e}t{\'e} de probabilit{\'e}s de Saint-Flour
  XIX---1989}, pages 165--251. Springer, 1991.

\bibitem[Tia17]{tian2017symmetry}
Yuandong Tian.
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with {ReLU} nonlinearity.
\newblock In {\em Workshop at International Conference on Learning
  Representation (ICLR)}, 2017.

\bibitem[WML17]{wang2017scaling}
Chuang Wang, Jonathan Mattingly, and Yue~M Lu.
\newblock Scaling limit: Exact and tractable analysis of online learning
  algorithms with applications to regularized regression and pca.
\newblock {\em {\sf arXiv:1712.04332}}, 2017.

\bibitem[ZLJ16]{zhang2016l1}
Yuchen Zhang, Jason~D. Lee, and Michael~I. Jordan.
\newblock L1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock In {\em Proc. of International Conference on Machine Learning
  (ICML)}, pages 993--1001, 2016.

\bibitem[ZSJ{\etalchar{+}}17]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L. Bartlett, and Inderjit~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \texttt{arXiv:1706.03175}, 2017.

\end{thebibliography}
